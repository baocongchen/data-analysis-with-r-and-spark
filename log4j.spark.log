17/01/13 20:24:32 INFO SparkContext: Running Spark version 1.6.2
17/01/13 20:24:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/01/13 20:24:34 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:355)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:370)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:363)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:104)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:86)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:66)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:280)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:271)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:248)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:763)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:748)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:621)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2198)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2198)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2198)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:322)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2281)
	at org.apache.spark.SparkContext.getOrCreate(SparkContext.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at sparklyr.Invoke$.invoke(invoke.scala:94)
	at sparklyr.StreamHandler$.handleMethodCall(stream.scala:89)
	at sparklyr.StreamHandler$.read(stream.scala:55)
	at sparklyr.BackendHandler.channelRead0(handler.scala:49)
	at sparklyr.BackendHandler.channelRead0(handler.scala:14)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
	at java.lang.Thread.run(Unknown Source)
17/01/13 20:24:34 INFO SecurityManager: Changing view acls to: Baoco
17/01/13 20:24:34 INFO SecurityManager: Changing modify acls to: Baoco
17/01/13 20:24:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(Baoco); users with modify permissions: Set(Baoco)
17/01/13 20:24:37 INFO Utils: Successfully started service 'sparkDriver' on port 61375.
17/01/13 20:24:38 INFO Slf4jLogger: Slf4jLogger started
17/01/13 20:24:38 INFO Remoting: Starting remoting
17/01/13 20:24:39 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@127.0.0.1:61388]
17/01/13 20:24:39 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 61388.
17/01/13 20:24:39 INFO SparkEnv: Registering MapOutputTracker
17/01/13 20:24:39 INFO SparkEnv: Registering BlockManagerMaster
17/01/13 20:24:39 INFO DiskBlockManager: Created local directory at C:\Users\Baoco\AppData\Local\Temp\blockmgr-32b580da-83c0-4dd3-a29a-5017a901dfcb
17/01/13 20:24:39 INFO MemoryStore: MemoryStore started with capacity 511.1 MB
17/01/13 20:24:40 INFO SparkEnv: Registering OutputCommitCoordinator
17/01/13 20:24:41 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/01/13 20:24:41 INFO SparkUI: Started SparkUI at http://127.0.0.1:4040
17/01/13 20:24:41 INFO HttpFileServer: HTTP File server directory is C:\Users\Baoco\AppData\Local\Temp\spark-a0cad002-714e-4681-aec1-f99cc09426e4\httpd-89c8511e-ea2c-4543-b338-a63738ecaa5d
17/01/13 20:24:41 INFO HttpServer: Starting HTTP Server
17/01/13 20:24:41 INFO Utils: Successfully started service 'HTTP file server' on port 61391.
17/01/13 20:24:41 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/spark-csv_2.11-1.3.0.jar at http://127.0.0.1:61391/jars/spark-csv_2.11-1.3.0.jar with timestamp 1484367881256
17/01/13 20:24:41 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/commons-csv-1.1.jar at http://127.0.0.1:61391/jars/commons-csv-1.1.jar with timestamp 1484367881257
17/01/13 20:24:41 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/univocity-parsers-1.5.1.jar at http://127.0.0.1:61391/jars/univocity-parsers-1.5.1.jar with timestamp 1484367881260
17/01/13 20:24:41 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/sparklyr-1.6-2.10.jar at http://127.0.0.1:61391/jars/sparklyr-1.6-2.10.jar with timestamp 1484367881261
17/01/13 20:24:41 INFO Executor: Starting executor ID driver on host localhost
17/01/13 20:24:41 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61408.
17/01/13 20:24:41 INFO NettyBlockTransferService: Server created on 61408
17/01/13 20:24:41 INFO BlockManagerMaster: Trying to register BlockManager
17/01/13 20:24:41 INFO BlockManagerMasterEndpoint: Registering block manager localhost:61408 with 511.1 MB RAM, BlockManagerId(driver, localhost, 61408)
17/01/13 20:24:41 INFO BlockManagerMaster: Registered BlockManager
17/01/13 20:24:45 INFO HiveContext: Initializing execution hive, version 1.2.1
17/01/13 20:24:45 INFO ClientWrapper: Inspected Hadoop version: 2.6.0
17/01/13 20:24:45 INFO ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
17/01/13 20:24:46 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/01/13 20:24:46 INFO ObjectStore: ObjectStore, initialize called
17/01/13 20:24:47 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/01/13 20:24:47 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/01/13 20:24:47 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/01/13 20:24:48 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/01/13 20:24:56 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/01/13 20:24:58 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 20:24:58 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 20:25:05 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 20:25:05 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 20:25:06 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/01/13 20:25:06 INFO ObjectStore: Initialized ObjectStore
17/01/13 20:25:07 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/01/13 20:25:07 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/01/13 20:25:13 INFO HiveMetaStore: Added admin role in metastore
17/01/13 20:25:13 INFO HiveMetaStore: Added public role in metastore
17/01/13 20:25:14 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/01/13 20:25:14 INFO HiveMetaStore: 0: get_all_databases
17/01/13 20:25:14 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_all_databases	
17/01/13 20:25:14 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/01/13 20:25:14 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/01/13 20:25:14 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 20:25:20 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 20:25:20 INFO DAGScheduler: Got job 0 (collect at utils.scala:195) with 1 output partitions
17/01/13 20:25:20 INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:195)
17/01/13 20:25:20 INFO DAGScheduler: Parents of final stage: List()
17/01/13 20:25:20 INFO DAGScheduler: Missing parents: List()
17/01/13 20:25:20 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at collect at utils.scala:195), which has no missing parents
17/01/13 20:25:21 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1968.0 B, free 1968.0 B)
17/01/13 20:25:21 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1230.0 B, free 3.1 KB)
17/01/13 20:25:21 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:61408 (size: 1230.0 B, free: 511.1 MB)
17/01/13 20:25:21 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
17/01/13 20:25:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at collect at utils.scala:195)
17/01/13 20:25:21 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
17/01/13 20:25:21 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2348 bytes)
17/01/13 20:25:21 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/01/13 20:25:21 INFO Executor: Fetching http://127.0.0.1:61391/jars/commons-csv-1.1.jar with timestamp 1484367881257
17/01/13 20:25:22 INFO Utils: Fetching http://127.0.0.1:61391/jars/commons-csv-1.1.jar to C:\Users\Baoco\AppData\Local\Temp\spark-a0cad002-714e-4681-aec1-f99cc09426e4\userFiles-fbce4ad2-3363-453f-893b-22e15d78ad7f\fetchFileTemp3171284407097823672.tmp
17/01/13 20:25:22 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(Unknown Source)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:482)
	at org.apache.hadoop.util.Shell.run(Shell.java:455)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:407)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:430)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:422)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:422)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
17/01/13 20:25:22 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(Unknown Source)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:482)
	at org.apache.hadoop.util.Shell.run(Shell.java:455)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:407)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:430)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:422)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:422)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)

17/01/13 20:25:22 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
17/01/13 20:25:22 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/01/13 20:25:22 INFO TaskSchedulerImpl: Cancelling stage 0
17/01/13 20:25:22 INFO DAGScheduler: ResultStage 0 (collect at utils.scala:195) failed in 0.852 s
17/01/13 20:25:22 INFO DAGScheduler: Job 0 failed: collect at utils.scala:195, took 1.768187 s
17/01/13 20:35:48 INFO SparkContext: Starting job: collect at utils.scala:59
17/01/13 20:35:48 INFO DAGScheduler: Got job 1 (collect at utils.scala:59) with 1 output partitions
17/01/13 20:35:48 INFO DAGScheduler: Final stage: ResultStage 1 (collect at utils.scala:59)
17/01/13 20:35:48 INFO DAGScheduler: Parents of final stage: List()
17/01/13 20:35:48 INFO DAGScheduler: Missing parents: List()
17/01/13 20:35:48 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at map at utils.scala:56), which has no missing parents
17/01/13 20:35:48 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 5.4 KB, free 8.6 KB)
17/01/13 20:35:48 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.0 KB, free 11.5 KB)
17/01/13 20:35:48 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:61408 (size: 3.0 KB, free: 511.1 MB)
17/01/13 20:35:48 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
17/01/13 20:35:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at map at utils.scala:56)
17/01/13 20:35:48 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
17/01/13 20:35:48 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2348 bytes)
17/01/13 20:35:48 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
17/01/13 20:35:48 INFO Executor: Fetching http://127.0.0.1:61391/jars/commons-csv-1.1.jar with timestamp 1484367881257
17/01/13 20:35:48 INFO Utils: Fetching http://127.0.0.1:61391/jars/commons-csv-1.1.jar to C:\Users\Baoco\AppData\Local\Temp\spark-a0cad002-714e-4681-aec1-f99cc09426e4\userFiles-fbce4ad2-3363-453f-893b-22e15d78ad7f\fetchFileTemp1990480845876400583.tmp
17/01/13 20:35:48 INFO Utils: C:\Users\Baoco\AppData\Local\Temp\spark-a0cad002-714e-4681-aec1-f99cc09426e4\userFiles-fbce4ad2-3363-453f-893b-22e15d78ad7f\fetchFileTemp1990480845876400583.tmp has been previously copied to C:\Users\Baoco\AppData\Local\Temp\spark-a0cad002-714e-4681-aec1-f99cc09426e4\userFiles-fbce4ad2-3363-453f-893b-22e15d78ad7f\commons-csv-1.1.jar
17/01/13 20:35:48 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1)
java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(Unknown Source)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:482)
	at org.apache.hadoop.util.Shell.run(Shell.java:455)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:407)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:430)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:422)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:422)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
17/01/13 20:35:48 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(Unknown Source)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:482)
	at org.apache.hadoop.util.Shell.run(Shell.java:455)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:407)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:430)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:422)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:422)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)

17/01/13 20:35:48 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job
17/01/13 20:35:48 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/01/13 20:35:48 INFO TaskSchedulerImpl: Cancelling stage 1
17/01/13 20:35:48 INFO DAGScheduler: ResultStage 1 (collect at utils.scala:59) failed in 0.038 s
17/01/13 20:35:48 INFO DAGScheduler: Job 1 failed: collect at utils.scala:59, took 0.050001 s
17/01/13 20:35:53 INFO SparkContext: Starting job: collect at utils.scala:59
17/01/13 20:35:53 INFO DAGScheduler: Got job 2 (collect at utils.scala:59) with 1 output partitions
17/01/13 20:35:53 INFO DAGScheduler: Final stage: ResultStage 2 (collect at utils.scala:59)
17/01/13 20:35:53 INFO DAGScheduler: Parents of final stage: List()
17/01/13 20:35:53 INFO DAGScheduler: Missing parents: List()
17/01/13 20:35:53 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[9] at map at utils.scala:56), which has no missing parents
17/01/13 20:35:53 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 5.4 KB, free 17.0 KB)
17/01/13 20:35:53 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.0 KB, free 19.9 KB)
17/01/13 20:35:53 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:61408 (size: 3.0 KB, free: 511.1 MB)
17/01/13 20:35:53 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
17/01/13 20:35:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at map at utils.scala:56)
17/01/13 20:35:53 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
17/01/13 20:35:53 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2348 bytes)
17/01/13 20:35:53 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
17/01/13 20:35:53 INFO Executor: Fetching http://127.0.0.1:61391/jars/commons-csv-1.1.jar with timestamp 1484367881257
17/01/13 20:35:53 INFO Utils: Fetching http://127.0.0.1:61391/jars/commons-csv-1.1.jar to C:\Users\Baoco\AppData\Local\Temp\spark-a0cad002-714e-4681-aec1-f99cc09426e4\userFiles-fbce4ad2-3363-453f-893b-22e15d78ad7f\fetchFileTemp5997941001154600355.tmp
17/01/13 20:35:53 INFO Utils: C:\Users\Baoco\AppData\Local\Temp\spark-a0cad002-714e-4681-aec1-f99cc09426e4\userFiles-fbce4ad2-3363-453f-893b-22e15d78ad7f\fetchFileTemp5997941001154600355.tmp has been previously copied to C:\Users\Baoco\AppData\Local\Temp\spark-a0cad002-714e-4681-aec1-f99cc09426e4\userFiles-fbce4ad2-3363-453f-893b-22e15d78ad7f\commons-csv-1.1.jar
17/01/13 20:35:53 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 2)
java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(Unknown Source)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:482)
	at org.apache.hadoop.util.Shell.run(Shell.java:455)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:407)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:430)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:422)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:422)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
17/01/13 20:35:53 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 2, localhost): java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(Unknown Source)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:482)
	at org.apache.hadoop.util.Shell.run(Shell.java:455)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:407)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:430)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:422)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:422)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)

17/01/13 20:35:53 ERROR TaskSetManager: Task 0 in stage 2.0 failed 1 times; aborting job
17/01/13 20:35:53 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
17/01/13 20:35:53 INFO TaskSchedulerImpl: Cancelling stage 2
17/01/13 20:35:53 INFO DAGScheduler: ResultStage 2 (collect at utils.scala:59) failed in 0.028 s
17/01/13 20:35:53 INFO DAGScheduler: Job 2 failed: collect at utils.scala:59, took 0.037426 s
17/01/13 20:35:56 INFO SparkContext: Starting job: collect at utils.scala:59
17/01/13 20:35:56 INFO DAGScheduler: Got job 3 (collect at utils.scala:59) with 1 output partitions
17/01/13 20:35:56 INFO DAGScheduler: Final stage: ResultStage 3 (collect at utils.scala:59)
17/01/13 20:35:56 INFO DAGScheduler: Parents of final stage: List()
17/01/13 20:35:56 INFO DAGScheduler: Missing parents: List()
17/01/13 20:35:56 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[13] at map at utils.scala:56), which has no missing parents
17/01/13 20:35:56 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 5.4 KB, free 25.4 KB)
17/01/13 20:35:56 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.0 KB, free 28.4 KB)
17/01/13 20:35:56 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:61408 (size: 3.0 KB, free: 511.1 MB)
17/01/13 20:35:56 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1006
17/01/13 20:35:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at map at utils.scala:56)
17/01/13 20:35:56 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
17/01/13 20:35:56 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 2348 bytes)
17/01/13 20:35:56 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
17/01/13 20:35:56 INFO Executor: Fetching http://127.0.0.1:61391/jars/commons-csv-1.1.jar with timestamp 1484367881257
17/01/13 20:35:56 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:61408 in memory (size: 3.0 KB, free: 511.1 MB)
17/01/13 20:35:56 INFO ContextCleaner: Cleaned accumulator 6
17/01/13 20:35:56 INFO ContextCleaner: Cleaned accumulator 5
17/01/13 20:35:56 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:61408 in memory (size: 3.0 KB, free: 511.1 MB)
17/01/13 20:35:56 INFO ContextCleaner: Cleaned accumulator 4
17/01/13 20:35:56 INFO ContextCleaner: Cleaned accumulator 3
17/01/13 20:35:56 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:61408 in memory (size: 1230.0 B, free: 511.1 MB)
17/01/13 20:35:56 INFO ContextCleaner: Cleaned accumulator 1
17/01/13 20:35:56 INFO Utils: Fetching http://127.0.0.1:61391/jars/commons-csv-1.1.jar to C:\Users\Baoco\AppData\Local\Temp\spark-a0cad002-714e-4681-aec1-f99cc09426e4\userFiles-fbce4ad2-3363-453f-893b-22e15d78ad7f\fetchFileTemp6100591244924465811.tmp
17/01/13 20:35:56 INFO Utils: C:\Users\Baoco\AppData\Local\Temp\spark-a0cad002-714e-4681-aec1-f99cc09426e4\userFiles-fbce4ad2-3363-453f-893b-22e15d78ad7f\fetchFileTemp6100591244924465811.tmp has been previously copied to C:\Users\Baoco\AppData\Local\Temp\spark-a0cad002-714e-4681-aec1-f99cc09426e4\userFiles-fbce4ad2-3363-453f-893b-22e15d78ad7f\commons-csv-1.1.jar
17/01/13 20:35:56 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)
java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(Unknown Source)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:482)
	at org.apache.hadoop.util.Shell.run(Shell.java:455)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:407)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:430)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:422)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:422)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
17/01/13 20:35:56 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, localhost): java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(Unknown Source)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:482)
	at org.apache.hadoop.util.Shell.run(Shell.java:455)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:407)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:430)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:422)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:422)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)

17/01/13 20:35:56 ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job
17/01/13 20:35:56 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
17/01/13 20:35:56 INFO TaskSchedulerImpl: Cancelling stage 3
17/01/13 20:35:56 INFO DAGScheduler: ResultStage 3 (collect at utils.scala:59) failed in 0.086 s
17/01/13 20:35:56 INFO DAGScheduler: Job 3 failed: collect at utils.scala:59, took 0.131349 s
17/01/13 20:35:57 INFO SparkContext: Starting job: collect at utils.scala:59
17/01/13 20:35:57 INFO DAGScheduler: Got job 4 (collect at utils.scala:59) with 1 output partitions
17/01/13 20:35:57 INFO DAGScheduler: Final stage: ResultStage 4 (collect at utils.scala:59)
17/01/13 20:35:57 INFO DAGScheduler: Parents of final stage: List()
17/01/13 20:35:57 INFO DAGScheduler: Missing parents: List()
17/01/13 20:35:57 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[17] at map at utils.scala:56), which has no missing parents
17/01/13 20:35:57 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 5.4 KB, free 13.8 KB)
17/01/13 20:35:57 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.0 KB, free 16.8 KB)
17/01/13 20:35:57 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:61408 (size: 3.0 KB, free: 511.1 MB)
17/01/13 20:35:57 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1006
17/01/13 20:35:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[17] at map at utils.scala:56)
17/01/13 20:35:57 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
17/01/13 20:35:57 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 2348 bytes)
17/01/13 20:35:57 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
17/01/13 20:35:57 INFO Executor: Fetching http://127.0.0.1:61391/jars/commons-csv-1.1.jar with timestamp 1484367881257
17/01/13 20:35:57 INFO Utils: Fetching http://127.0.0.1:61391/jars/commons-csv-1.1.jar to C:\Users\Baoco\AppData\Local\Temp\spark-a0cad002-714e-4681-aec1-f99cc09426e4\userFiles-fbce4ad2-3363-453f-893b-22e15d78ad7f\fetchFileTemp5219774968878814204.tmp
17/01/13 20:35:57 INFO Utils: C:\Users\Baoco\AppData\Local\Temp\spark-a0cad002-714e-4681-aec1-f99cc09426e4\userFiles-fbce4ad2-3363-453f-893b-22e15d78ad7f\fetchFileTemp5219774968878814204.tmp has been previously copied to C:\Users\Baoco\AppData\Local\Temp\spark-a0cad002-714e-4681-aec1-f99cc09426e4\userFiles-fbce4ad2-3363-453f-893b-22e15d78ad7f\commons-csv-1.1.jar
17/01/13 20:35:57 ERROR Executor: Exception in task 0.0 in stage 4.0 (TID 4)
java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(Unknown Source)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:482)
	at org.apache.hadoop.util.Shell.run(Shell.java:455)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:407)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:430)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:422)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:422)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
17/01/13 20:35:57 WARN TaskSetManager: Lost task 0.0 in stage 4.0 (TID 4, localhost): java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(Unknown Source)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:482)
	at org.apache.hadoop.util.Shell.run(Shell.java:455)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:407)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:430)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:422)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:422)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)

17/01/13 20:35:57 ERROR TaskSetManager: Task 0 in stage 4.0 failed 1 times; aborting job
17/01/13 20:35:57 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
17/01/13 20:35:57 INFO TaskSchedulerImpl: Cancelling stage 4
17/01/13 20:35:57 INFO DAGScheduler: ResultStage 4 (collect at utils.scala:59) failed in 0.044 s
17/01/13 20:35:57 INFO DAGScheduler: Job 4 failed: collect at utils.scala:59, took 0.055762 s
17/01/13 20:36:28 INFO SparkContext: Starting job: collect at utils.scala:59
17/01/13 20:36:28 INFO DAGScheduler: Got job 5 (collect at utils.scala:59) with 1 output partitions
17/01/13 20:36:28 INFO DAGScheduler: Final stage: ResultStage 5 (collect at utils.scala:59)
17/01/13 20:36:28 INFO DAGScheduler: Parents of final stage: List()
17/01/13 20:36:28 INFO DAGScheduler: Missing parents: List()
17/01/13 20:36:28 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[21] at map at utils.scala:56), which has no missing parents
17/01/13 20:36:28 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 5.4 KB, free 22.2 KB)
17/01/13 20:36:28 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.0 KB, free 25.2 KB)
17/01/13 20:36:28 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:61408 (size: 3.0 KB, free: 511.1 MB)
17/01/13 20:36:28 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1006
17/01/13 20:36:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[21] at map at utils.scala:56)
17/01/13 20:36:28 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
17/01/13 20:36:28 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 2348 bytes)
17/01/13 20:36:28 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
17/01/13 20:36:28 INFO Executor: Fetching http://127.0.0.1:61391/jars/commons-csv-1.1.jar with timestamp 1484367881257
17/01/13 20:36:28 INFO Utils: Fetching http://127.0.0.1:61391/jars/commons-csv-1.1.jar to C:\Users\Baoco\AppData\Local\Temp\spark-a0cad002-714e-4681-aec1-f99cc09426e4\userFiles-fbce4ad2-3363-453f-893b-22e15d78ad7f\fetchFileTemp2088494409965449171.tmp
17/01/13 20:36:28 INFO Utils: C:\Users\Baoco\AppData\Local\Temp\spark-a0cad002-714e-4681-aec1-f99cc09426e4\userFiles-fbce4ad2-3363-453f-893b-22e15d78ad7f\fetchFileTemp2088494409965449171.tmp has been previously copied to C:\Users\Baoco\AppData\Local\Temp\spark-a0cad002-714e-4681-aec1-f99cc09426e4\userFiles-fbce4ad2-3363-453f-893b-22e15d78ad7f\commons-csv-1.1.jar
17/01/13 20:36:28 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 5)
java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(Unknown Source)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:482)
	at org.apache.hadoop.util.Shell.run(Shell.java:455)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:407)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:430)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:422)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:422)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
17/01/13 20:36:28 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 5, localhost): java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(Unknown Source)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:482)
	at org.apache.hadoop.util.Shell.run(Shell.java:455)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:407)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:430)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:422)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:422)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)

17/01/13 20:36:28 ERROR TaskSetManager: Task 0 in stage 5.0 failed 1 times; aborting job
17/01/13 20:36:28 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
17/01/13 20:36:28 INFO TaskSchedulerImpl: Cancelling stage 5
17/01/13 20:36:28 INFO DAGScheduler: ResultStage 5 (collect at utils.scala:59) failed in 0.047 s
17/01/13 20:36:28 INFO DAGScheduler: Job 5 failed: collect at utils.scala:59, took 0.060196 s
17/01/13 20:36:31 INFO SparkContext: Invoking stop() from shutdown hook
17/01/13 20:36:31 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
17/01/13 20:36:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/01/13 20:36:31 INFO MemoryStore: MemoryStore cleared
17/01/13 20:36:31 INFO BlockManager: BlockManager stopped
17/01/13 20:36:31 INFO BlockManagerMaster: BlockManagerMaster stopped
17/01/13 20:36:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/01/13 20:36:31 INFO SparkContext: Successfully stopped SparkContext
17/01/13 20:36:31 INFO ShutdownHookManager: Shutdown hook called
17/01/13 20:36:31 INFO ShutdownHookManager: Deleting directory C:\Users\Baoco\AppData\Local\Temp\spark-a0cad002-714e-4681-aec1-f99cc09426e4\httpd-89c8511e-ea2c-4543-b338-a63738ecaa5d
17/01/13 20:36:31 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
17/01/13 20:36:31 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
17/01/13 20:36:31 INFO ShutdownHookManager: Deleting directory C:\Users\Baoco\AppData\Local\Temp\spark-c883f03e-63c6-4eaa-9dbf-e008c345ea62
17/01/13 20:36:31 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
17/01/13 20:36:31 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\Baoco\AppData\Local\Temp\spark-c883f03e-63c6-4eaa-9dbf-e008c345ea62
java.io.IOException: Failed to delete: C:\Users\Baoco\AppData\Local\Temp\spark-c883f03e-63c6-4eaa-9dbf-e008c345ea62
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/01/13 20:36:31 INFO ShutdownHookManager: Deleting directory C:\Users\Baoco\AppData\Local\Temp\spark-a0cad002-714e-4681-aec1-f99cc09426e4
17/01/13 20:40:23 INFO SparkContext: Running Spark version 1.6.2
17/01/13 20:40:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/01/13 20:40:24 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:355)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:370)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:363)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:104)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:86)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:66)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:280)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:271)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:248)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:763)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:748)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:621)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2198)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2198)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2198)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:322)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2281)
	at org.apache.spark.SparkContext.getOrCreate(SparkContext.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at sparklyr.Invoke$.invoke(invoke.scala:94)
	at sparklyr.StreamHandler$.handleMethodCall(stream.scala:89)
	at sparklyr.StreamHandler$.read(stream.scala:55)
	at sparklyr.BackendHandler.channelRead0(handler.scala:49)
	at sparklyr.BackendHandler.channelRead0(handler.scala:14)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
	at java.lang.Thread.run(Unknown Source)
17/01/13 20:40:24 INFO SecurityManager: Changing view acls to: Baoco
17/01/13 20:40:24 INFO SecurityManager: Changing modify acls to: Baoco
17/01/13 20:40:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(Baoco); users with modify permissions: Set(Baoco)
17/01/13 20:40:24 INFO Utils: Successfully started service 'sparkDriver' on port 61614.
17/01/13 20:40:25 INFO Slf4jLogger: Slf4jLogger started
17/01/13 20:40:25 INFO Remoting: Starting remoting
17/01/13 20:40:25 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@127.0.0.1:61627]
17/01/13 20:40:25 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 61627.
17/01/13 20:40:25 INFO SparkEnv: Registering MapOutputTracker
17/01/13 20:40:25 INFO SparkEnv: Registering BlockManagerMaster
17/01/13 20:40:25 INFO DiskBlockManager: Created local directory at C:\Users\Baoco\AppData\Local\Temp\blockmgr-1a15f60c-7985-439e-875b-40b4b1c5b42a
17/01/13 20:40:25 INFO MemoryStore: MemoryStore started with capacity 511.1 MB
17/01/13 20:40:25 INFO SparkEnv: Registering OutputCommitCoordinator
17/01/13 20:40:25 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/01/13 20:40:25 INFO SparkUI: Started SparkUI at http://127.0.0.1:4040
17/01/13 20:40:25 INFO HttpFileServer: HTTP File server directory is C:\Users\Baoco\AppData\Local\Temp\spark-5370e172-9179-4248-aafb-4f095a8c7a19\httpd-e2528cfa-52ca-4422-9da2-5dbca747b856
17/01/13 20:40:25 INFO HttpServer: Starting HTTP Server
17/01/13 20:40:25 INFO Utils: Successfully started service 'HTTP file server' on port 61630.
17/01/13 20:40:25 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/spark-csv_2.11-1.3.0.jar at http://127.0.0.1:61630/jars/spark-csv_2.11-1.3.0.jar with timestamp 1484368825643
17/01/13 20:40:25 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/commons-csv-1.1.jar at http://127.0.0.1:61630/jars/commons-csv-1.1.jar with timestamp 1484368825645
17/01/13 20:40:25 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/univocity-parsers-1.5.1.jar at http://127.0.0.1:61630/jars/univocity-parsers-1.5.1.jar with timestamp 1484368825648
17/01/13 20:40:25 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/sparklyr-1.6-2.10.jar at http://127.0.0.1:61630/jars/sparklyr-1.6-2.10.jar with timestamp 1484368825650
17/01/13 20:40:25 INFO Executor: Starting executor ID driver on host localhost
17/01/13 20:40:25 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61647.
17/01/13 20:40:25 INFO NettyBlockTransferService: Server created on 61647
17/01/13 20:40:25 INFO BlockManagerMaster: Trying to register BlockManager
17/01/13 20:40:25 INFO BlockManagerMasterEndpoint: Registering block manager localhost:61647 with 511.1 MB RAM, BlockManagerId(driver, localhost, 61647)
17/01/13 20:40:25 INFO BlockManagerMaster: Registered BlockManager
17/01/13 20:40:26 INFO HiveContext: Initializing execution hive, version 1.2.1
17/01/13 20:40:26 INFO ClientWrapper: Inspected Hadoop version: 2.6.0
17/01/13 20:40:26 INFO ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
17/01/13 20:40:27 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/01/13 20:40:27 INFO ObjectStore: ObjectStore, initialize called
17/01/13 20:40:27 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/01/13 20:40:27 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/01/13 20:40:27 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/01/13 20:40:27 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/01/13 20:40:35 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/01/13 20:40:37 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 20:40:37 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 20:40:42 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 20:40:42 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 20:40:44 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/01/13 20:40:44 INFO ObjectStore: Initialized ObjectStore
17/01/13 20:40:45 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/01/13 20:40:45 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/01/13 20:40:50 INFO HiveMetaStore: Added admin role in metastore
17/01/13 20:40:50 INFO HiveMetaStore: Added public role in metastore
17/01/13 20:40:50 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/01/13 20:40:50 INFO HiveMetaStore: 0: get_all_databases
17/01/13 20:40:50 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_all_databases	
17/01/13 20:40:50 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/01/13 20:40:50 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/01/13 20:40:50 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 20:40:53 INFO SparkContext: Invoking stop() from shutdown hook
17/01/13 20:40:53 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
17/01/13 20:40:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/01/13 20:40:53 INFO MemoryStore: MemoryStore cleared
17/01/13 20:40:53 INFO BlockManager: BlockManager stopped
17/01/13 20:40:53 INFO BlockManagerMaster: BlockManagerMaster stopped
17/01/13 20:40:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/01/13 20:40:53 INFO SparkContext: Successfully stopped SparkContext
17/01/13 20:40:53 INFO ShutdownHookManager: Shutdown hook called
17/01/13 20:40:53 INFO ShutdownHookManager: Deleting directory C:\Users\Baoco\AppData\Local\Temp\spark-5370e172-9179-4248-aafb-4f095a8c7a19\httpd-e2528cfa-52ca-4422-9da2-5dbca747b856
17/01/13 20:40:53 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
17/01/13 20:40:53 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
17/01/13 20:40:53 INFO ShutdownHookManager: Deleting directory C:\Users\Baoco\AppData\Local\Temp\spark-8ef3a9db-20db-44e1-b755-9de7a1ae35fe
17/01/13 20:40:53 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
17/01/13 20:40:53 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\Baoco\AppData\Local\Temp\spark-8ef3a9db-20db-44e1-b755-9de7a1ae35fe
java.io.IOException: Failed to delete: C:\Users\Baoco\AppData\Local\Temp\spark-8ef3a9db-20db-44e1-b755-9de7a1ae35fe
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/01/13 20:40:53 INFO ShutdownHookManager: Deleting directory C:\Users\Baoco\AppData\Local\Temp\spark-5370e172-9179-4248-aafb-4f095a8c7a19
17/01/13 20:42:26 INFO SparkContext: Running Spark version 1.6.2
17/01/13 20:42:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/01/13 20:42:26 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:355)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:370)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:363)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:104)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:86)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:66)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:280)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:271)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:248)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:763)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:748)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:621)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2198)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2198)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2198)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:322)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2281)
	at org.apache.spark.SparkContext.getOrCreate(SparkContext.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at sparklyr.Invoke$.invoke(invoke.scala:94)
	at sparklyr.StreamHandler$.handleMethodCall(stream.scala:89)
	at sparklyr.StreamHandler$.read(stream.scala:55)
	at sparklyr.BackendHandler.channelRead0(handler.scala:49)
	at sparklyr.BackendHandler.channelRead0(handler.scala:14)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
	at java.lang.Thread.run(Unknown Source)
17/01/13 20:42:26 INFO SecurityManager: Changing view acls to: Baoco
17/01/13 20:42:26 INFO SecurityManager: Changing modify acls to: Baoco
17/01/13 20:42:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(Baoco); users with modify permissions: Set(Baoco)
17/01/13 20:42:27 INFO Utils: Successfully started service 'sparkDriver' on port 61684.
17/01/13 20:42:27 INFO Slf4jLogger: Slf4jLogger started
17/01/13 20:42:27 INFO Remoting: Starting remoting
17/01/13 20:42:27 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@127.0.0.1:61697]
17/01/13 20:42:27 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 61697.
17/01/13 20:42:27 INFO SparkEnv: Registering MapOutputTracker
17/01/13 20:42:27 INFO SparkEnv: Registering BlockManagerMaster
17/01/13 20:42:27 INFO DiskBlockManager: Created local directory at C:\Users\Baoco\AppData\Local\Temp\blockmgr-1d8414dc-29bd-41ae-bac8-23d2eca1c5b7
17/01/13 20:42:27 INFO MemoryStore: MemoryStore started with capacity 511.1 MB
17/01/13 20:42:27 INFO SparkEnv: Registering OutputCommitCoordinator
17/01/13 20:42:27 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/01/13 20:42:27 INFO SparkUI: Started SparkUI at http://127.0.0.1:4040
17/01/13 20:42:27 INFO HttpFileServer: HTTP File server directory is C:\Users\Baoco\AppData\Local\Temp\spark-53fa0eee-e31b-4d76-a80f-370c369e65f4\httpd-bd5b47f6-773d-443a-9b7f-2f6b7c75ee06
17/01/13 20:42:27 INFO HttpServer: Starting HTTP Server
17/01/13 20:42:27 INFO Utils: Successfully started service 'HTTP file server' on port 61700.
17/01/13 20:42:27 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/spark-csv_2.11-1.3.0.jar at http://127.0.0.1:61700/jars/spark-csv_2.11-1.3.0.jar with timestamp 1484368947871
17/01/13 20:42:27 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/commons-csv-1.1.jar at http://127.0.0.1:61700/jars/commons-csv-1.1.jar with timestamp 1484368947874
17/01/13 20:42:27 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/univocity-parsers-1.5.1.jar at http://127.0.0.1:61700/jars/univocity-parsers-1.5.1.jar with timestamp 1484368947876
17/01/13 20:42:27 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/sparklyr-1.6-2.10.jar at http://127.0.0.1:61700/jars/sparklyr-1.6-2.10.jar with timestamp 1484368947878
17/01/13 20:42:27 INFO Executor: Starting executor ID driver on host localhost
17/01/13 20:42:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61717.
17/01/13 20:42:27 INFO NettyBlockTransferService: Server created on 61717
17/01/13 20:42:27 INFO BlockManagerMaster: Trying to register BlockManager
17/01/13 20:42:27 INFO BlockManagerMasterEndpoint: Registering block manager localhost:61717 with 511.1 MB RAM, BlockManagerId(driver, localhost, 61717)
17/01/13 20:42:27 INFO BlockManagerMaster: Registered BlockManager
17/01/13 20:42:28 INFO HiveContext: Initializing execution hive, version 1.2.1
17/01/13 20:42:29 INFO ClientWrapper: Inspected Hadoop version: 2.6.0
17/01/13 20:42:29 INFO ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
17/01/13 20:42:29 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/01/13 20:42:29 INFO ObjectStore: ObjectStore, initialize called
17/01/13 20:42:29 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/01/13 20:42:29 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/01/13 20:42:29 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/01/13 20:42:29 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/01/13 20:42:37 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/01/13 20:42:38 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 20:42:38 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 20:42:44 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 20:42:44 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 20:42:46 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/01/13 20:42:46 INFO ObjectStore: Initialized ObjectStore
17/01/13 20:42:46 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/01/13 20:42:46 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/01/13 20:42:52 INFO HiveMetaStore: Added admin role in metastore
17/01/13 20:42:52 INFO HiveMetaStore: Added public role in metastore
17/01/13 20:42:52 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/01/13 20:42:52 INFO HiveMetaStore: 0: get_all_databases
17/01/13 20:42:52 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_all_databases	
17/01/13 20:42:52 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/01/13 20:42:52 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/01/13 20:42:52 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 20:42:55 INFO SparkContext: Invoking stop() from shutdown hook
17/01/13 20:42:55 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
17/01/13 20:42:55 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/01/13 20:42:55 INFO MemoryStore: MemoryStore cleared
17/01/13 20:42:55 INFO BlockManager: BlockManager stopped
17/01/13 20:42:55 INFO BlockManagerMaster: BlockManagerMaster stopped
17/01/13 20:42:55 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/01/13 20:42:55 INFO SparkContext: Successfully stopped SparkContext
17/01/13 20:42:55 INFO ShutdownHookManager: Shutdown hook called
17/01/13 20:42:55 INFO ShutdownHookManager: Deleting directory C:\Users\Baoco\AppData\Local\Temp\spark-53fa0eee-e31b-4d76-a80f-370c369e65f4\httpd-bd5b47f6-773d-443a-9b7f-2f6b7c75ee06
17/01/13 20:42:55 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
17/01/13 20:42:55 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
17/01/13 20:42:55 INFO ShutdownHookManager: Deleting directory C:\Users\Baoco\AppData\Local\Temp\spark-7572d7d3-22bf-4a41-abf2-1e9b78e70633
17/01/13 20:42:55 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
17/01/13 20:42:55 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\Baoco\AppData\Local\Temp\spark-7572d7d3-22bf-4a41-abf2-1e9b78e70633
java.io.IOException: Failed to delete: C:\Users\Baoco\AppData\Local\Temp\spark-7572d7d3-22bf-4a41-abf2-1e9b78e70633
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/01/13 20:42:55 INFO ShutdownHookManager: Deleting directory C:\Users\Baoco\AppData\Local\Temp\spark-53fa0eee-e31b-4d76-a80f-370c369e65f4
17/01/13 20:45:00 INFO SparkContext: Running Spark version 1.6.2
17/01/13 20:45:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/01/13 20:45:00 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:355)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:370)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:363)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:104)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:86)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:66)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:280)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:271)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:248)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:763)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:748)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:621)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2198)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2198)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2198)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:322)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2281)
	at org.apache.spark.SparkContext.getOrCreate(SparkContext.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at sparklyr.Invoke$.invoke(invoke.scala:94)
	at sparklyr.StreamHandler$.handleMethodCall(stream.scala:89)
	at sparklyr.StreamHandler$.read(stream.scala:55)
	at sparklyr.BackendHandler.channelRead0(handler.scala:49)
	at sparklyr.BackendHandler.channelRead0(handler.scala:14)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
	at java.lang.Thread.run(Unknown Source)
17/01/13 20:45:01 INFO SecurityManager: Changing view acls to: Baoco
17/01/13 20:45:01 INFO SecurityManager: Changing modify acls to: Baoco
17/01/13 20:45:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(Baoco); users with modify permissions: Set(Baoco)
17/01/13 20:45:01 INFO Utils: Successfully started service 'sparkDriver' on port 61757.
17/01/13 20:45:01 INFO Slf4jLogger: Slf4jLogger started
17/01/13 20:45:01 INFO Remoting: Starting remoting
17/01/13 20:45:01 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@127.0.0.1:61770]
17/01/13 20:45:01 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 61770.
17/01/13 20:45:01 INFO SparkEnv: Registering MapOutputTracker
17/01/13 20:45:01 INFO SparkEnv: Registering BlockManagerMaster
17/01/13 20:45:01 INFO DiskBlockManager: Created local directory at C:\Users\Baoco\AppData\Local\Temp\blockmgr-d53aa309-79cb-4301-ade9-2f046950a68b
17/01/13 20:45:02 INFO MemoryStore: MemoryStore started with capacity 511.1 MB
17/01/13 20:45:02 INFO SparkEnv: Registering OutputCommitCoordinator
17/01/13 20:45:02 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/01/13 20:45:02 INFO SparkUI: Started SparkUI at http://127.0.0.1:4040
17/01/13 20:45:02 INFO HttpFileServer: HTTP File server directory is C:\Users\Baoco\AppData\Local\Temp\spark-8afd13f1-6ceb-4985-9674-05d81cf0c795\httpd-0cc26560-afdf-42ca-a807-193aab094ad7
17/01/13 20:45:02 INFO HttpServer: Starting HTTP Server
17/01/13 20:45:02 INFO Utils: Successfully started service 'HTTP file server' on port 61773.
17/01/13 20:45:02 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/spark-csv_2.11-1.3.0.jar at http://127.0.0.1:61773/jars/spark-csv_2.11-1.3.0.jar with timestamp 1484369102364
17/01/13 20:45:02 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/commons-csv-1.1.jar at http://127.0.0.1:61773/jars/commons-csv-1.1.jar with timestamp 1484369102366
17/01/13 20:45:02 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/univocity-parsers-1.5.1.jar at http://127.0.0.1:61773/jars/univocity-parsers-1.5.1.jar with timestamp 1484369102369
17/01/13 20:45:02 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/sparklyr-1.6-2.10.jar at http://127.0.0.1:61773/jars/sparklyr-1.6-2.10.jar with timestamp 1484369102370
17/01/13 20:45:02 INFO Executor: Starting executor ID driver on host localhost
17/01/13 20:45:02 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61790.
17/01/13 20:45:02 INFO NettyBlockTransferService: Server created on 61790
17/01/13 20:45:02 INFO BlockManagerMaster: Trying to register BlockManager
17/01/13 20:45:02 INFO BlockManagerMasterEndpoint: Registering block manager localhost:61790 with 511.1 MB RAM, BlockManagerId(driver, localhost, 61790)
17/01/13 20:45:02 INFO BlockManagerMaster: Registered BlockManager
17/01/13 20:45:03 INFO HiveContext: Initializing execution hive, version 1.2.1
17/01/13 20:45:03 INFO ClientWrapper: Inspected Hadoop version: 2.6.0
17/01/13 20:45:03 INFO ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
17/01/13 20:45:03 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/01/13 20:45:03 INFO ObjectStore: ObjectStore, initialize called
17/01/13 20:45:03 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/01/13 20:45:03 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/01/13 20:45:04 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/01/13 20:45:04 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/01/13 20:45:10 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/01/13 20:45:11 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 20:45:11 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 20:45:18 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 20:45:18 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 20:45:19 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/01/13 20:45:19 INFO ObjectStore: Initialized ObjectStore
17/01/13 20:45:20 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/01/13 20:45:20 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/01/13 20:45:25 INFO HiveMetaStore: Added admin role in metastore
17/01/13 20:45:25 INFO HiveMetaStore: Added public role in metastore
17/01/13 20:45:26 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/01/13 20:45:26 INFO HiveMetaStore: 0: get_all_databases
17/01/13 20:45:26 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_all_databases	
17/01/13 20:45:26 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/01/13 20:45:26 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/01/13 20:45:26 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 20:47:50 INFO SparkContext: Starting job: collect at utils.scala:59
17/01/13 20:47:50 INFO DAGScheduler: Got job 0 (collect at utils.scala:59) with 1 output partitions
17/01/13 20:47:50 INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:59)
17/01/13 20:47:50 INFO DAGScheduler: Parents of final stage: List()
17/01/13 20:47:50 INFO DAGScheduler: Missing parents: List()
17/01/13 20:47:50 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at map at utils.scala:56), which has no missing parents
17/01/13 20:47:51 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 5.4 KB, free 5.4 KB)
17/01/13 20:47:51 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.0 KB, free 8.4 KB)
17/01/13 20:47:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:61790 (size: 3.0 KB, free: 511.1 MB)
17/01/13 20:47:51 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
17/01/13 20:47:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at map at utils.scala:56)
17/01/13 20:47:51 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
17/01/13 20:47:51 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2348 bytes)
17/01/13 20:47:51 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/01/13 20:47:51 INFO Executor: Fetching http://127.0.0.1:61773/jars/commons-csv-1.1.jar with timestamp 1484369102366
17/01/13 20:47:51 INFO Utils: Fetching http://127.0.0.1:61773/jars/commons-csv-1.1.jar to C:\Users\Baoco\AppData\Local\Temp\spark-8afd13f1-6ceb-4985-9674-05d81cf0c795\userFiles-449e72e2-2c6e-49dd-80c0-6164b6cecfc0\fetchFileTemp4921988554283684379.tmp
17/01/13 20:47:51 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(Unknown Source)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:482)
	at org.apache.hadoop.util.Shell.run(Shell.java:455)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:407)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:430)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:422)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:422)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
17/01/13 20:47:51 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(Unknown Source)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:482)
	at org.apache.hadoop.util.Shell.run(Shell.java:455)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:407)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:430)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:422)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:422)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)

17/01/13 20:47:51 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
17/01/13 20:47:51 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/01/13 20:47:51 INFO TaskSchedulerImpl: Cancelling stage 0
17/01/13 20:47:51 INFO DAGScheduler: ResultStage 0 (collect at utils.scala:59) failed in 0.180 s
17/01/13 20:47:51 INFO DAGScheduler: Job 0 failed: collect at utils.scala:59, took 0.349410 s
17/01/13 20:47:52 INFO SparkContext: Invoking stop() from shutdown hook
17/01/13 20:47:52 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
17/01/13 20:47:52 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/01/13 20:47:52 INFO MemoryStore: MemoryStore cleared
17/01/13 20:47:52 INFO BlockManager: BlockManager stopped
17/01/13 20:47:52 INFO BlockManagerMaster: BlockManagerMaster stopped
17/01/13 20:47:52 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/01/13 20:47:52 INFO SparkContext: Successfully stopped SparkContext
17/01/13 20:47:52 INFO ShutdownHookManager: Shutdown hook called
17/01/13 20:47:52 INFO ShutdownHookManager: Deleting directory C:\Users\Baoco\AppData\Local\Temp\spark-8afd13f1-6ceb-4985-9674-05d81cf0c795
17/01/13 20:47:52 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
17/01/13 20:47:52 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
17/01/13 20:47:52 INFO ShutdownHookManager: Deleting directory C:\Users\Baoco\AppData\Local\Temp\spark-0cdcb5a5-eff1-4b7f-9fd8-208fbd4fb18d
17/01/13 20:47:52 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
17/01/13 20:47:52 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\Baoco\AppData\Local\Temp\spark-0cdcb5a5-eff1-4b7f-9fd8-208fbd4fb18d
java.io.IOException: Failed to delete: C:\Users\Baoco\AppData\Local\Temp\spark-0cdcb5a5-eff1-4b7f-9fd8-208fbd4fb18d
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/01/13 20:47:52 INFO ShutdownHookManager: Deleting directory C:\Users\Baoco\AppData\Local\Temp\spark-8afd13f1-6ceb-4985-9674-05d81cf0c795\httpd-0cc26560-afdf-42ca-a807-193aab094ad7
17/01/13 21:51:11 INFO SparkContext: Running Spark version 1.6.2
17/01/13 21:51:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/01/13 21:51:12 INFO SecurityManager: Changing view acls to: Baoco
17/01/13 21:51:12 INFO SecurityManager: Changing modify acls to: Baoco
17/01/13 21:51:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(Baoco); users with modify permissions: Set(Baoco)
17/01/13 21:51:12 INFO Utils: Successfully started service 'sparkDriver' on port 62228.
17/01/13 21:51:12 INFO Slf4jLogger: Slf4jLogger started
17/01/13 21:51:12 INFO Remoting: Starting remoting
17/01/13 21:51:13 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@127.0.0.1:62241]
17/01/13 21:51:13 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 62241.
17/01/13 21:51:13 INFO SparkEnv: Registering MapOutputTracker
17/01/13 21:51:13 INFO SparkEnv: Registering BlockManagerMaster
17/01/13 21:51:13 INFO DiskBlockManager: Created local directory at C:\Users\Baoco\AppData\Local\Temp\blockmgr-8d8629c9-83f9-494a-a4c8-de633279e6f0
17/01/13 21:51:13 INFO MemoryStore: MemoryStore started with capacity 511.1 MB
17/01/13 21:51:13 INFO SparkEnv: Registering OutputCommitCoordinator
17/01/13 21:51:13 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/01/13 21:51:13 INFO SparkUI: Started SparkUI at http://127.0.0.1:4040
17/01/13 21:51:13 INFO HttpFileServer: HTTP File server directory is C:\Users\Baoco\AppData\Local\Temp\spark-a9fd84ba-89b7-4317-8ab0-77a8e6700683\httpd-2326e080-bf13-45bb-abd1-ce7b31e13c8c
17/01/13 21:51:13 INFO HttpServer: Starting HTTP Server
17/01/13 21:51:13 INFO Utils: Successfully started service 'HTTP file server' on port 62244.
17/01/13 21:51:13 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/spark-csv_2.11-1.3.0.jar at http://127.0.0.1:62244/jars/spark-csv_2.11-1.3.0.jar with timestamp 1484373073471
17/01/13 21:51:13 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/commons-csv-1.1.jar at http://127.0.0.1:62244/jars/commons-csv-1.1.jar with timestamp 1484373073473
17/01/13 21:51:13 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/univocity-parsers-1.5.1.jar at http://127.0.0.1:62244/jars/univocity-parsers-1.5.1.jar with timestamp 1484373073475
17/01/13 21:51:13 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/sparklyr-1.6-2.10.jar at http://127.0.0.1:62244/jars/sparklyr-1.6-2.10.jar with timestamp 1484373073477
17/01/13 21:51:13 INFO Executor: Starting executor ID driver on host localhost
17/01/13 21:51:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62261.
17/01/13 21:51:13 INFO NettyBlockTransferService: Server created on 62261
17/01/13 21:51:13 INFO BlockManagerMaster: Trying to register BlockManager
17/01/13 21:51:13 INFO BlockManagerMasterEndpoint: Registering block manager localhost:62261 with 511.1 MB RAM, BlockManagerId(driver, localhost, 62261)
17/01/13 21:51:13 INFO BlockManagerMaster: Registered BlockManager
17/01/13 21:51:14 INFO HiveContext: Initializing execution hive, version 1.2.1
17/01/13 21:51:14 INFO ClientWrapper: Inspected Hadoop version: 2.6.0
17/01/13 21:51:14 INFO ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
17/01/13 21:51:14 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/01/13 21:51:14 INFO ObjectStore: ObjectStore, initialize called
17/01/13 21:51:15 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/01/13 21:51:15 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/01/13 21:51:15 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/01/13 21:51:15 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/01/13 21:51:22 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/01/13 21:51:24 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 21:51:24 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 21:51:29 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 21:51:29 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 21:51:30 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/01/13 21:51:30 INFO ObjectStore: Initialized ObjectStore
17/01/13 21:51:30 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/01/13 21:51:31 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/01/13 21:51:36 INFO HiveMetaStore: Added admin role in metastore
17/01/13 21:51:36 INFO HiveMetaStore: Added public role in metastore
17/01/13 21:51:37 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/01/13 21:51:37 INFO HiveMetaStore: 0: get_all_databases
17/01/13 21:51:37 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_all_databases	
17/01/13 21:51:37 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/01/13 21:51:37 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/01/13 21:51:37 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 21:51:38 INFO SessionState: Created HDFS directory: C:/Users/Baoco/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/Baoco
17/01/13 21:51:38 INFO SessionState: Created local directory: C:/Users/Baoco/AppData/Local/Temp/5a287c74-9311-4b49-82ea-f9565e0a2a5f_resources
17/01/13 21:51:38 INFO SessionState: Created HDFS directory: C:/Users/Baoco/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/Baoco/5a287c74-9311-4b49-82ea-f9565e0a2a5f
17/01/13 21:51:38 INFO SessionState: Created local directory: C:/Users/Baoco/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/5a287c74-9311-4b49-82ea-f9565e0a2a5f
17/01/13 21:51:38 INFO SessionState: Created HDFS directory: C:/Users/Baoco/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/Baoco/5a287c74-9311-4b49-82ea-f9565e0a2a5f/_tmp_space.db
17/01/13 21:51:38 INFO HiveContext: default warehouse location is C:\Users\Baoco\AppData\Local\rstudio\spark\Cache\spark-1.6.2-bin-hadoop2.6\tmp\hive
17/01/13 21:51:38 INFO HiveContext: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/01/13 21:51:38 INFO ClientWrapper: Inspected Hadoop version: 2.6.0
17/01/13 21:51:38 INFO ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
17/01/13 21:51:39 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/01/13 21:51:39 INFO ObjectStore: ObjectStore, initialize called
17/01/13 21:51:39 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/01/13 21:51:39 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/01/13 21:51:39 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/01/13 21:51:39 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/01/13 21:51:40 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/01/13 21:51:41 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 21:51:41 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 21:51:42 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 21:51:42 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 21:51:42 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/01/13 21:51:42 INFO ObjectStore: Initialized ObjectStore
17/01/13 21:51:42 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/01/13 21:51:42 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/01/13 21:51:42 INFO HiveMetaStore: Added admin role in metastore
17/01/13 21:51:42 INFO HiveMetaStore: Added public role in metastore
17/01/13 21:51:42 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/01/13 21:51:43 INFO HiveMetaStore: 0: get_all_databases
17/01/13 21:51:43 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_all_databases	
17/01/13 21:51:43 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/01/13 21:51:43 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/01/13 21:51:43 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 21:51:43 INFO SessionState: Created local directory: C:/Users/Baoco/AppData/Local/Temp/ea7f9243-11f3-428c-9eff-c68d20d86452_resources
17/01/13 21:51:43 INFO SessionState: Created HDFS directory: C:/Users/Baoco/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/Baoco/ea7f9243-11f3-428c-9eff-c68d20d86452
17/01/13 21:51:43 INFO SessionState: Created local directory: C:/Users/Baoco/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/ea7f9243-11f3-428c-9eff-c68d20d86452
17/01/13 21:51:43 INFO SessionState: Created HDFS directory: C:/Users/Baoco/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/Baoco/ea7f9243-11f3-428c-9eff-c68d20d86452/_tmp_space.db
17/01/13 21:52:49 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/01/13 21:52:49 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/01/13 21:52:49 INFO SparkContext: Starting job: collect at utils.scala:59
17/01/13 21:52:49 INFO DAGScheduler: Got job 0 (collect at utils.scala:59) with 1 output partitions
17/01/13 21:52:49 INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:59)
17/01/13 21:52:49 INFO DAGScheduler: Parents of final stage: List()
17/01/13 21:52:49 INFO DAGScheduler: Missing parents: List()
17/01/13 21:52:49 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at map at utils.scala:56), which has no missing parents
17/01/13 21:52:49 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 5.4 KB, free 5.4 KB)
17/01/13 21:52:49 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.0 KB, free 8.4 KB)
17/01/13 21:52:49 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:62261 (size: 3.0 KB, free: 511.1 MB)
17/01/13 21:52:49 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
17/01/13 21:52:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at map at utils.scala:56)
17/01/13 21:52:49 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
17/01/13 21:52:49 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2348 bytes)
17/01/13 21:52:49 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/01/13 21:52:49 INFO Executor: Fetching http://127.0.0.1:62244/jars/univocity-parsers-1.5.1.jar with timestamp 1484373073475
17/01/13 21:52:49 INFO Utils: Fetching http://127.0.0.1:62244/jars/univocity-parsers-1.5.1.jar to C:\Users\Baoco\AppData\Local\Temp\spark-a9fd84ba-89b7-4317-8ab0-77a8e6700683\userFiles-4f656c1e-c2ed-44ed-9c30-6c02eec74b09\fetchFileTemp6028741239723014256.tmp
17/01/13 21:52:50 INFO Executor: Adding file:/C:/Users/Baoco/AppData/Local/Temp/spark-a9fd84ba-89b7-4317-8ab0-77a8e6700683/userFiles-4f656c1e-c2ed-44ed-9c30-6c02eec74b09/univocity-parsers-1.5.1.jar to class loader
17/01/13 21:52:50 INFO Executor: Fetching http://127.0.0.1:62244/jars/sparklyr-1.6-2.10.jar with timestamp 1484373073477
17/01/13 21:52:50 INFO Utils: Fetching http://127.0.0.1:62244/jars/sparklyr-1.6-2.10.jar to C:\Users\Baoco\AppData\Local\Temp\spark-a9fd84ba-89b7-4317-8ab0-77a8e6700683\userFiles-4f656c1e-c2ed-44ed-9c30-6c02eec74b09\fetchFileTemp6392352587369129566.tmp
17/01/13 21:52:50 INFO Executor: Adding file:/C:/Users/Baoco/AppData/Local/Temp/spark-a9fd84ba-89b7-4317-8ab0-77a8e6700683/userFiles-4f656c1e-c2ed-44ed-9c30-6c02eec74b09/sparklyr-1.6-2.10.jar to class loader
17/01/13 21:52:50 INFO Executor: Fetching http://127.0.0.1:62244/jars/spark-csv_2.11-1.3.0.jar with timestamp 1484373073471
17/01/13 21:52:50 INFO Utils: Fetching http://127.0.0.1:62244/jars/spark-csv_2.11-1.3.0.jar to C:\Users\Baoco\AppData\Local\Temp\spark-a9fd84ba-89b7-4317-8ab0-77a8e6700683\userFiles-4f656c1e-c2ed-44ed-9c30-6c02eec74b09\fetchFileTemp6184698259844579425.tmp
17/01/13 21:52:50 INFO Executor: Adding file:/C:/Users/Baoco/AppData/Local/Temp/spark-a9fd84ba-89b7-4317-8ab0-77a8e6700683/userFiles-4f656c1e-c2ed-44ed-9c30-6c02eec74b09/spark-csv_2.11-1.3.0.jar to class loader
17/01/13 21:52:50 INFO Executor: Fetching http://127.0.0.1:62244/jars/commons-csv-1.1.jar with timestamp 1484373073473
17/01/13 21:52:50 INFO Utils: Fetching http://127.0.0.1:62244/jars/commons-csv-1.1.jar to C:\Users\Baoco\AppData\Local\Temp\spark-a9fd84ba-89b7-4317-8ab0-77a8e6700683\userFiles-4f656c1e-c2ed-44ed-9c30-6c02eec74b09\fetchFileTemp6413907391841987561.tmp
17/01/13 21:52:50 INFO Executor: Adding file:/C:/Users/Baoco/AppData/Local/Temp/spark-a9fd84ba-89b7-4317-8ab0-77a8e6700683/userFiles-4f656c1e-c2ed-44ed-9c30-6c02eec74b09/commons-csv-1.1.jar to class loader
17/01/13 21:52:50 INFO GenerateUnsafeProjection: Code generated in 256.086821 ms
17/01/13 21:52:50 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1060 bytes result sent to driver
17/01/13 21:52:50 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1064 ms on localhost (1/1)
17/01/13 21:52:50 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/01/13 21:52:50 INFO DAGScheduler: ResultStage 0 (collect at utils.scala:59) finished in 1.101 s
17/01/13 21:52:50 INFO DAGScheduler: Job 0 finished: collect at utils.scala:59, took 1.330913 s
17/01/13 21:52:51 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 61.8 KB, free 70.2 KB)
17/01/13 21:52:51 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.3 KB, free 89.5 KB)
17/01/13 21:52:51 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:62261 (size: 19.3 KB, free: 511.1 MB)
17/01/13 21:52:51 INFO SparkContext: Created broadcast 1 from textFile at TextFile.scala:30
17/01/13 21:52:51 INFO FileInputFormat: Total input paths to process : 1
17/01/13 21:52:51 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/01/13 21:52:51 INFO DAGScheduler: Got job 1 (take at CsvRelation.scala:249) with 1 output partitions
17/01/13 21:52:51 INFO DAGScheduler: Final stage: ResultStage 1 (take at CsvRelation.scala:249)
17/01/13 21:52:51 INFO DAGScheduler: Parents of final stage: List()
17/01/13 21:52:51 INFO DAGScheduler: Missing parents: List()
17/01/13 21:52:51 INFO DAGScheduler: Submitting ResultStage 1 (C:\Users\Baoco\AppData\Local\Temp\RtmpUBSmNb/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv MapPartitionsRDD[5] at textFile at TextFile.scala:30), which has no missing parents
17/01/13 21:52:51 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 3.2 KB, free 92.7 KB)
17/01/13 21:52:51 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 1945.0 B, free 94.6 KB)
17/01/13 21:52:51 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:62261 (size: 1945.0 B, free: 511.1 MB)
17/01/13 21:52:51 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
17/01/13 21:52:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (C:\Users\Baoco\AppData\Local\Temp\RtmpUBSmNb/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv MapPartitionsRDD[5] at textFile at TextFile.scala:30)
17/01/13 21:52:51 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
17/01/13 21:52:51 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 21:52:51 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
17/01/13 21:52:51 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpUBSmNb/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv:0+2088
17/01/13 21:52:51 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
17/01/13 21:52:51 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
17/01/13 21:52:51 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
17/01/13 21:52:51 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
17/01/13 21:52:51 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
17/01/13 21:52:51 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2354 bytes result sent to driver
17/01/13 21:52:51 INFO DAGScheduler: ResultStage 1 (take at CsvRelation.scala:249) finished in 0.142 s
17/01/13 21:52:51 INFO DAGScheduler: Job 1 finished: take at CsvRelation.scala:249, took 0.158844 s
17/01/13 21:52:51 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 140 ms on localhost (1/1)
17/01/13 21:52:51 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/01/13 21:52:52 INFO ParseDriver: Parsing command: SELECT * FROM  `iris`
17/01/13 21:52:52 INFO ParseDriver: Parse Completed
17/01/13 21:52:53 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 208.5 KB, free 303.1 KB)
17/01/13 21:52:53 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 19.3 KB, free 322.5 KB)
17/01/13 21:52:53 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:62261 (size: 19.3 KB, free: 511.1 MB)
17/01/13 21:52:53 INFO SparkContext: Created broadcast 3 from textFile at TextFile.scala:30
17/01/13 21:52:53 INFO FileInputFormat: Total input paths to process : 1
17/01/13 21:52:53 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/01/13 21:52:53 INFO DAGScheduler: Got job 2 (take at CsvRelation.scala:249) with 1 output partitions
17/01/13 21:52:53 INFO DAGScheduler: Final stage: ResultStage 2 (take at CsvRelation.scala:249)
17/01/13 21:52:53 INFO DAGScheduler: Parents of final stage: List()
17/01/13 21:52:53 INFO DAGScheduler: Missing parents: List()
17/01/13 21:52:53 INFO DAGScheduler: Submitting ResultStage 2 (C:\Users\Baoco\AppData\Local\Temp\RtmpUBSmNb/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv MapPartitionsRDD[7] at textFile at TextFile.scala:30), which has no missing parents
17/01/13 21:52:53 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 3.2 KB, free 325.7 KB)
17/01/13 21:52:53 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 1946.0 B, free 327.6 KB)
17/01/13 21:52:53 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:62261 (size: 1946.0 B, free: 511.1 MB)
17/01/13 21:52:53 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1006
17/01/13 21:52:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (C:\Users\Baoco\AppData\Local\Temp\RtmpUBSmNb/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv MapPartitionsRDD[7] at textFile at TextFile.scala:30)
17/01/13 21:52:53 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
17/01/13 21:52:53 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 21:52:53 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
17/01/13 21:52:53 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpUBSmNb/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv:0+2088
17/01/13 21:52:53 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2354 bytes result sent to driver
17/01/13 21:52:53 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 9 ms on localhost (1/1)
17/01/13 21:52:53 INFO DAGScheduler: ResultStage 2 (take at CsvRelation.scala:249) finished in 0.010 s
17/01/13 21:52:53 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
17/01/13 21:52:53 INFO DAGScheduler: Job 2 finished: take at CsvRelation.scala:249, took 0.021421 s
17/01/13 21:52:53 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 208.5 KB, free 536.1 KB)
17/01/13 21:52:53 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 19.3 KB, free 555.4 KB)
17/01/13 21:52:53 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:62261 (size: 19.3 KB, free: 511.1 MB)
17/01/13 21:52:53 INFO SparkContext: Created broadcast 5 from textFile at TextFile.scala:30
17/01/13 21:52:53 INFO FileInputFormat: Total input paths to process : 1
17/01/13 21:52:53 INFO SparkContext: Starting job: sql at null:-2
17/01/13 21:52:54 INFO DAGScheduler: Registering RDD 17 (sql at null:-2)
17/01/13 21:52:54 INFO DAGScheduler: Got job 3 (sql at null:-2) with 1 output partitions
17/01/13 21:52:54 INFO DAGScheduler: Final stage: ResultStage 4 (sql at null:-2)
17/01/13 21:52:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
17/01/13 21:52:54 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
17/01/13 21:52:54 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[17] at sql at null:-2), which has no missing parents
17/01/13 21:52:54 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 18.3 KB, free 573.7 KB)
17/01/13 21:52:54 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 8.7 KB, free 582.3 KB)
17/01/13 21:52:54 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:62261 (size: 8.7 KB, free: 511.1 MB)
17/01/13 21:52:54 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1006
17/01/13 21:52:54 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[17] at sql at null:-2)
17/01/13 21:52:54 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks
17/01/13 21:52:54 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 21:52:54 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 4, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 21:52:54 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
17/01/13 21:52:54 INFO Executor: Running task 1.0 in stage 3.0 (TID 4)
17/01/13 21:52:54 INFO CacheManager: Partition rdd_14_0 not found, computing it
17/01/13 21:52:54 INFO CacheManager: Partition rdd_14_1 not found, computing it
17/01/13 21:52:54 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpUBSmNb/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv:0+2088
17/01/13 21:52:54 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpUBSmNb/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv:2088+2089
17/01/13 21:52:54 INFO GenerateUnsafeProjection: Code generated in 21.791554 ms
17/01/13 21:52:54 INFO MemoryStore: Block rdd_14_0 stored as values in memory (estimated size 3.2 KB, free 585.5 KB)
17/01/13 21:52:54 INFO MemoryStore: Block rdd_14_1 stored as values in memory (estimated size 3.1 KB, free 588.7 KB)
17/01/13 21:52:54 INFO BlockManagerInfo: Added rdd_14_0 in memory on localhost:62261 (size: 3.2 KB, free: 511.1 MB)
17/01/13 21:52:54 INFO BlockManagerInfo: Added rdd_14_1 in memory on localhost:62261 (size: 3.1 KB, free: 511.0 MB)
17/01/13 21:52:54 INFO GeneratePredicate: Code generated in 9.066232 ms
17/01/13 21:52:54 INFO GenerateColumnAccessor: Code generated in 15.003294 ms
17/01/13 21:52:54 INFO GenerateMutableProjection: Code generated in 7.060474 ms
17/01/13 21:52:54 INFO GenerateUnsafeProjection: Code generated in 4.529063 ms
17/01/13 21:52:54 INFO GenerateMutableProjection: Code generated in 9.379832 ms
17/01/13 21:52:54 INFO GenerateUnsafeRowJoiner: Code generated in 7.78026 ms
17/01/13 21:52:54 INFO GenerateUnsafeProjection: Code generated in 9.180578 ms
17/01/13 21:52:54 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3784 bytes result sent to driver
17/01/13 21:52:54 INFO Executor: Finished task 1.0 in stage 3.0 (TID 4). 3787 bytes result sent to driver
17/01/13 21:52:54 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 570 ms on localhost (1/2)
17/01/13 21:52:54 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 4) in 569 ms on localhost (2/2)
17/01/13 21:52:54 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
17/01/13 21:52:54 INFO DAGScheduler: ShuffleMapStage 3 (sql at null:-2) finished in 0.571 s
17/01/13 21:52:54 INFO DAGScheduler: looking for newly runnable stages
17/01/13 21:52:54 INFO DAGScheduler: running: Set()
17/01/13 21:52:54 INFO DAGScheduler: waiting: Set(ResultStage 4)
17/01/13 21:52:54 INFO DAGScheduler: failed: Set()
17/01/13 21:52:54 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[20] at sql at null:-2), which has no missing parents
17/01/13 21:52:54 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 9.3 KB, free 597.9 KB)
17/01/13 21:52:54 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 4.6 KB, free 602.6 KB)
17/01/13 21:52:54 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:62261 (size: 4.6 KB, free: 511.0 MB)
17/01/13 21:52:54 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1006
17/01/13 21:52:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[20] at sql at null:-2)
17/01/13 21:52:54 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
17/01/13 21:52:54 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 5, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 21:52:54 INFO Executor: Running task 0.0 in stage 4.0 (TID 5)
17/01/13 21:52:54 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 21:52:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
17/01/13 21:52:54 INFO GenerateMutableProjection: Code generated in 8.3665 ms
17/01/13 21:52:54 INFO GenerateMutableProjection: Code generated in 4.171516 ms
17/01/13 21:52:54 INFO Executor: Finished task 0.0 in stage 4.0 (TID 5). 1830 bytes result sent to driver
17/01/13 21:52:54 INFO DAGScheduler: ResultStage 4 (sql at null:-2) finished in 0.196 s
17/01/13 21:52:54 INFO DAGScheduler: Job 3 finished: sql at null:-2, took 0.832644 s
17/01/13 21:52:54 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 5) in 196 ms on localhost (1/1)
17/01/13 21:52:54 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
17/01/13 21:52:54 INFO ParseDriver: Parsing command: SELECT count(*) FROM  `iris`
17/01/13 21:52:54 INFO ParseDriver: Parse Completed
17/01/13 21:52:55 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 21:52:55 INFO DAGScheduler: Registering RDD 24 (collect at utils.scala:195)
17/01/13 21:52:55 INFO DAGScheduler: Got job 4 (collect at utils.scala:195) with 1 output partitions
17/01/13 21:52:55 INFO DAGScheduler: Final stage: ResultStage 6 (collect at utils.scala:195)
17/01/13 21:52:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
17/01/13 21:52:55 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 5)
17/01/13 21:52:55 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[24] at collect at utils.scala:195), which has no missing parents
17/01/13 21:52:55 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 18.4 KB, free 620.9 KB)
17/01/13 21:52:55 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.7 KB, free 629.6 KB)
17/01/13 21:52:55 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:62261 (size: 8.7 KB, free: 511.0 MB)
17/01/13 21:52:55 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1006
17/01/13 21:52:55 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[24] at collect at utils.scala:195)
17/01/13 21:52:55 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks
17/01/13 21:52:55 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 21:52:55 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 7, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 21:52:55 INFO Executor: Running task 0.0 in stage 5.0 (TID 6)
17/01/13 21:52:55 INFO Executor: Running task 1.0 in stage 5.0 (TID 7)
17/01/13 21:52:55 INFO BlockManager: Found block rdd_14_1 locally
17/01/13 21:52:55 INFO BlockManager: Found block rdd_14_0 locally
17/01/13 21:52:55 INFO Executor: Finished task 1.0 in stage 5.0 (TID 7). 2679 bytes result sent to driver
17/01/13 21:52:55 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 7) in 30 ms on localhost (1/2)
17/01/13 21:52:55 INFO Executor: Finished task 0.0 in stage 5.0 (TID 6). 2679 bytes result sent to driver
17/01/13 21:52:55 INFO DAGScheduler: ShuffleMapStage 5 (collect at utils.scala:195) finished in 0.039 s
17/01/13 21:52:55 INFO DAGScheduler: looking for newly runnable stages
17/01/13 21:52:55 INFO DAGScheduler: running: Set()
17/01/13 21:52:55 INFO DAGScheduler: waiting: Set(ResultStage 6)
17/01/13 21:52:55 INFO DAGScheduler: failed: Set()
17/01/13 21:52:55 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[27] at collect at utils.scala:195), which has no missing parents
17/01/13 21:52:55 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 9.4 KB, free 639.0 KB)
17/01/13 21:52:55 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 6) in 38 ms on localhost (2/2)
17/01/13 21:52:55 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
17/01/13 21:52:55 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.6 KB, free 643.6 KB)
17/01/13 21:52:55 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:62261 (size: 4.6 KB, free: 511.0 MB)
17/01/13 21:52:55 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1006
17/01/13 21:52:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[27] at collect at utils.scala:195)
17/01/13 21:52:55 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
17/01/13 21:52:55 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 21:52:55 INFO Executor: Running task 0.0 in stage 6.0 (TID 8)
17/01/13 21:52:55 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 21:52:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/01/13 21:52:55 INFO Executor: Finished task 0.0 in stage 6.0 (TID 8). 1830 bytes result sent to driver
17/01/13 21:52:55 INFO DAGScheduler: ResultStage 6 (collect at utils.scala:195) finished in 0.014 s
17/01/13 21:52:55 INFO DAGScheduler: Job 4 finished: collect at utils.scala:195, took 0.072875 s
17/01/13 21:52:55 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 13 ms on localhost (1/1)
17/01/13 21:52:55 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
17/01/13 21:52:55 INFO ParseDriver: Parsing command: SELECT *
FROM `iris` AS `zzz1`
WHERE (0 = 1)
17/01/13 21:52:55 INFO ParseDriver: Parse Completed
17/01/13 21:52:55 INFO ContextCleaner: Cleaned accumulator 10
17/01/13 21:52:55 INFO ContextCleaner: Cleaned accumulator 9
17/01/13 21:52:55 INFO ContextCleaner: Cleaned accumulator 8
17/01/13 21:52:55 INFO ContextCleaner: Cleaned accumulator 7
17/01/13 21:52:55 INFO BlockManagerInfo: Removed broadcast_4_piece0 on localhost:62261 in memory (size: 1946.0 B, free: 511.0 MB)
17/01/13 21:52:55 INFO ContextCleaner: Cleaned accumulator 5
17/01/13 21:52:55 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:62261 in memory (size: 19.3 KB, free: 511.1 MB)
17/01/13 21:52:55 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:62261 in memory (size: 1945.0 B, free: 511.1 MB)
17/01/13 21:52:55 INFO ContextCleaner: Cleaned accumulator 4
17/01/13 21:52:55 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:62261 in memory (size: 19.3 KB, free: 511.1 MB)
17/01/13 21:52:55 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:62261 in memory (size: 3.0 KB, free: 511.1 MB)
17/01/13 21:52:55 INFO ContextCleaner: Cleaned accumulator 3
17/01/13 21:52:55 INFO ContextCleaner: Cleaned accumulator 2
17/01/13 21:52:55 INFO BlockManagerInfo: Removed broadcast_9_piece0 on localhost:62261 in memory (size: 4.6 KB, free: 511.1 MB)
17/01/13 21:52:55 INFO ContextCleaner: Cleaned accumulator 26
17/01/13 21:52:55 INFO BlockManagerInfo: Removed broadcast_8_piece0 on localhost:62261 in memory (size: 8.7 KB, free: 511.1 MB)
17/01/13 21:52:55 INFO ContextCleaner: Cleaned accumulator 25
17/01/13 21:52:55 INFO ContextCleaner: Cleaned shuffle 1
17/01/13 21:52:55 INFO BlockManagerInfo: Removed broadcast_7_piece0 on localhost:62261 in memory (size: 4.6 KB, free: 511.1 MB)
17/01/13 21:52:55 INFO ContextCleaner: Cleaned accumulator 16
17/01/13 21:52:55 INFO BlockManagerInfo: Removed broadcast_6_piece0 on localhost:62261 in memory (size: 8.7 KB, free: 511.1 MB)
17/01/13 21:52:55 INFO ContextCleaner: Cleaned accumulator 15
17/01/13 21:52:55 INFO ContextCleaner: Cleaned shuffle 0
17/01/13 21:52:55 INFO ContextCleaner: Cleaned accumulator 14
17/01/13 21:52:55 INFO ContextCleaner: Cleaned accumulator 13
17/01/13 21:52:55 INFO ContextCleaner: Cleaned accumulator 12
17/01/13 21:52:55 INFO ContextCleaner: Cleaned accumulator 11
17/01/13 21:52:55 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/01/13 21:52:55 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/01/13 21:52:55 INFO SparkContext: Starting job: collect at utils.scala:59
17/01/13 21:52:55 INFO DAGScheduler: Got job 5 (collect at utils.scala:59) with 1 output partitions
17/01/13 21:52:55 INFO DAGScheduler: Final stage: ResultStage 7 (collect at utils.scala:59)
17/01/13 21:52:55 INFO DAGScheduler: Parents of final stage: List()
17/01/13 21:52:55 INFO DAGScheduler: Missing parents: List()
17/01/13 21:52:55 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[31] at map at utils.scala:56), which has no missing parents
17/01/13 21:52:55 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.4 KB, free 239.6 KB)
17/01/13 21:52:55 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.0 KB, free 242.6 KB)
17/01/13 21:52:55 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:62261 (size: 3.0 KB, free: 511.1 MB)
17/01/13 21:52:55 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1006
17/01/13 21:52:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[31] at map at utils.scala:56)
17/01/13 21:52:55 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks
17/01/13 21:52:55 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 9, localhost, partition 0,PROCESS_LOCAL, 2646 bytes)
17/01/13 21:52:55 INFO Executor: Running task 0.0 in stage 7.0 (TID 9)
17/01/13 21:52:55 INFO Executor: Finished task 0.0 in stage 7.0 (TID 9). 1067 bytes result sent to driver
17/01/13 21:52:55 INFO DAGScheduler: ResultStage 7 (collect at utils.scala:59) finished in 0.008 s
17/01/13 21:52:55 INFO DAGScheduler: Job 5 finished: collect at utils.scala:59, took 0.014997 s
17/01/13 21:52:55 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 9) in 7 ms on localhost (1/1)
17/01/13 21:52:55 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
17/01/13 21:53:06 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 208.5 KB, free 451.1 KB)
17/01/13 21:53:06 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 19.3 KB, free 470.4 KB)
17/01/13 21:53:06 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:62261 (size: 19.3 KB, free: 511.1 MB)
17/01/13 21:53:06 INFO SparkContext: Created broadcast 11 from textFile at TextFile.scala:30
17/01/13 21:53:06 INFO FileInputFormat: Total input paths to process : 1
17/01/13 21:53:06 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/01/13 21:53:06 INFO DAGScheduler: Got job 6 (take at CsvRelation.scala:249) with 1 output partitions
17/01/13 21:53:06 INFO DAGScheduler: Final stage: ResultStage 8 (take at CsvRelation.scala:249)
17/01/13 21:53:06 INFO DAGScheduler: Parents of final stage: List()
17/01/13 21:53:06 INFO DAGScheduler: Missing parents: List()
17/01/13 21:53:06 INFO DAGScheduler: Submitting ResultStage 8 (C:\Users\Baoco\AppData\Local\Temp\RtmpUBSmNb/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv MapPartitionsRDD[33] at textFile at TextFile.scala:30), which has no missing parents
17/01/13 21:53:06 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 3.2 KB, free 473.6 KB)
17/01/13 21:53:06 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 1944.0 B, free 475.5 KB)
17/01/13 21:53:06 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:62261 (size: 1944.0 B, free: 511.1 MB)
17/01/13 21:53:06 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1006
17/01/13 21:53:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (C:\Users\Baoco\AppData\Local\Temp\RtmpUBSmNb/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv MapPartitionsRDD[33] at textFile at TextFile.scala:30)
17/01/13 21:53:06 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks
17/01/13 21:53:06 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 10, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 21:53:06 INFO Executor: Running task 0.0 in stage 8.0 (TID 10)
17/01/13 21:53:06 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpUBSmNb/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv:0+16824941
17/01/13 21:53:06 INFO Executor: Finished task 0.0 in stage 8.0 (TID 10). 3117 bytes result sent to driver
17/01/13 21:53:06 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 10) in 5 ms on localhost (1/1)
17/01/13 21:53:06 INFO DAGScheduler: ResultStage 8 (take at CsvRelation.scala:249) finished in 0.006 s
17/01/13 21:53:06 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
17/01/13 21:53:06 INFO DAGScheduler: Job 6 finished: take at CsvRelation.scala:249, took 0.011421 s
17/01/13 21:53:06 INFO ParseDriver: Parsing command: SELECT * FROM  `flights`
17/01/13 21:53:06 INFO ParseDriver: Parse Completed
17/01/13 21:53:06 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 208.5 KB, free 684.0 KB)
17/01/13 21:53:06 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 19.3 KB, free 703.3 KB)
17/01/13 21:53:06 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on localhost:62261 (size: 19.3 KB, free: 511.1 MB)
17/01/13 21:53:06 INFO SparkContext: Created broadcast 13 from textFile at TextFile.scala:30
17/01/13 21:53:06 INFO FileInputFormat: Total input paths to process : 1
17/01/13 21:53:06 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/01/13 21:53:06 INFO DAGScheduler: Got job 7 (take at CsvRelation.scala:249) with 1 output partitions
17/01/13 21:53:06 INFO DAGScheduler: Final stage: ResultStage 9 (take at CsvRelation.scala:249)
17/01/13 21:53:06 INFO DAGScheduler: Parents of final stage: List()
17/01/13 21:53:06 INFO DAGScheduler: Missing parents: List()
17/01/13 21:53:06 INFO DAGScheduler: Submitting ResultStage 9 (C:\Users\Baoco\AppData\Local\Temp\RtmpUBSmNb/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv MapPartitionsRDD[35] at textFile at TextFile.scala:30), which has no missing parents
17/01/13 21:53:06 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 3.2 KB, free 706.5 KB)
17/01/13 21:53:06 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 1944.0 B, free 708.4 KB)
17/01/13 21:53:06 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on localhost:62261 (size: 1944.0 B, free: 511.1 MB)
17/01/13 21:53:06 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1006
17/01/13 21:53:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (C:\Users\Baoco\AppData\Local\Temp\RtmpUBSmNb/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv MapPartitionsRDD[35] at textFile at TextFile.scala:30)
17/01/13 21:53:06 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
17/01/13 21:53:06 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 11, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 21:53:06 INFO Executor: Running task 0.0 in stage 9.0 (TID 11)
17/01/13 21:53:06 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpUBSmNb/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv:0+16824941
17/01/13 21:53:06 INFO Executor: Finished task 0.0 in stage 9.0 (TID 11). 3117 bytes result sent to driver
17/01/13 21:53:06 INFO DAGScheduler: ResultStage 9 (take at CsvRelation.scala:249) finished in 0.008 s
17/01/13 21:53:06 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 11) in 8 ms on localhost (1/1)
17/01/13 21:53:06 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
17/01/13 21:53:06 INFO DAGScheduler: Job 7 finished: take at CsvRelation.scala:249, took 0.012854 s
17/01/13 21:53:06 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 208.5 KB, free 916.9 KB)
17/01/13 21:53:06 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 19.3 KB, free 936.3 KB)
17/01/13 21:53:06 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on localhost:62261 (size: 19.3 KB, free: 511.0 MB)
17/01/13 21:53:06 INFO SparkContext: Created broadcast 15 from textFile at TextFile.scala:30
17/01/13 21:53:06 INFO FileInputFormat: Total input paths to process : 1
17/01/13 21:53:06 INFO SparkContext: Starting job: sql at null:-2
17/01/13 21:53:06 INFO DAGScheduler: Registering RDD 45 (sql at null:-2)
17/01/13 21:53:06 INFO DAGScheduler: Got job 8 (sql at null:-2) with 1 output partitions
17/01/13 21:53:06 INFO DAGScheduler: Final stage: ResultStage 11 (sql at null:-2)
17/01/13 21:53:06 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
17/01/13 21:53:06 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 10)
17/01/13 21:53:06 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[45] at sql at null:-2), which has no missing parents
17/01/13 21:53:06 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 26.8 KB, free 963.1 KB)
17/01/13 21:53:06 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 11.0 KB, free 974.1 KB)
17/01/13 21:53:06 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on localhost:62261 (size: 11.0 KB, free: 511.0 MB)
17/01/13 21:53:06 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1006
17/01/13 21:53:06 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[45] at sql at null:-2)
17/01/13 21:53:06 INFO TaskSchedulerImpl: Adding task set 10.0 with 2 tasks
17/01/13 21:53:06 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 12, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 21:53:06 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 13, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 21:53:06 INFO Executor: Running task 0.0 in stage 10.0 (TID 12)
17/01/13 21:53:06 INFO Executor: Running task 1.0 in stage 10.0 (TID 13)
17/01/13 21:53:06 INFO CacheManager: Partition rdd_42_0 not found, computing it
17/01/13 21:53:06 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpUBSmNb/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv:0+16824941
17/01/13 21:53:06 INFO CacheManager: Partition rdd_42_1 not found, computing it
17/01/13 21:53:06 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpUBSmNb/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv:16824941+16824942
17/01/13 21:53:06 INFO GenerateUnsafeProjection: Code generated in 25.374698 ms
17/01/13 21:53:07 INFO BlockManagerInfo: Removed broadcast_14_piece0 on localhost:62261 in memory (size: 1944.0 B, free: 511.0 MB)
17/01/13 21:53:07 INFO ContextCleaner: Cleaned accumulator 30
17/01/13 21:53:07 INFO BlockManagerInfo: Removed broadcast_13_piece0 on localhost:62261 in memory (size: 19.3 KB, free: 511.0 MB)
17/01/13 21:53:07 INFO BlockManagerInfo: Removed broadcast_12_piece0 on localhost:62261 in memory (size: 1944.0 B, free: 511.0 MB)
17/01/13 21:53:07 INFO ContextCleaner: Cleaned accumulator 29
17/01/13 21:53:07 INFO BlockManagerInfo: Removed broadcast_11_piece0 on localhost:62261 in memory (size: 19.3 KB, free: 511.1 MB)
17/01/13 21:53:07 INFO BlockManagerInfo: Removed broadcast_10_piece0 on localhost:62261 in memory (size: 3.0 KB, free: 511.1 MB)
17/01/13 21:53:07 INFO ContextCleaner: Cleaned accumulator 28
17/01/13 21:53:07 INFO ContextCleaner: Cleaned accumulator 27
17/01/13 21:53:11 INFO MemoryStore: Block rdd_42_1 stored as values in memory (estimated size 12.2 MB, free 12.7 MB)
17/01/13 21:53:11 INFO BlockManagerInfo: Added rdd_42_1 in memory on localhost:62261 (size: 12.2 MB, free: 498.9 MB)
17/01/13 21:53:11 INFO MemoryStore: Block rdd_42_0 stored as values in memory (estimated size 12.2 MB, free 24.9 MB)
17/01/13 21:53:11 INFO BlockManagerInfo: Added rdd_42_0 in memory on localhost:62261 (size: 12.2 MB, free: 486.7 MB)
17/01/13 21:53:11 INFO Executor: Finished task 1.0 in stage 10.0 (TID 13). 21132 bytes result sent to driver
17/01/13 21:53:11 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 13) in 5316 ms on localhost (1/2)
17/01/13 21:53:11 INFO Executor: Finished task 0.0 in stage 10.0 (TID 12). 21077 bytes result sent to driver
17/01/13 21:53:11 INFO DAGScheduler: ShuffleMapStage 10 (sql at null:-2) finished in 5.333 s
17/01/13 21:53:11 INFO DAGScheduler: looking for newly runnable stages
17/01/13 21:53:11 INFO DAGScheduler: running: Set()
17/01/13 21:53:11 INFO DAGScheduler: waiting: Set(ResultStage 11)
17/01/13 21:53:11 INFO DAGScheduler: failed: Set()
17/01/13 21:53:11 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[48] at sql at null:-2), which has no missing parents
17/01/13 21:53:11 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 9.3 KB, free 24.9 MB)
17/01/13 21:53:11 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 12) in 5332 ms on localhost (2/2)
17/01/13 21:53:11 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
17/01/13 21:53:11 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 4.6 KB, free 24.9 MB)
17/01/13 21:53:11 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on localhost:62261 (size: 4.6 KB, free: 486.7 MB)
17/01/13 21:53:11 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1006
17/01/13 21:53:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[48] at sql at null:-2)
17/01/13 21:53:11 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks
17/01/13 21:53:11 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 14, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 21:53:11 INFO Executor: Running task 0.0 in stage 11.0 (TID 14)
17/01/13 21:53:11 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 21:53:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/01/13 21:53:11 INFO Executor: Finished task 0.0 in stage 11.0 (TID 14). 1830 bytes result sent to driver
17/01/13 21:53:11 INFO DAGScheduler: ResultStage 11 (sql at null:-2) finished in 0.014 s
17/01/13 21:53:11 INFO DAGScheduler: Job 8 finished: sql at null:-2, took 5.362539 s
17/01/13 21:53:11 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 14) in 13 ms on localhost (1/1)
17/01/13 21:53:11 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
17/01/13 21:53:11 INFO ParseDriver: Parsing command: SELECT count(*) FROM  `flights`
17/01/13 21:53:11 INFO ParseDriver: Parse Completed
17/01/13 21:53:12 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 21:53:12 INFO DAGScheduler: Registering RDD 52 (collect at utils.scala:195)
17/01/13 21:53:12 INFO DAGScheduler: Got job 9 (collect at utils.scala:195) with 1 output partitions
17/01/13 21:53:12 INFO DAGScheduler: Final stage: ResultStage 13 (collect at utils.scala:195)
17/01/13 21:53:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)
17/01/13 21:53:12 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 12)
17/01/13 21:53:12 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[52] at collect at utils.scala:195), which has no missing parents
17/01/13 21:53:12 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 26.9 KB, free 24.9 MB)
17/01/13 21:53:12 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 11.0 KB, free 24.9 MB)
17/01/13 21:53:12 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on localhost:62261 (size: 11.0 KB, free: 486.7 MB)
17/01/13 21:53:12 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1006
17/01/13 21:53:12 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[52] at collect at utils.scala:195)
17/01/13 21:53:12 INFO TaskSchedulerImpl: Adding task set 12.0 with 2 tasks
17/01/13 21:53:12 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 15, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 21:53:12 INFO TaskSetManager: Starting task 1.0 in stage 12.0 (TID 16, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 21:53:12 INFO Executor: Running task 0.0 in stage 12.0 (TID 15)
17/01/13 21:53:12 INFO Executor: Running task 1.0 in stage 12.0 (TID 16)
17/01/13 21:53:12 INFO BlockManager: Found block rdd_42_1 locally
17/01/13 21:53:12 INFO BlockManager: Found block rdd_42_0 locally
17/01/13 21:53:12 INFO Executor: Finished task 0.0 in stage 12.0 (TID 15). 2679 bytes result sent to driver
17/01/13 21:53:12 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 15) in 39 ms on localhost (1/2)
17/01/13 21:53:12 INFO Executor: Finished task 1.0 in stage 12.0 (TID 16). 2679 bytes result sent to driver
17/01/13 21:53:12 INFO DAGScheduler: ShuffleMapStage 12 (collect at utils.scala:195) finished in 0.047 s
17/01/13 21:53:12 INFO TaskSetManager: Finished task 1.0 in stage 12.0 (TID 16) in 46 ms on localhost (2/2)
17/01/13 21:53:12 INFO DAGScheduler: looking for newly runnable stages
17/01/13 21:53:12 INFO DAGScheduler: running: Set()
17/01/13 21:53:12 INFO DAGScheduler: waiting: Set(ResultStage 13)
17/01/13 21:53:12 INFO DAGScheduler: failed: Set()
17/01/13 21:53:12 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
17/01/13 21:53:12 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[55] at collect at utils.scala:195), which has no missing parents
17/01/13 21:53:12 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 9.4 KB, free 24.9 MB)
17/01/13 21:53:12 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 4.6 KB, free 24.9 MB)
17/01/13 21:53:12 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on localhost:62261 (size: 4.6 KB, free: 486.7 MB)
17/01/13 21:53:12 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1006
17/01/13 21:53:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[55] at collect at utils.scala:195)
17/01/13 21:53:12 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks
17/01/13 21:53:12 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 17, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 21:53:12 INFO Executor: Running task 0.0 in stage 13.0 (TID 17)
17/01/13 21:53:12 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 21:53:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/01/13 21:53:12 INFO Executor: Finished task 0.0 in stage 13.0 (TID 17). 1830 bytes result sent to driver
17/01/13 21:53:12 INFO DAGScheduler: ResultStage 13 (collect at utils.scala:195) finished in 0.018 s
17/01/13 21:53:12 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 17) in 17 ms on localhost (1/1)
17/01/13 21:53:12 INFO DAGScheduler: Job 9 finished: collect at utils.scala:195, took 0.083856 s
17/01/13 21:53:12 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
17/01/13 21:53:12 INFO ParseDriver: Parsing command: SELECT *
FROM `flights` AS `zzz2`
WHERE (0 = 1)
17/01/13 21:53:12 INFO ParseDriver: Parse Completed
17/01/13 21:53:12 INFO BlockManagerInfo: Removed broadcast_19_piece0 on localhost:62261 in memory (size: 4.6 KB, free: 486.7 MB)
17/01/13 21:53:12 INFO BlockManagerInfo: Removed broadcast_17_piece0 on localhost:62261 in memory (size: 4.6 KB, free: 486.7 MB)
17/01/13 21:53:12 INFO ContextCleaner: Cleaned accumulator 51
17/01/13 21:53:12 INFO BlockManagerInfo: Removed broadcast_18_piece0 on localhost:62261 in memory (size: 11.0 KB, free: 486.7 MB)
17/01/13 21:53:12 INFO ContextCleaner: Cleaned accumulator 50
17/01/13 21:53:12 INFO ContextCleaner: Cleaned shuffle 3
17/01/13 21:53:12 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/01/13 21:53:12 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/01/13 21:53:12 INFO SparkContext: Starting job: collect at utils.scala:59
17/01/13 21:53:12 INFO DAGScheduler: Got job 10 (collect at utils.scala:59) with 1 output partitions
17/01/13 21:53:12 INFO DAGScheduler: Final stage: ResultStage 14 (collect at utils.scala:59)
17/01/13 21:53:12 INFO DAGScheduler: Parents of final stage: List()
17/01/13 21:53:12 INFO DAGScheduler: Missing parents: List()
17/01/13 21:53:12 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[59] at map at utils.scala:56), which has no missing parents
17/01/13 21:53:12 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 5.4 KB, free 24.9 MB)
17/01/13 21:53:12 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 3.0 KB, free 24.9 MB)
17/01/13 21:53:12 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on localhost:62261 (size: 3.0 KB, free: 486.7 MB)
17/01/13 21:53:12 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1006
17/01/13 21:53:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[59] at map at utils.scala:56)
17/01/13 21:53:12 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks
17/01/13 21:53:12 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 18, localhost, partition 0,PROCESS_LOCAL, 2687 bytes)
17/01/13 21:53:12 INFO Executor: Running task 0.0 in stage 14.0 (TID 18)
17/01/13 21:53:12 INFO Executor: Finished task 0.0 in stage 14.0 (TID 18). 1077 bytes result sent to driver
17/01/13 21:53:12 INFO DAGScheduler: ResultStage 14 (collect at utils.scala:59) finished in 0.006 s
17/01/13 21:53:12 INFO DAGScheduler: Job 10 finished: collect at utils.scala:59, took 0.011436 s
17/01/13 21:53:12 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 18) in 6 ms on localhost (1/1)
17/01/13 21:53:12 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
17/01/13 21:53:14 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 208.5 KB, free 25.1 MB)
17/01/13 21:53:14 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 19.3 KB, free 25.1 MB)
17/01/13 21:53:14 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on localhost:62261 (size: 19.3 KB, free: 486.7 MB)
17/01/13 21:53:14 INFO SparkContext: Created broadcast 21 from textFile at TextFile.scala:30
17/01/13 21:53:14 INFO FileInputFormat: Total input paths to process : 1
17/01/13 21:53:14 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/01/13 21:53:14 INFO DAGScheduler: Got job 11 (take at CsvRelation.scala:249) with 1 output partitions
17/01/13 21:53:14 INFO DAGScheduler: Final stage: ResultStage 15 (take at CsvRelation.scala:249)
17/01/13 21:53:14 INFO DAGScheduler: Parents of final stage: List()
17/01/13 21:53:14 INFO DAGScheduler: Missing parents: List()
17/01/13 21:53:14 INFO DAGScheduler: Submitting ResultStage 15 (C:\Users\Baoco\AppData\Local\Temp\RtmpUBSmNb/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv MapPartitionsRDD[61] at textFile at TextFile.scala:30), which has no missing parents
17/01/13 21:53:14 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 3.2 KB, free 25.1 MB)
17/01/13 21:53:14 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 1944.0 B, free 25.1 MB)
17/01/13 21:53:14 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on localhost:62261 (size: 1944.0 B, free: 486.7 MB)
17/01/13 21:53:14 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1006
17/01/13 21:53:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (C:\Users\Baoco\AppData\Local\Temp\RtmpUBSmNb/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv MapPartitionsRDD[61] at textFile at TextFile.scala:30)
17/01/13 21:53:14 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks
17/01/13 21:53:14 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 19, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 21:53:14 INFO Executor: Running task 0.0 in stage 15.0 (TID 19)
17/01/13 21:53:14 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpUBSmNb/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv:0+3427780
17/01/13 21:53:14 INFO Executor: Finished task 0.0 in stage 15.0 (TID 19). 2768 bytes result sent to driver
17/01/13 21:53:14 INFO DAGScheduler: ResultStage 15 (take at CsvRelation.scala:249) finished in 0.014 s
17/01/13 21:53:14 INFO DAGScheduler: Job 11 finished: take at CsvRelation.scala:249, took 0.019220 s
17/01/13 21:53:14 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 19) in 14 ms on localhost (1/1)
17/01/13 21:53:14 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
17/01/13 21:53:14 INFO ParseDriver: Parsing command: SELECT * FROM  `batting`
17/01/13 21:53:14 INFO ParseDriver: Parse Completed
17/01/13 21:53:14 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 208.5 KB, free 25.3 MB)
17/01/13 21:53:14 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 19.3 KB, free 25.3 MB)
17/01/13 21:53:14 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on localhost:62261 (size: 19.3 KB, free: 486.7 MB)
17/01/13 21:53:14 INFO SparkContext: Created broadcast 23 from textFile at TextFile.scala:30
17/01/13 21:53:14 INFO FileInputFormat: Total input paths to process : 1
17/01/13 21:53:14 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/01/13 21:53:14 INFO DAGScheduler: Got job 12 (take at CsvRelation.scala:249) with 1 output partitions
17/01/13 21:53:14 INFO DAGScheduler: Final stage: ResultStage 16 (take at CsvRelation.scala:249)
17/01/13 21:53:14 INFO DAGScheduler: Parents of final stage: List()
17/01/13 21:53:14 INFO DAGScheduler: Missing parents: List()
17/01/13 21:53:14 INFO DAGScheduler: Submitting ResultStage 16 (C:\Users\Baoco\AppData\Local\Temp\RtmpUBSmNb/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv MapPartitionsRDD[63] at textFile at TextFile.scala:30), which has no missing parents
17/01/13 21:53:14 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 3.2 KB, free 25.3 MB)
17/01/13 21:53:14 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 1944.0 B, free 25.3 MB)
17/01/13 21:53:14 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on localhost:62261 (size: 1944.0 B, free: 486.7 MB)
17/01/13 21:53:14 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1006
17/01/13 21:53:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (C:\Users\Baoco\AppData\Local\Temp\RtmpUBSmNb/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv MapPartitionsRDD[63] at textFile at TextFile.scala:30)
17/01/13 21:53:14 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks
17/01/13 21:53:14 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 20, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 21:53:14 INFO Executor: Running task 0.0 in stage 16.0 (TID 20)
17/01/13 21:53:14 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpUBSmNb/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv:0+3427780
17/01/13 21:53:14 INFO Executor: Finished task 0.0 in stage 16.0 (TID 20). 2768 bytes result sent to driver
17/01/13 21:53:14 INFO DAGScheduler: ResultStage 16 (take at CsvRelation.scala:249) finished in 0.007 s
17/01/13 21:53:14 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 20) in 7 ms on localhost (1/1)
17/01/13 21:53:14 INFO DAGScheduler: Job 12 finished: take at CsvRelation.scala:249, took 0.011621 s
17/01/13 21:53:14 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
17/01/13 21:53:14 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 208.5 KB, free 25.5 MB)
17/01/13 21:53:14 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 19.3 KB, free 25.5 MB)
17/01/13 21:53:14 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on localhost:62261 (size: 19.3 KB, free: 486.6 MB)
17/01/13 21:53:14 INFO SparkContext: Created broadcast 25 from textFile at TextFile.scala:30
17/01/13 21:53:14 INFO FileInputFormat: Total input paths to process : 1
17/01/13 21:53:14 INFO SparkContext: Starting job: sql at null:-2
17/01/13 21:53:14 INFO DAGScheduler: Registering RDD 73 (sql at null:-2)
17/01/13 21:53:14 INFO DAGScheduler: Got job 13 (sql at null:-2) with 1 output partitions
17/01/13 21:53:14 INFO DAGScheduler: Final stage: ResultStage 18 (sql at null:-2)
17/01/13 21:53:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)
17/01/13 21:53:14 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 17)
17/01/13 21:53:14 INFO DAGScheduler: Submitting ShuffleMapStage 17 (MapPartitionsRDD[73] at sql at null:-2), which has no missing parents
17/01/13 21:53:14 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 27.9 KB, free 25.6 MB)
17/01/13 21:53:14 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 11.2 KB, free 25.6 MB)
17/01/13 21:53:14 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on localhost:62261 (size: 11.2 KB, free: 486.6 MB)
17/01/13 21:53:14 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1006
17/01/13 21:53:14 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[73] at sql at null:-2)
17/01/13 21:53:14 INFO TaskSchedulerImpl: Adding task set 17.0 with 2 tasks
17/01/13 21:53:14 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 21, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 21:53:14 INFO TaskSetManager: Starting task 1.0 in stage 17.0 (TID 22, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 21:53:14 INFO Executor: Running task 0.0 in stage 17.0 (TID 21)
17/01/13 21:53:14 INFO Executor: Running task 1.0 in stage 17.0 (TID 22)
17/01/13 21:53:14 INFO CacheManager: Partition rdd_70_1 not found, computing it
17/01/13 21:53:14 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpUBSmNb/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv:3427780+3427781
17/01/13 21:53:14 INFO CacheManager: Partition rdd_70_0 not found, computing it
17/01/13 21:53:14 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpUBSmNb/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv:0+3427780
17/01/13 21:53:14 INFO GenerateUnsafeProjection: Code generated in 32.863546 ms
17/01/13 21:53:14 INFO BlockManagerInfo: Removed broadcast_24_piece0 on localhost:62261 in memory (size: 1944.0 B, free: 486.6 MB)
17/01/13 21:53:14 INFO ContextCleaner: Cleaned accumulator 55
17/01/13 21:53:14 INFO BlockManagerInfo: Removed broadcast_23_piece0 on localhost:62261 in memory (size: 19.3 KB, free: 486.6 MB)
17/01/13 21:53:14 INFO BlockManagerInfo: Removed broadcast_22_piece0 on localhost:62261 in memory (size: 1944.0 B, free: 486.6 MB)
17/01/13 21:53:14 INFO ContextCleaner: Cleaned accumulator 54
17/01/13 21:53:14 INFO BlockManagerInfo: Removed broadcast_21_piece0 on localhost:62261 in memory (size: 19.3 KB, free: 486.7 MB)
17/01/13 21:53:14 INFO BlockManagerInfo: Removed broadcast_20_piece0 on localhost:62261 in memory (size: 3.0 KB, free: 486.7 MB)
17/01/13 21:53:14 INFO ContextCleaner: Cleaned accumulator 53
17/01/13 21:53:14 INFO ContextCleaner: Cleaned accumulator 52
17/01/13 21:53:16 INFO MemoryStore: Block rdd_70_0 stored as values in memory (estimated size 1897.3 KB, free 27.0 MB)
17/01/13 21:53:16 INFO BlockManagerInfo: Added rdd_70_0 in memory on localhost:62261 (size: 1897.3 KB, free: 484.8 MB)
17/01/13 21:53:16 INFO MemoryStore: Block rdd_70_1 stored as values in memory (estimated size 1699.8 KB, free 28.6 MB)
17/01/13 21:53:16 INFO BlockManagerInfo: Added rdd_70_1 in memory on localhost:62261 (size: 1699.8 KB, free: 483.2 MB)
17/01/13 21:53:16 INFO Executor: Finished task 0.0 in stage 17.0 (TID 21). 9713 bytes result sent to driver
17/01/13 21:53:16 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 21) in 1477 ms on localhost (1/2)
17/01/13 21:53:16 INFO Executor: Finished task 1.0 in stage 17.0 (TID 22). 9862 bytes result sent to driver
17/01/13 21:53:16 INFO DAGScheduler: ShuffleMapStage 17 (sql at null:-2) finished in 1.495 s
17/01/13 21:53:16 INFO DAGScheduler: looking for newly runnable stages
17/01/13 21:53:16 INFO DAGScheduler: running: Set()
17/01/13 21:53:16 INFO DAGScheduler: waiting: Set(ResultStage 18)
17/01/13 21:53:16 INFO DAGScheduler: failed: Set()
17/01/13 21:53:16 INFO TaskSetManager: Finished task 1.0 in stage 17.0 (TID 22) in 1493 ms on localhost (2/2)
17/01/13 21:53:16 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
17/01/13 21:53:16 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[76] at sql at null:-2), which has no missing parents
17/01/13 21:53:16 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 9.3 KB, free 28.6 MB)
17/01/13 21:53:16 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 4.6 KB, free 28.6 MB)
17/01/13 21:53:16 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on localhost:62261 (size: 4.6 KB, free: 483.2 MB)
17/01/13 21:53:16 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1006
17/01/13 21:53:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[76] at sql at null:-2)
17/01/13 21:53:16 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks
17/01/13 21:53:16 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 23, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 21:53:16 INFO Executor: Running task 0.0 in stage 18.0 (TID 23)
17/01/13 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 21:53:16 INFO Executor: Finished task 0.0 in stage 18.0 (TID 23). 1830 bytes result sent to driver
17/01/13 21:53:16 INFO DAGScheduler: ResultStage 18 (sql at null:-2) finished in 0.011 s
17/01/13 21:53:16 INFO DAGScheduler: Job 13 finished: sql at null:-2, took 1.518695 s
17/01/13 21:53:16 INFO ParseDriver: Parsing command: SELECT count(*) FROM  `batting`
17/01/13 21:53:16 INFO ParseDriver: Parse Completed
17/01/13 21:53:16 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 23) in 10 ms on localhost (1/1)
17/01/13 21:53:16 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
17/01/13 21:53:16 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 21:53:16 INFO DAGScheduler: Registering RDD 80 (collect at utils.scala:195)
17/01/13 21:53:16 INFO DAGScheduler: Got job 14 (collect at utils.scala:195) with 1 output partitions
17/01/13 21:53:16 INFO DAGScheduler: Final stage: ResultStage 20 (collect at utils.scala:195)
17/01/13 21:53:16 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)
17/01/13 21:53:16 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 19)
17/01/13 21:53:16 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[80] at collect at utils.scala:195), which has no missing parents
17/01/13 21:53:16 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 28.0 KB, free 28.7 MB)
17/01/13 21:53:16 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 11.2 KB, free 28.7 MB)
17/01/13 21:53:16 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on localhost:62261 (size: 11.2 KB, free: 483.1 MB)
17/01/13 21:53:16 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1006
17/01/13 21:53:16 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[80] at collect at utils.scala:195)
17/01/13 21:53:16 INFO TaskSchedulerImpl: Adding task set 19.0 with 2 tasks
17/01/13 21:53:16 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 24, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 21:53:16 INFO TaskSetManager: Starting task 1.0 in stage 19.0 (TID 25, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 21:53:16 INFO Executor: Running task 0.0 in stage 19.0 (TID 24)
17/01/13 21:53:16 INFO Executor: Running task 1.0 in stage 19.0 (TID 25)
17/01/13 21:53:16 INFO BlockManager: Found block rdd_70_0 locally
17/01/13 21:53:16 INFO BlockManager: Found block rdd_70_1 locally
17/01/13 21:53:16 INFO Executor: Finished task 0.0 in stage 19.0 (TID 24). 2679 bytes result sent to driver
17/01/13 21:53:16 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 24) in 23 ms on localhost (1/2)
17/01/13 21:53:16 INFO Executor: Finished task 1.0 in stage 19.0 (TID 25). 2679 bytes result sent to driver
17/01/13 21:53:16 INFO DAGScheduler: ShuffleMapStage 19 (collect at utils.scala:195) finished in 0.025 s
17/01/13 21:53:16 INFO DAGScheduler: looking for newly runnable stages
17/01/13 21:53:16 INFO DAGScheduler: running: Set()
17/01/13 21:53:16 INFO DAGScheduler: waiting: Set(ResultStage 20)
17/01/13 21:53:16 INFO DAGScheduler: failed: Set()
17/01/13 21:53:16 INFO TaskSetManager: Finished task 1.0 in stage 19.0 (TID 25) in 25 ms on localhost (2/2)
17/01/13 21:53:16 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
17/01/13 21:53:16 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[83] at collect at utils.scala:195), which has no missing parents
17/01/13 21:53:16 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 9.4 KB, free 28.7 MB)
17/01/13 21:53:16 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 4.7 KB, free 28.7 MB)
17/01/13 21:53:16 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on localhost:62261 (size: 4.7 KB, free: 483.1 MB)
17/01/13 21:53:16 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1006
17/01/13 21:53:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[83] at collect at utils.scala:195)
17/01/13 21:53:16 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks
17/01/13 21:53:16 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 26, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 21:53:16 INFO Executor: Running task 0.0 in stage 20.0 (TID 26)
17/01/13 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 21:53:16 INFO Executor: Finished task 0.0 in stage 20.0 (TID 26). 1830 bytes result sent to driver
17/01/13 21:53:16 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 26) in 9 ms on localhost (1/1)
17/01/13 21:53:16 INFO DAGScheduler: ResultStage 20 (collect at utils.scala:195) finished in 0.010 s
17/01/13 21:53:16 INFO DAGScheduler: Job 14 finished: collect at utils.scala:195, took 0.044962 s
17/01/13 21:53:16 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
17/01/13 21:53:16 INFO ParseDriver: Parsing command: SELECT *
FROM `batting` AS `zzz3`
WHERE (0 = 1)
17/01/13 21:53:16 INFO ParseDriver: Parse Completed
17/01/13 21:53:16 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/01/13 21:53:16 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/01/13 21:53:16 INFO SparkContext: Starting job: collect at utils.scala:59
17/01/13 21:53:16 INFO DAGScheduler: Got job 15 (collect at utils.scala:59) with 1 output partitions
17/01/13 21:53:16 INFO DAGScheduler: Final stage: ResultStage 21 (collect at utils.scala:59)
17/01/13 21:53:16 INFO DAGScheduler: Parents of final stage: List()
17/01/13 21:53:16 INFO DAGScheduler: Missing parents: List()
17/01/13 21:53:16 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[87] at map at utils.scala:56), which has no missing parents
17/01/13 21:53:16 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 5.4 KB, free 28.7 MB)
17/01/13 21:53:16 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 3.0 KB, free 28.7 MB)
17/01/13 21:53:16 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on localhost:62261 (size: 3.0 KB, free: 483.1 MB)
17/01/13 21:53:16 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1006
17/01/13 21:53:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[87] at map at utils.scala:56)
17/01/13 21:53:16 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks
17/01/13 21:53:16 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 27, localhost, partition 0,PROCESS_LOCAL, 2728 bytes)
17/01/13 21:53:16 INFO Executor: Running task 0.0 in stage 21.0 (TID 27)
17/01/13 21:53:16 INFO Executor: Finished task 0.0 in stage 21.0 (TID 27). 1087 bytes result sent to driver
17/01/13 21:53:16 INFO DAGScheduler: ResultStage 21 (collect at utils.scala:59) finished in 0.006 s
17/01/13 21:53:16 INFO DAGScheduler: Job 15 finished: collect at utils.scala:59, took 0.014459 s
17/01/13 21:53:16 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 27) in 5 ms on localhost (1/1)
17/01/13 21:53:16 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
17/01/13 21:53:18 INFO SparkContext: Invoking stop() from shutdown hook
17/01/13 21:53:18 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
17/01/13 21:53:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/01/13 21:53:18 INFO MemoryStore: MemoryStore cleared
17/01/13 21:53:18 INFO BlockManager: BlockManager stopped
17/01/13 21:53:18 INFO BlockManagerMaster: BlockManagerMaster stopped
17/01/13 21:53:18 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/01/13 21:53:18 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\Baoco\AppData\Local\Temp\spark-a9fd84ba-89b7-4317-8ab0-77a8e6700683\userFiles-4f656c1e-c2ed-44ed-9c30-6c02eec74b09
java.io.IOException: Failed to delete: C:\Users\Baoco\AppData\Local\Temp\spark-a9fd84ba-89b7-4317-8ab0-77a8e6700683\userFiles-4f656c1e-c2ed-44ed-9c30-6c02eec74b09
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:119)
	at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1219)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1755)
	at org.apache.spark.SparkContext$$anonfun$3.apply$mcV$sp(SparkContext.scala:596)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/01/13 21:53:18 INFO SparkContext: Successfully stopped SparkContext
17/01/13 21:53:18 INFO ShutdownHookManager: Shutdown hook called
17/01/13 21:53:18 INFO ShutdownHookManager: Deleting directory C:\Users\Baoco\AppData\Local\Temp\spark-a9fd84ba-89b7-4317-8ab0-77a8e6700683
17/01/13 21:53:18 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
17/01/13 21:53:18 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
17/01/13 21:53:18 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\Baoco\AppData\Local\Temp\spark-a9fd84ba-89b7-4317-8ab0-77a8e6700683
java.io.IOException: Failed to delete: C:\Users\Baoco\AppData\Local\Temp\spark-a9fd84ba-89b7-4317-8ab0-77a8e6700683
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/01/13 21:53:18 INFO ShutdownHookManager: Deleting directory C:\Users\Baoco\AppData\Local\Temp\spark-1292b310-f09c-4313-92fe-d3eb380a930e
17/01/13 21:53:18 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
17/01/13 21:53:18 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\Baoco\AppData\Local\Temp\spark-1292b310-f09c-4313-92fe-d3eb380a930e
java.io.IOException: Failed to delete: C:\Users\Baoco\AppData\Local\Temp\spark-1292b310-f09c-4313-92fe-d3eb380a930e
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/01/13 21:53:18 INFO ShutdownHookManager: Deleting directory C:\Users\Baoco\AppData\Local\Temp\spark-a9fd84ba-89b7-4317-8ab0-77a8e6700683\userFiles-4f656c1e-c2ed-44ed-9c30-6c02eec74b09
17/01/13 21:53:18 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\Baoco\AppData\Local\Temp\spark-a9fd84ba-89b7-4317-8ab0-77a8e6700683\userFiles-4f656c1e-c2ed-44ed-9c30-6c02eec74b09
java.io.IOException: Failed to delete: C:\Users\Baoco\AppData\Local\Temp\spark-a9fd84ba-89b7-4317-8ab0-77a8e6700683\userFiles-4f656c1e-c2ed-44ed-9c30-6c02eec74b09
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/01/13 21:53:18 INFO ShutdownHookManager: Deleting directory C:\Users\Baoco\AppData\Local\Temp\spark-a9fd84ba-89b7-4317-8ab0-77a8e6700683\httpd-2326e080-bf13-45bb-abd1-ce7b31e13c8c
17/01/13 22:37:53 INFO SparkContext: Running Spark version 1.6.2
17/01/13 22:37:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/01/13 22:37:54 INFO SecurityManager: Changing view acls to: Baoco
17/01/13 22:37:54 INFO SecurityManager: Changing modify acls to: Baoco
17/01/13 22:37:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(Baoco); users with modify permissions: Set(Baoco)
17/01/13 22:37:54 INFO Utils: Successfully started service 'sparkDriver' on port 62894.
17/01/13 22:37:55 INFO Slf4jLogger: Slf4jLogger started
17/01/13 22:37:55 INFO Remoting: Starting remoting
17/01/13 22:37:55 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@127.0.0.1:62907]
17/01/13 22:37:55 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 62907.
17/01/13 22:37:55 INFO SparkEnv: Registering MapOutputTracker
17/01/13 22:37:55 INFO SparkEnv: Registering BlockManagerMaster
17/01/13 22:37:55 INFO DiskBlockManager: Created local directory at C:\Users\Baoco\AppData\Local\Temp\blockmgr-b1f135f7-fa44-42c5-8b1b-b390a16cc0d1
17/01/13 22:37:55 INFO MemoryStore: MemoryStore started with capacity 511.1 MB
17/01/13 22:37:55 INFO SparkEnv: Registering OutputCommitCoordinator
17/01/13 22:37:55 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/01/13 22:37:55 INFO SparkUI: Started SparkUI at http://127.0.0.1:4040
17/01/13 22:37:55 INFO HttpFileServer: HTTP File server directory is C:\Users\Baoco\AppData\Local\Temp\spark-c537a403-ca67-411f-bcab-27aab16e4c1e\httpd-f5ab8bfd-8903-4b03-9f58-a6b9dc4233a9
17/01/13 22:37:55 INFO HttpServer: Starting HTTP Server
17/01/13 22:37:55 INFO Utils: Successfully started service 'HTTP file server' on port 62911.
17/01/13 22:37:55 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/spark-csv_2.11-1.3.0.jar at http://127.0.0.1:62911/jars/spark-csv_2.11-1.3.0.jar with timestamp 1484375875866
17/01/13 22:37:55 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/commons-csv-1.1.jar at http://127.0.0.1:62911/jars/commons-csv-1.1.jar with timestamp 1484375875868
17/01/13 22:37:55 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/univocity-parsers-1.5.1.jar at http://127.0.0.1:62911/jars/univocity-parsers-1.5.1.jar with timestamp 1484375875873
17/01/13 22:37:55 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/sparklyr-1.6-2.10.jar at http://127.0.0.1:62911/jars/sparklyr-1.6-2.10.jar with timestamp 1484375875875
17/01/13 22:37:55 INFO Executor: Starting executor ID driver on host localhost
17/01/13 22:37:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62928.
17/01/13 22:37:55 INFO NettyBlockTransferService: Server created on 62928
17/01/13 22:37:55 INFO BlockManagerMaster: Trying to register BlockManager
17/01/13 22:37:55 INFO BlockManagerMasterEndpoint: Registering block manager localhost:62928 with 511.1 MB RAM, BlockManagerId(driver, localhost, 62928)
17/01/13 22:37:55 INFO BlockManagerMaster: Registered BlockManager
17/01/13 22:37:57 INFO HiveContext: Initializing execution hive, version 1.2.1
17/01/13 22:37:57 INFO ClientWrapper: Inspected Hadoop version: 2.6.0
17/01/13 22:37:57 INFO ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
17/01/13 22:37:57 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/01/13 22:37:57 INFO ObjectStore: ObjectStore, initialize called
17/01/13 22:37:57 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/01/13 22:37:57 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/01/13 22:37:57 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/01/13 22:37:58 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/01/13 22:38:05 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/01/13 22:38:06 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 22:38:06 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 22:38:12 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 22:38:12 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 22:38:15 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/01/13 22:38:15 INFO ObjectStore: Initialized ObjectStore
17/01/13 22:38:15 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/01/13 22:38:16 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/01/13 22:38:21 INFO HiveMetaStore: Added admin role in metastore
17/01/13 22:38:21 INFO HiveMetaStore: Added public role in metastore
17/01/13 22:38:22 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/01/13 22:38:22 INFO HiveMetaStore: 0: get_all_databases
17/01/13 22:38:22 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_all_databases	
17/01/13 22:38:22 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/01/13 22:38:22 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/01/13 22:38:22 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 22:38:23 INFO SessionState: Created local directory: C:/Users/Baoco/AppData/Local/Temp/147a30b2-fbf5-4453-88f4-b098133f4886_resources
17/01/13 22:38:23 INFO SessionState: Created HDFS directory: C:/Users/Baoco/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/Baoco/147a30b2-fbf5-4453-88f4-b098133f4886
17/01/13 22:38:23 INFO SessionState: Created local directory: C:/Users/Baoco/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/147a30b2-fbf5-4453-88f4-b098133f4886
17/01/13 22:38:23 INFO SessionState: Created HDFS directory: C:/Users/Baoco/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/Baoco/147a30b2-fbf5-4453-88f4-b098133f4886/_tmp_space.db
17/01/13 22:38:23 INFO HiveContext: default warehouse location is C:\Users\Baoco\AppData\Local\rstudio\spark\Cache\spark-1.6.2-bin-hadoop2.6\tmp\hive
17/01/13 22:38:23 INFO HiveContext: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/01/13 22:38:23 INFO ClientWrapper: Inspected Hadoop version: 2.6.0
17/01/13 22:38:23 INFO ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
17/01/13 22:38:24 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/01/13 22:38:24 INFO ObjectStore: ObjectStore, initialize called
17/01/13 22:38:24 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/01/13 22:38:24 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/01/13 22:38:24 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/01/13 22:38:24 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/01/13 22:38:25 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/01/13 22:38:26 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 22:38:26 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 22:38:26 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 22:38:26 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 22:38:27 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/01/13 22:38:27 INFO ObjectStore: Initialized ObjectStore
17/01/13 22:38:27 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/01/13 22:38:27 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/01/13 22:38:27 INFO HiveMetaStore: Added admin role in metastore
17/01/13 22:38:27 INFO HiveMetaStore: Added public role in metastore
17/01/13 22:38:27 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/01/13 22:38:27 INFO HiveMetaStore: 0: get_all_databases
17/01/13 22:38:27 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_all_databases	
17/01/13 22:38:27 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/01/13 22:38:27 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/01/13 22:38:27 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 22:38:27 INFO SessionState: Created local directory: C:/Users/Baoco/AppData/Local/Temp/33e954c9-297b-44d5-a988-0b26a98d05ff_resources
17/01/13 22:38:27 INFO SessionState: Created HDFS directory: C:/Users/Baoco/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/Baoco/33e954c9-297b-44d5-a988-0b26a98d05ff
17/01/13 22:38:27 INFO SessionState: Created local directory: C:/Users/Baoco/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/33e954c9-297b-44d5-a988-0b26a98d05ff
17/01/13 22:38:27 INFO SessionState: Created HDFS directory: C:/Users/Baoco/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/Baoco/33e954c9-297b-44d5-a988-0b26a98d05ff/_tmp_space.db
17/01/13 22:38:40 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/01/13 22:38:40 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/01/13 22:38:40 INFO SparkContext: Starting job: collect at utils.scala:59
17/01/13 22:38:40 INFO DAGScheduler: Got job 0 (collect at utils.scala:59) with 1 output partitions
17/01/13 22:38:40 INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:59)
17/01/13 22:38:40 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:38:40 INFO DAGScheduler: Missing parents: List()
17/01/13 22:38:40 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at map at utils.scala:56), which has no missing parents
17/01/13 22:38:40 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 5.4 KB, free 5.4 KB)
17/01/13 22:38:40 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.0 KB, free 8.4 KB)
17/01/13 22:38:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:62928 (size: 3.0 KB, free: 511.1 MB)
17/01/13 22:38:40 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
17/01/13 22:38:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at map at utils.scala:56)
17/01/13 22:38:40 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
17/01/13 22:38:40 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2348 bytes)
17/01/13 22:38:40 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/01/13 22:38:40 INFO Executor: Fetching http://127.0.0.1:62911/jars/spark-csv_2.11-1.3.0.jar with timestamp 1484375875866
17/01/13 22:38:40 INFO Utils: Fetching http://127.0.0.1:62911/jars/spark-csv_2.11-1.3.0.jar to C:\Users\Baoco\AppData\Local\Temp\spark-c537a403-ca67-411f-bcab-27aab16e4c1e\userFiles-9e5d75fe-be45-4911-a388-5568494c92a5\fetchFileTemp2627715096357178046.tmp
17/01/13 22:38:40 INFO Executor: Adding file:/C:/Users/Baoco/AppData/Local/Temp/spark-c537a403-ca67-411f-bcab-27aab16e4c1e/userFiles-9e5d75fe-be45-4911-a388-5568494c92a5/spark-csv_2.11-1.3.0.jar to class loader
17/01/13 22:38:40 INFO Executor: Fetching http://127.0.0.1:62911/jars/commons-csv-1.1.jar with timestamp 1484375875868
17/01/13 22:38:40 INFO Utils: Fetching http://127.0.0.1:62911/jars/commons-csv-1.1.jar to C:\Users\Baoco\AppData\Local\Temp\spark-c537a403-ca67-411f-bcab-27aab16e4c1e\userFiles-9e5d75fe-be45-4911-a388-5568494c92a5\fetchFileTemp2710059019479850147.tmp
17/01/13 22:38:41 INFO Executor: Adding file:/C:/Users/Baoco/AppData/Local/Temp/spark-c537a403-ca67-411f-bcab-27aab16e4c1e/userFiles-9e5d75fe-be45-4911-a388-5568494c92a5/commons-csv-1.1.jar to class loader
17/01/13 22:38:41 INFO Executor: Fetching http://127.0.0.1:62911/jars/univocity-parsers-1.5.1.jar with timestamp 1484375875873
17/01/13 22:38:41 INFO Utils: Fetching http://127.0.0.1:62911/jars/univocity-parsers-1.5.1.jar to C:\Users\Baoco\AppData\Local\Temp\spark-c537a403-ca67-411f-bcab-27aab16e4c1e\userFiles-9e5d75fe-be45-4911-a388-5568494c92a5\fetchFileTemp6341036243532355342.tmp
17/01/13 22:38:41 INFO Executor: Adding file:/C:/Users/Baoco/AppData/Local/Temp/spark-c537a403-ca67-411f-bcab-27aab16e4c1e/userFiles-9e5d75fe-be45-4911-a388-5568494c92a5/univocity-parsers-1.5.1.jar to class loader
17/01/13 22:38:41 INFO Executor: Fetching http://127.0.0.1:62911/jars/sparklyr-1.6-2.10.jar with timestamp 1484375875875
17/01/13 22:38:41 INFO Utils: Fetching http://127.0.0.1:62911/jars/sparklyr-1.6-2.10.jar to C:\Users\Baoco\AppData\Local\Temp\spark-c537a403-ca67-411f-bcab-27aab16e4c1e\userFiles-9e5d75fe-be45-4911-a388-5568494c92a5\fetchFileTemp8712028456975999513.tmp
17/01/13 22:38:41 INFO Executor: Adding file:/C:/Users/Baoco/AppData/Local/Temp/spark-c537a403-ca67-411f-bcab-27aab16e4c1e/userFiles-9e5d75fe-be45-4911-a388-5568494c92a5/sparklyr-1.6-2.10.jar to class loader
17/01/13 22:38:41 INFO GenerateUnsafeProjection: Code generated in 140.84596 ms
17/01/13 22:38:41 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1060 bytes result sent to driver
17/01/13 22:38:41 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 771 ms on localhost (1/1)
17/01/13 22:38:41 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/01/13 22:38:41 INFO DAGScheduler: ResultStage 0 (collect at utils.scala:59) finished in 0.789 s
17/01/13 22:38:41 INFO DAGScheduler: Job 0 finished: collect at utils.scala:59, took 1.035348 s
17/01/13 22:38:41 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 61.8 KB, free 70.2 KB)
17/01/13 22:38:41 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.3 KB, free 89.5 KB)
17/01/13 22:38:41 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:62928 (size: 19.3 KB, free: 511.1 MB)
17/01/13 22:38:41 INFO SparkContext: Created broadcast 1 from textFile at TextFile.scala:30
17/01/13 22:38:41 INFO FileInputFormat: Total input paths to process : 1
17/01/13 22:38:41 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/01/13 22:38:41 INFO DAGScheduler: Got job 1 (take at CsvRelation.scala:249) with 1 output partitions
17/01/13 22:38:41 INFO DAGScheduler: Final stage: ResultStage 1 (take at CsvRelation.scala:249)
17/01/13 22:38:41 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:38:41 INFO DAGScheduler: Missing parents: List()
17/01/13 22:38:41 INFO DAGScheduler: Submitting ResultStage 1 (C:\Users\Baoco\AppData\Local\Temp\RtmpEtqPHH/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv MapPartitionsRDD[5] at textFile at TextFile.scala:30), which has no missing parents
17/01/13 22:38:41 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 3.2 KB, free 92.7 KB)
17/01/13 22:38:41 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 1947.0 B, free 94.6 KB)
17/01/13 22:38:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:62928 (size: 1947.0 B, free: 511.1 MB)
17/01/13 22:38:41 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
17/01/13 22:38:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (C:\Users\Baoco\AppData\Local\Temp\RtmpEtqPHH/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv MapPartitionsRDD[5] at textFile at TextFile.scala:30)
17/01/13 22:38:41 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
17/01/13 22:38:41 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 22:38:41 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
17/01/13 22:38:41 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpEtqPHH/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv:0+2088
17/01/13 22:38:41 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
17/01/13 22:38:41 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
17/01/13 22:38:41 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
17/01/13 22:38:41 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
17/01/13 22:38:41 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
17/01/13 22:38:41 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2354 bytes result sent to driver
17/01/13 22:38:41 INFO DAGScheduler: ResultStage 1 (take at CsvRelation.scala:249) finished in 0.086 s
17/01/13 22:38:41 INFO DAGScheduler: Job 1 finished: take at CsvRelation.scala:249, took 0.097765 s
17/01/13 22:38:41 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 86 ms on localhost (1/1)
17/01/13 22:38:41 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/01/13 22:38:41 INFO ParseDriver: Parsing command: SELECT * FROM  `iris`
17/01/13 22:38:42 INFO ParseDriver: Parse Completed
17/01/13 22:38:42 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 208.5 KB, free 303.1 KB)
17/01/13 22:38:42 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 19.3 KB, free 322.5 KB)
17/01/13 22:38:42 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:62928 (size: 19.3 KB, free: 511.1 MB)
17/01/13 22:38:42 INFO SparkContext: Created broadcast 3 from textFile at TextFile.scala:30
17/01/13 22:38:42 INFO FileInputFormat: Total input paths to process : 1
17/01/13 22:38:42 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/01/13 22:38:42 INFO DAGScheduler: Got job 2 (take at CsvRelation.scala:249) with 1 output partitions
17/01/13 22:38:42 INFO DAGScheduler: Final stage: ResultStage 2 (take at CsvRelation.scala:249)
17/01/13 22:38:42 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:38:42 INFO DAGScheduler: Missing parents: List()
17/01/13 22:38:42 INFO DAGScheduler: Submitting ResultStage 2 (C:\Users\Baoco\AppData\Local\Temp\RtmpEtqPHH/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv MapPartitionsRDD[7] at textFile at TextFile.scala:30), which has no missing parents
17/01/13 22:38:42 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 3.2 KB, free 325.7 KB)
17/01/13 22:38:42 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 1946.0 B, free 327.6 KB)
17/01/13 22:38:42 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:62928 (size: 1946.0 B, free: 511.1 MB)
17/01/13 22:38:42 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1006
17/01/13 22:38:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (C:\Users\Baoco\AppData\Local\Temp\RtmpEtqPHH/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv MapPartitionsRDD[7] at textFile at TextFile.scala:30)
17/01/13 22:38:42 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
17/01/13 22:38:42 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 22:38:42 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
17/01/13 22:38:42 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpEtqPHH/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv:0+2088
17/01/13 22:38:42 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2354 bytes result sent to driver
17/01/13 22:38:42 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 10 ms on localhost (1/1)
17/01/13 22:38:42 INFO DAGScheduler: ResultStage 2 (take at CsvRelation.scala:249) finished in 0.011 s
17/01/13 22:38:42 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
17/01/13 22:38:42 INFO DAGScheduler: Job 2 finished: take at CsvRelation.scala:249, took 0.021643 s
17/01/13 22:38:42 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 208.5 KB, free 536.1 KB)
17/01/13 22:38:42 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 19.3 KB, free 555.4 KB)
17/01/13 22:38:42 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:62928 (size: 19.3 KB, free: 511.1 MB)
17/01/13 22:38:42 INFO SparkContext: Created broadcast 5 from textFile at TextFile.scala:30
17/01/13 22:38:42 INFO FileInputFormat: Total input paths to process : 1
17/01/13 22:38:42 INFO SparkContext: Starting job: sql at null:-2
17/01/13 22:38:42 INFO DAGScheduler: Registering RDD 17 (sql at null:-2)
17/01/13 22:38:42 INFO DAGScheduler: Got job 3 (sql at null:-2) with 1 output partitions
17/01/13 22:38:42 INFO DAGScheduler: Final stage: ResultStage 4 (sql at null:-2)
17/01/13 22:38:42 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
17/01/13 22:38:42 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
17/01/13 22:38:42 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[17] at sql at null:-2), which has no missing parents
17/01/13 22:38:42 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 18.3 KB, free 573.7 KB)
17/01/13 22:38:42 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 8.7 KB, free 582.3 KB)
17/01/13 22:38:42 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:62928 (size: 8.7 KB, free: 511.1 MB)
17/01/13 22:38:42 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1006
17/01/13 22:38:42 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[17] at sql at null:-2)
17/01/13 22:38:42 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks
17/01/13 22:38:42 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:38:42 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 4, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:38:42 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
17/01/13 22:38:42 INFO Executor: Running task 1.0 in stage 3.0 (TID 4)
17/01/13 22:38:43 INFO CacheManager: Partition rdd_14_0 not found, computing it
17/01/13 22:38:43 INFO CacheManager: Partition rdd_14_1 not found, computing it
17/01/13 22:38:43 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpEtqPHH/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv:0+2088
17/01/13 22:38:43 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpEtqPHH/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv:2088+2089
17/01/13 22:38:43 INFO GenerateUnsafeProjection: Code generated in 16.837106 ms
17/01/13 22:38:43 INFO MemoryStore: Block rdd_14_1 stored as values in memory (estimated size 3.1 KB, free 585.5 KB)
17/01/13 22:38:43 INFO MemoryStore: Block rdd_14_0 stored as values in memory (estimated size 3.2 KB, free 588.7 KB)
17/01/13 22:38:43 INFO BlockManagerInfo: Added rdd_14_1 in memory on localhost:62928 (size: 3.1 KB, free: 511.1 MB)
17/01/13 22:38:43 INFO BlockManagerInfo: Added rdd_14_0 in memory on localhost:62928 (size: 3.2 KB, free: 511.0 MB)
17/01/13 22:38:43 INFO GeneratePredicate: Code generated in 10.830071 ms
17/01/13 22:38:43 INFO GenerateColumnAccessor: Code generated in 14.094495 ms
17/01/13 22:38:43 INFO GenerateMutableProjection: Code generated in 4.799996 ms
17/01/13 22:38:43 INFO GenerateUnsafeProjection: Code generated in 6.591994 ms
17/01/13 22:38:43 INFO GenerateMutableProjection: Code generated in 11.74527 ms
17/01/13 22:38:43 INFO GenerateUnsafeRowJoiner: Code generated in 10.517751 ms
17/01/13 22:38:43 INFO GenerateUnsafeProjection: Code generated in 7.816526 ms
17/01/13 22:38:43 INFO Executor: Finished task 1.0 in stage 3.0 (TID 4). 3787 bytes result sent to driver
17/01/13 22:38:43 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3784 bytes result sent to driver
17/01/13 22:38:43 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 342 ms on localhost (1/2)
17/01/13 22:38:43 INFO DAGScheduler: ShuffleMapStage 3 (sql at null:-2) finished in 0.348 s
17/01/13 22:38:43 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:38:43 INFO DAGScheduler: running: Set()
17/01/13 22:38:43 INFO DAGScheduler: waiting: Set(ResultStage 4)
17/01/13 22:38:43 INFO DAGScheduler: failed: Set()
17/01/13 22:38:43 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[20] at sql at null:-2), which has no missing parents
17/01/13 22:38:43 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 4) in 345 ms on localhost (2/2)
17/01/13 22:38:43 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
17/01/13 22:38:43 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 9.3 KB, free 597.9 KB)
17/01/13 22:38:43 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 4.6 KB, free 602.6 KB)
17/01/13 22:38:43 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:62928 (size: 4.6 KB, free: 511.0 MB)
17/01/13 22:38:43 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1006
17/01/13 22:38:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[20] at sql at null:-2)
17/01/13 22:38:43 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
17/01/13 22:38:43 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 5, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 22:38:43 INFO Executor: Running task 0.0 in stage 4.0 (TID 5)
17/01/13 22:38:43 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:38:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
17/01/13 22:38:43 INFO GenerateMutableProjection: Code generated in 8.2257 ms
17/01/13 22:38:43 INFO GenerateMutableProjection: Code generated in 6.539515 ms
17/01/13 22:38:43 INFO Executor: Finished task 0.0 in stage 4.0 (TID 5). 1830 bytes result sent to driver
17/01/13 22:38:43 INFO DAGScheduler: ResultStage 4 (sql at null:-2) finished in 0.116 s
17/01/13 22:38:43 INFO DAGScheduler: Job 3 finished: sql at null:-2, took 0.508708 s
17/01/13 22:38:43 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 5) in 115 ms on localhost (1/1)
17/01/13 22:38:43 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
17/01/13 22:38:43 INFO ParseDriver: Parsing command: SELECT count(*) FROM  `iris`
17/01/13 22:38:43 INFO ParseDriver: Parse Completed
17/01/13 22:38:43 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 22:38:43 INFO DAGScheduler: Registering RDD 24 (collect at utils.scala:195)
17/01/13 22:38:43 INFO DAGScheduler: Got job 4 (collect at utils.scala:195) with 1 output partitions
17/01/13 22:38:43 INFO DAGScheduler: Final stage: ResultStage 6 (collect at utils.scala:195)
17/01/13 22:38:43 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
17/01/13 22:38:43 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 5)
17/01/13 22:38:43 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[24] at collect at utils.scala:195), which has no missing parents
17/01/13 22:38:43 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 18.4 KB, free 620.9 KB)
17/01/13 22:38:43 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.7 KB, free 629.6 KB)
17/01/13 22:38:43 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:62928 (size: 8.7 KB, free: 511.0 MB)
17/01/13 22:38:43 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1006
17/01/13 22:38:43 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[24] at collect at utils.scala:195)
17/01/13 22:38:43 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks
17/01/13 22:38:43 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:38:43 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 7, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:38:43 INFO Executor: Running task 0.0 in stage 5.0 (TID 6)
17/01/13 22:38:43 INFO Executor: Running task 1.0 in stage 5.0 (TID 7)
17/01/13 22:38:43 INFO BlockManager: Found block rdd_14_0 locally
17/01/13 22:38:43 INFO BlockManager: Found block rdd_14_1 locally
17/01/13 22:38:43 INFO Executor: Finished task 0.0 in stage 5.0 (TID 6). 2679 bytes result sent to driver
17/01/13 22:38:43 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 6) in 34 ms on localhost (1/2)
17/01/13 22:38:43 INFO Executor: Finished task 1.0 in stage 5.0 (TID 7). 2679 bytes result sent to driver
17/01/13 22:38:43 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 7) in 37 ms on localhost (2/2)
17/01/13 22:38:43 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
17/01/13 22:38:43 INFO DAGScheduler: ShuffleMapStage 5 (collect at utils.scala:195) finished in 0.038 s
17/01/13 22:38:43 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:38:43 INFO DAGScheduler: running: Set()
17/01/13 22:38:43 INFO DAGScheduler: waiting: Set(ResultStage 6)
17/01/13 22:38:43 INFO DAGScheduler: failed: Set()
17/01/13 22:38:43 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[27] at collect at utils.scala:195), which has no missing parents
17/01/13 22:38:43 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 9.4 KB, free 639.0 KB)
17/01/13 22:38:43 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.6 KB, free 643.6 KB)
17/01/13 22:38:43 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:62928 (size: 4.6 KB, free: 511.0 MB)
17/01/13 22:38:43 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1006
17/01/13 22:38:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[27] at collect at utils.scala:195)
17/01/13 22:38:43 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
17/01/13 22:38:43 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 22:38:43 INFO Executor: Running task 0.0 in stage 6.0 (TID 8)
17/01/13 22:38:43 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:38:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/01/13 22:38:43 INFO Executor: Finished task 0.0 in stage 6.0 (TID 8). 1830 bytes result sent to driver
17/01/13 22:38:43 INFO DAGScheduler: ResultStage 6 (collect at utils.scala:195) finished in 0.013 s
17/01/13 22:38:43 INFO DAGScheduler: Job 4 finished: collect at utils.scala:195, took 0.071292 s
17/01/13 22:38:43 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 12 ms on localhost (1/1)
17/01/13 22:38:43 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
17/01/13 22:38:43 INFO ParseDriver: Parsing command: SELECT *
FROM `iris` AS `zzz1`
WHERE (0 = 1)
17/01/13 22:38:43 INFO ParseDriver: Parse Completed
17/01/13 22:38:43 INFO ContextCleaner: Cleaned accumulator 2
17/01/13 22:38:43 INFO ContextCleaner: Cleaned accumulator 10
17/01/13 22:38:43 INFO ContextCleaner: Cleaned accumulator 9
17/01/13 22:38:43 INFO ContextCleaner: Cleaned accumulator 8
17/01/13 22:38:43 INFO ContextCleaner: Cleaned accumulator 7
17/01/13 22:38:43 INFO BlockManagerInfo: Removed broadcast_4_piece0 on localhost:62928 in memory (size: 1946.0 B, free: 511.0 MB)
17/01/13 22:38:43 INFO ContextCleaner: Cleaned accumulator 5
17/01/13 22:38:43 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:62928 in memory (size: 19.3 KB, free: 511.1 MB)
17/01/13 22:38:43 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:62928 in memory (size: 1947.0 B, free: 511.1 MB)
17/01/13 22:38:43 INFO ContextCleaner: Cleaned accumulator 4
17/01/13 22:38:43 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:62928 in memory (size: 19.3 KB, free: 511.1 MB)
17/01/13 22:38:43 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:62928 in memory (size: 3.0 KB, free: 511.1 MB)
17/01/13 22:38:43 INFO ContextCleaner: Cleaned accumulator 3
17/01/13 22:38:43 INFO BlockManagerInfo: Removed broadcast_9_piece0 on localhost:62928 in memory (size: 4.6 KB, free: 511.1 MB)
17/01/13 22:38:43 INFO ContextCleaner: Cleaned accumulator 26
17/01/13 22:38:43 INFO BlockManagerInfo: Removed broadcast_8_piece0 on localhost:62928 in memory (size: 8.7 KB, free: 511.1 MB)
17/01/13 22:38:43 INFO ContextCleaner: Cleaned accumulator 25
17/01/13 22:38:43 INFO ContextCleaner: Cleaned shuffle 1
17/01/13 22:38:43 INFO BlockManagerInfo: Removed broadcast_7_piece0 on localhost:62928 in memory (size: 4.6 KB, free: 511.1 MB)
17/01/13 22:38:43 INFO ContextCleaner: Cleaned accumulator 16
17/01/13 22:38:43 INFO BlockManagerInfo: Removed broadcast_6_piece0 on localhost:62928 in memory (size: 8.7 KB, free: 511.1 MB)
17/01/13 22:38:43 INFO ContextCleaner: Cleaned accumulator 15
17/01/13 22:38:43 INFO ContextCleaner: Cleaned shuffle 0
17/01/13 22:38:43 INFO ContextCleaner: Cleaned accumulator 14
17/01/13 22:38:43 INFO ContextCleaner: Cleaned accumulator 13
17/01/13 22:38:43 INFO ContextCleaner: Cleaned accumulator 12
17/01/13 22:38:43 INFO ContextCleaner: Cleaned accumulator 11
17/01/13 22:38:44 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/01/13 22:38:44 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/01/13 22:38:44 INFO SparkContext: Starting job: collect at utils.scala:59
17/01/13 22:38:44 INFO DAGScheduler: Got job 5 (collect at utils.scala:59) with 1 output partitions
17/01/13 22:38:44 INFO DAGScheduler: Final stage: ResultStage 7 (collect at utils.scala:59)
17/01/13 22:38:44 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:38:44 INFO DAGScheduler: Missing parents: List()
17/01/13 22:38:44 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[31] at map at utils.scala:56), which has no missing parents
17/01/13 22:38:44 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.4 KB, free 239.6 KB)
17/01/13 22:38:44 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.0 KB, free 242.6 KB)
17/01/13 22:38:44 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:62928 (size: 3.0 KB, free: 511.1 MB)
17/01/13 22:38:44 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1006
17/01/13 22:38:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[31] at map at utils.scala:56)
17/01/13 22:38:44 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks
17/01/13 22:38:44 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 9, localhost, partition 0,PROCESS_LOCAL, 2646 bytes)
17/01/13 22:38:44 INFO Executor: Running task 0.0 in stage 7.0 (TID 9)
17/01/13 22:38:44 INFO Executor: Finished task 0.0 in stage 7.0 (TID 9). 1067 bytes result sent to driver
17/01/13 22:38:44 INFO DAGScheduler: ResultStage 7 (collect at utils.scala:59) finished in 0.005 s
17/01/13 22:38:44 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 9) in 4 ms on localhost (1/1)
17/01/13 22:38:44 INFO DAGScheduler: Job 5 finished: collect at utils.scala:59, took 0.011089 s
17/01/13 22:38:44 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
17/01/13 22:38:54 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 208.5 KB, free 451.1 KB)
17/01/13 22:38:54 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 19.3 KB, free 470.4 KB)
17/01/13 22:38:54 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:62928 (size: 19.3 KB, free: 511.1 MB)
17/01/13 22:38:54 INFO SparkContext: Created broadcast 11 from textFile at TextFile.scala:30
17/01/13 22:38:54 INFO FileInputFormat: Total input paths to process : 1
17/01/13 22:38:54 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/01/13 22:38:54 INFO DAGScheduler: Got job 6 (take at CsvRelation.scala:249) with 1 output partitions
17/01/13 22:38:54 INFO DAGScheduler: Final stage: ResultStage 8 (take at CsvRelation.scala:249)
17/01/13 22:38:54 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:38:54 INFO DAGScheduler: Missing parents: List()
17/01/13 22:38:54 INFO DAGScheduler: Submitting ResultStage 8 (C:\Users\Baoco\AppData\Local\Temp\RtmpEtqPHH/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv MapPartitionsRDD[33] at textFile at TextFile.scala:30), which has no missing parents
17/01/13 22:38:54 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 3.2 KB, free 473.6 KB)
17/01/13 22:38:54 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 1944.0 B, free 475.5 KB)
17/01/13 22:38:54 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:62928 (size: 1944.0 B, free: 511.1 MB)
17/01/13 22:38:54 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1006
17/01/13 22:38:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (C:\Users\Baoco\AppData\Local\Temp\RtmpEtqPHH/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv MapPartitionsRDD[33] at textFile at TextFile.scala:30)
17/01/13 22:38:54 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks
17/01/13 22:38:54 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 10, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 22:38:54 INFO Executor: Running task 0.0 in stage 8.0 (TID 10)
17/01/13 22:38:54 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpEtqPHH/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv:0+16824941
17/01/13 22:38:54 INFO Executor: Finished task 0.0 in stage 8.0 (TID 10). 3117 bytes result sent to driver
17/01/13 22:38:54 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 10) in 7 ms on localhost (1/1)
17/01/13 22:38:54 INFO DAGScheduler: ResultStage 8 (take at CsvRelation.scala:249) finished in 0.007 s
17/01/13 22:38:54 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
17/01/13 22:38:54 INFO DAGScheduler: Job 6 finished: take at CsvRelation.scala:249, took 0.012404 s
17/01/13 22:38:54 INFO ParseDriver: Parsing command: SELECT * FROM  `flights`
17/01/13 22:38:54 INFO ParseDriver: Parse Completed
17/01/13 22:38:54 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 208.5 KB, free 684.0 KB)
17/01/13 22:38:54 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 19.3 KB, free 703.3 KB)
17/01/13 22:38:54 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on localhost:62928 (size: 19.3 KB, free: 511.1 MB)
17/01/13 22:38:54 INFO SparkContext: Created broadcast 13 from textFile at TextFile.scala:30
17/01/13 22:38:54 INFO FileInputFormat: Total input paths to process : 1
17/01/13 22:38:54 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/01/13 22:38:54 INFO DAGScheduler: Got job 7 (take at CsvRelation.scala:249) with 1 output partitions
17/01/13 22:38:54 INFO DAGScheduler: Final stage: ResultStage 9 (take at CsvRelation.scala:249)
17/01/13 22:38:54 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:38:54 INFO DAGScheduler: Missing parents: List()
17/01/13 22:38:54 INFO DAGScheduler: Submitting ResultStage 9 (C:\Users\Baoco\AppData\Local\Temp\RtmpEtqPHH/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv MapPartitionsRDD[35] at textFile at TextFile.scala:30), which has no missing parents
17/01/13 22:38:54 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 3.2 KB, free 706.5 KB)
17/01/13 22:38:54 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 1944.0 B, free 708.4 KB)
17/01/13 22:38:54 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on localhost:62928 (size: 1944.0 B, free: 511.1 MB)
17/01/13 22:38:54 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1006
17/01/13 22:38:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (C:\Users\Baoco\AppData\Local\Temp\RtmpEtqPHH/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv MapPartitionsRDD[35] at textFile at TextFile.scala:30)
17/01/13 22:38:54 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
17/01/13 22:38:54 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 11, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 22:38:54 INFO Executor: Running task 0.0 in stage 9.0 (TID 11)
17/01/13 22:38:54 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpEtqPHH/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv:0+16824941
17/01/13 22:38:54 INFO Executor: Finished task 0.0 in stage 9.0 (TID 11). 3117 bytes result sent to driver
17/01/13 22:38:54 INFO DAGScheduler: ResultStage 9 (take at CsvRelation.scala:249) finished in 0.010 s
17/01/13 22:38:54 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 11) in 10 ms on localhost (1/1)
17/01/13 22:38:54 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
17/01/13 22:38:54 INFO DAGScheduler: Job 7 finished: take at CsvRelation.scala:249, took 0.018956 s
17/01/13 22:38:54 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 208.5 KB, free 916.9 KB)
17/01/13 22:38:54 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 19.3 KB, free 936.3 KB)
17/01/13 22:38:54 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on localhost:62928 (size: 19.3 KB, free: 511.0 MB)
17/01/13 22:38:54 INFO SparkContext: Created broadcast 15 from textFile at TextFile.scala:30
17/01/13 22:38:54 INFO FileInputFormat: Total input paths to process : 1
17/01/13 22:38:54 INFO SparkContext: Starting job: sql at null:-2
17/01/13 22:38:54 INFO DAGScheduler: Registering RDD 45 (sql at null:-2)
17/01/13 22:38:54 INFO DAGScheduler: Got job 8 (sql at null:-2) with 1 output partitions
17/01/13 22:38:54 INFO DAGScheduler: Final stage: ResultStage 11 (sql at null:-2)
17/01/13 22:38:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
17/01/13 22:38:54 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 10)
17/01/13 22:38:54 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[45] at sql at null:-2), which has no missing parents
17/01/13 22:38:54 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 26.8 KB, free 963.1 KB)
17/01/13 22:38:54 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 11.0 KB, free 974.1 KB)
17/01/13 22:38:54 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on localhost:62928 (size: 11.0 KB, free: 511.0 MB)
17/01/13 22:38:54 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1006
17/01/13 22:38:54 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[45] at sql at null:-2)
17/01/13 22:38:54 INFO TaskSchedulerImpl: Adding task set 10.0 with 2 tasks
17/01/13 22:38:54 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 12, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:38:54 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 13, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:38:54 INFO Executor: Running task 0.0 in stage 10.0 (TID 12)
17/01/13 22:38:54 INFO Executor: Running task 1.0 in stage 10.0 (TID 13)
17/01/13 22:38:54 INFO CacheManager: Partition rdd_42_0 not found, computing it
17/01/13 22:38:54 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpEtqPHH/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv:0+16824941
17/01/13 22:38:54 INFO CacheManager: Partition rdd_42_1 not found, computing it
17/01/13 22:38:54 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpEtqPHH/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv:16824941+16824942
17/01/13 22:38:54 INFO GenerateUnsafeProjection: Code generated in 26.141844 ms
17/01/13 22:38:55 INFO BlockManagerInfo: Removed broadcast_14_piece0 on localhost:62928 in memory (size: 1944.0 B, free: 511.0 MB)
17/01/13 22:38:55 INFO ContextCleaner: Cleaned accumulator 30
17/01/13 22:38:55 INFO BlockManagerInfo: Removed broadcast_13_piece0 on localhost:62928 in memory (size: 19.3 KB, free: 511.0 MB)
17/01/13 22:38:55 INFO BlockManagerInfo: Removed broadcast_12_piece0 on localhost:62928 in memory (size: 1944.0 B, free: 511.0 MB)
17/01/13 22:38:55 INFO ContextCleaner: Cleaned accumulator 29
17/01/13 22:38:55 INFO BlockManagerInfo: Removed broadcast_11_piece0 on localhost:62928 in memory (size: 19.3 KB, free: 511.1 MB)
17/01/13 22:38:55 INFO BlockManagerInfo: Removed broadcast_10_piece0 on localhost:62928 in memory (size: 3.0 KB, free: 511.1 MB)
17/01/13 22:38:55 INFO ContextCleaner: Cleaned accumulator 28
17/01/13 22:38:55 INFO ContextCleaner: Cleaned accumulator 27
17/01/13 22:38:59 INFO MemoryStore: Block rdd_42_1 stored as values in memory (estimated size 12.2 MB, free 12.7 MB)
17/01/13 22:38:59 INFO BlockManagerInfo: Added rdd_42_1 in memory on localhost:62928 (size: 12.2 MB, free: 498.9 MB)
17/01/13 22:38:59 INFO MemoryStore: Block rdd_42_0 stored as values in memory (estimated size 12.2 MB, free 24.9 MB)
17/01/13 22:38:59 INFO BlockManagerInfo: Added rdd_42_0 in memory on localhost:62928 (size: 12.2 MB, free: 486.7 MB)
17/01/13 22:38:59 INFO Executor: Finished task 0.0 in stage 10.0 (TID 12). 21077 bytes result sent to driver
17/01/13 22:38:59 INFO Executor: Finished task 1.0 in stage 10.0 (TID 13). 21132 bytes result sent to driver
17/01/13 22:38:59 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 12) in 5180 ms on localhost (1/2)
17/01/13 22:38:59 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 13) in 5182 ms on localhost (2/2)
17/01/13 22:38:59 INFO DAGScheduler: ShuffleMapStage 10 (sql at null:-2) finished in 5.183 s
17/01/13 22:38:59 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
17/01/13 22:38:59 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:38:59 INFO DAGScheduler: running: Set()
17/01/13 22:38:59 INFO DAGScheduler: waiting: Set(ResultStage 11)
17/01/13 22:38:59 INFO DAGScheduler: failed: Set()
17/01/13 22:38:59 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[48] at sql at null:-2), which has no missing parents
17/01/13 22:39:00 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 9.3 KB, free 24.9 MB)
17/01/13 22:39:00 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 4.6 KB, free 24.9 MB)
17/01/13 22:39:00 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on localhost:62928 (size: 4.6 KB, free: 486.7 MB)
17/01/13 22:39:00 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1006
17/01/13 22:39:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[48] at sql at null:-2)
17/01/13 22:39:00 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks
17/01/13 22:39:00 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 14, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 22:39:00 INFO Executor: Running task 0.0 in stage 11.0 (TID 14)
17/01/13 22:39:00 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:39:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:39:00 INFO Executor: Finished task 0.0 in stage 11.0 (TID 14). 1830 bytes result sent to driver
17/01/13 22:39:00 INFO DAGScheduler: ResultStage 11 (sql at null:-2) finished in 0.012 s
17/01/13 22:39:00 INFO DAGScheduler: Job 8 finished: sql at null:-2, took 5.211024 s
17/01/13 22:39:00 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 14) in 11 ms on localhost (1/1)
17/01/13 22:39:00 INFO ParseDriver: Parsing command: SELECT count(*) FROM  `flights`
17/01/13 22:39:00 INFO ParseDriver: Parse Completed
17/01/13 22:39:00 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
17/01/13 22:39:00 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 22:39:00 INFO DAGScheduler: Registering RDD 52 (collect at utils.scala:195)
17/01/13 22:39:00 INFO DAGScheduler: Got job 9 (collect at utils.scala:195) with 1 output partitions
17/01/13 22:39:00 INFO DAGScheduler: Final stage: ResultStage 13 (collect at utils.scala:195)
17/01/13 22:39:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)
17/01/13 22:39:00 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 12)
17/01/13 22:39:00 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[52] at collect at utils.scala:195), which has no missing parents
17/01/13 22:39:00 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 26.9 KB, free 24.9 MB)
17/01/13 22:39:00 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 11.0 KB, free 24.9 MB)
17/01/13 22:39:00 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on localhost:62928 (size: 11.0 KB, free: 486.7 MB)
17/01/13 22:39:00 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1006
17/01/13 22:39:00 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[52] at collect at utils.scala:195)
17/01/13 22:39:00 INFO TaskSchedulerImpl: Adding task set 12.0 with 2 tasks
17/01/13 22:39:00 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 15, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:39:00 INFO TaskSetManager: Starting task 1.0 in stage 12.0 (TID 16, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:39:00 INFO Executor: Running task 0.0 in stage 12.0 (TID 15)
17/01/13 22:39:00 INFO BlockManager: Found block rdd_42_0 locally
17/01/13 22:39:00 INFO Executor: Running task 1.0 in stage 12.0 (TID 16)
17/01/13 22:39:00 INFO BlockManager: Found block rdd_42_1 locally
17/01/13 22:39:00 INFO Executor: Finished task 1.0 in stage 12.0 (TID 16). 2679 bytes result sent to driver
17/01/13 22:39:00 INFO Executor: Finished task 0.0 in stage 12.0 (TID 15). 2679 bytes result sent to driver
17/01/13 22:39:00 INFO TaskSetManager: Finished task 1.0 in stage 12.0 (TID 16) in 38 ms on localhost (1/2)
17/01/13 22:39:00 INFO DAGScheduler: ShuffleMapStage 12 (collect at utils.scala:195) finished in 0.042 s
17/01/13 22:39:00 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:39:00 INFO DAGScheduler: running: Set()
17/01/13 22:39:00 INFO DAGScheduler: waiting: Set(ResultStage 13)
17/01/13 22:39:00 INFO DAGScheduler: failed: Set()
17/01/13 22:39:00 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[55] at collect at utils.scala:195), which has no missing parents
17/01/13 22:39:00 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 9.4 KB, free 24.9 MB)
17/01/13 22:39:00 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 15) in 41 ms on localhost (2/2)
17/01/13 22:39:00 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
17/01/13 22:39:00 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 4.6 KB, free 24.9 MB)
17/01/13 22:39:00 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on localhost:62928 (size: 4.6 KB, free: 486.7 MB)
17/01/13 22:39:00 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1006
17/01/13 22:39:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[55] at collect at utils.scala:195)
17/01/13 22:39:00 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks
17/01/13 22:39:00 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 17, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 22:39:00 INFO Executor: Running task 0.0 in stage 13.0 (TID 17)
17/01/13 22:39:00 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:39:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:39:00 INFO Executor: Finished task 0.0 in stage 13.0 (TID 17). 1830 bytes result sent to driver
17/01/13 22:39:00 INFO DAGScheduler: ResultStage 13 (collect at utils.scala:195) finished in 0.012 s
17/01/13 22:39:00 INFO DAGScheduler: Job 9 finished: collect at utils.scala:195, took 0.074167 s
17/01/13 22:39:00 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 17) in 12 ms on localhost (1/1)
17/01/13 22:39:00 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
17/01/13 22:39:00 INFO ParseDriver: Parsing command: SELECT *
FROM `flights` AS `zzz2`
WHERE (0 = 1)
17/01/13 22:39:00 INFO ParseDriver: Parse Completed
17/01/13 22:39:00 INFO BlockManagerInfo: Removed broadcast_19_piece0 on localhost:62928 in memory (size: 4.6 KB, free: 486.7 MB)
17/01/13 22:39:00 INFO ContextCleaner: Cleaned accumulator 51
17/01/13 22:39:00 INFO BlockManagerInfo: Removed broadcast_18_piece0 on localhost:62928 in memory (size: 11.0 KB, free: 486.7 MB)
17/01/13 22:39:00 INFO ContextCleaner: Cleaned accumulator 50
17/01/13 22:39:00 INFO ContextCleaner: Cleaned shuffle 3
17/01/13 22:39:00 INFO BlockManagerInfo: Removed broadcast_17_piece0 on localhost:62928 in memory (size: 4.6 KB, free: 486.7 MB)
17/01/13 22:39:00 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/01/13 22:39:00 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/01/13 22:39:00 INFO SparkContext: Starting job: collect at utils.scala:59
17/01/13 22:39:00 INFO DAGScheduler: Got job 10 (collect at utils.scala:59) with 1 output partitions
17/01/13 22:39:00 INFO DAGScheduler: Final stage: ResultStage 14 (collect at utils.scala:59)
17/01/13 22:39:00 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:39:00 INFO DAGScheduler: Missing parents: List()
17/01/13 22:39:00 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[59] at map at utils.scala:56), which has no missing parents
17/01/13 22:39:00 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 5.4 KB, free 24.9 MB)
17/01/13 22:39:00 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 3.0 KB, free 24.9 MB)
17/01/13 22:39:00 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on localhost:62928 (size: 3.0 KB, free: 486.7 MB)
17/01/13 22:39:00 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1006
17/01/13 22:39:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[59] at map at utils.scala:56)
17/01/13 22:39:00 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks
17/01/13 22:39:00 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 18, localhost, partition 0,PROCESS_LOCAL, 2687 bytes)
17/01/13 22:39:00 INFO Executor: Running task 0.0 in stage 14.0 (TID 18)
17/01/13 22:39:00 INFO Executor: Finished task 0.0 in stage 14.0 (TID 18). 1077 bytes result sent to driver
17/01/13 22:39:00 INFO DAGScheduler: ResultStage 14 (collect at utils.scala:59) finished in 0.008 s
17/01/13 22:39:00 INFO DAGScheduler: Job 10 finished: collect at utils.scala:59, took 0.057505 s
17/01/13 22:39:00 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 18) in 7 ms on localhost (1/1)
17/01/13 22:39:00 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
17/01/13 22:39:02 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 208.5 KB, free 25.1 MB)
17/01/13 22:39:02 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 19.3 KB, free 25.1 MB)
17/01/13 22:39:02 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on localhost:62928 (size: 19.3 KB, free: 486.7 MB)
17/01/13 22:39:02 INFO SparkContext: Created broadcast 21 from textFile at TextFile.scala:30
17/01/13 22:39:02 INFO FileInputFormat: Total input paths to process : 1
17/01/13 22:39:02 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/01/13 22:39:02 INFO DAGScheduler: Got job 11 (take at CsvRelation.scala:249) with 1 output partitions
17/01/13 22:39:02 INFO DAGScheduler: Final stage: ResultStage 15 (take at CsvRelation.scala:249)
17/01/13 22:39:02 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:39:02 INFO DAGScheduler: Missing parents: List()
17/01/13 22:39:02 INFO DAGScheduler: Submitting ResultStage 15 (C:\Users\Baoco\AppData\Local\Temp\RtmpEtqPHH/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv MapPartitionsRDD[61] at textFile at TextFile.scala:30), which has no missing parents
17/01/13 22:39:02 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 3.2 KB, free 25.1 MB)
17/01/13 22:39:02 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 1944.0 B, free 25.1 MB)
17/01/13 22:39:02 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on localhost:62928 (size: 1944.0 B, free: 486.7 MB)
17/01/13 22:39:02 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1006
17/01/13 22:39:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (C:\Users\Baoco\AppData\Local\Temp\RtmpEtqPHH/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv MapPartitionsRDD[61] at textFile at TextFile.scala:30)
17/01/13 22:39:02 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks
17/01/13 22:39:02 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 19, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 22:39:02 INFO Executor: Running task 0.0 in stage 15.0 (TID 19)
17/01/13 22:39:02 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpEtqPHH/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv:0+3427780
17/01/13 22:39:02 INFO Executor: Finished task 0.0 in stage 15.0 (TID 19). 2768 bytes result sent to driver
17/01/13 22:39:02 INFO DAGScheduler: ResultStage 15 (take at CsvRelation.scala:249) finished in 0.013 s
17/01/13 22:39:02 INFO DAGScheduler: Job 11 finished: take at CsvRelation.scala:249, took 0.017635 s
17/01/13 22:39:02 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 19) in 13 ms on localhost (1/1)
17/01/13 22:39:02 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
17/01/13 22:39:02 INFO ParseDriver: Parsing command: SELECT * FROM  `batting`
17/01/13 22:39:02 INFO ParseDriver: Parse Completed
17/01/13 22:39:02 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 208.5 KB, free 25.3 MB)
17/01/13 22:39:02 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 19.3 KB, free 25.3 MB)
17/01/13 22:39:02 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on localhost:62928 (size: 19.3 KB, free: 486.7 MB)
17/01/13 22:39:02 INFO SparkContext: Created broadcast 23 from textFile at TextFile.scala:30
17/01/13 22:39:02 INFO FileInputFormat: Total input paths to process : 1
17/01/13 22:39:02 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/01/13 22:39:02 INFO DAGScheduler: Got job 12 (take at CsvRelation.scala:249) with 1 output partitions
17/01/13 22:39:02 INFO DAGScheduler: Final stage: ResultStage 16 (take at CsvRelation.scala:249)
17/01/13 22:39:02 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:39:02 INFO DAGScheduler: Missing parents: List()
17/01/13 22:39:02 INFO DAGScheduler: Submitting ResultStage 16 (C:\Users\Baoco\AppData\Local\Temp\RtmpEtqPHH/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv MapPartitionsRDD[63] at textFile at TextFile.scala:30), which has no missing parents
17/01/13 22:39:02 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 3.2 KB, free 25.3 MB)
17/01/13 22:39:02 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 1944.0 B, free 25.3 MB)
17/01/13 22:39:02 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on localhost:62928 (size: 1944.0 B, free: 486.7 MB)
17/01/13 22:39:02 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1006
17/01/13 22:39:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (C:\Users\Baoco\AppData\Local\Temp\RtmpEtqPHH/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv MapPartitionsRDD[63] at textFile at TextFile.scala:30)
17/01/13 22:39:02 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks
17/01/13 22:39:02 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 20, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 22:39:02 INFO Executor: Running task 0.0 in stage 16.0 (TID 20)
17/01/13 22:39:02 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpEtqPHH/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv:0+3427780
17/01/13 22:39:02 INFO Executor: Finished task 0.0 in stage 16.0 (TID 20). 2768 bytes result sent to driver
17/01/13 22:39:02 INFO DAGScheduler: ResultStage 16 (take at CsvRelation.scala:249) finished in 0.012 s
17/01/13 22:39:02 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 20) in 11 ms on localhost (1/1)
17/01/13 22:39:02 INFO DAGScheduler: Job 12 finished: take at CsvRelation.scala:249, took 0.018986 s
17/01/13 22:39:02 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
17/01/13 22:39:02 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 208.5 KB, free 25.5 MB)
17/01/13 22:39:02 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 19.3 KB, free 25.5 MB)
17/01/13 22:39:02 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on localhost:62928 (size: 19.3 KB, free: 486.6 MB)
17/01/13 22:39:02 INFO SparkContext: Created broadcast 25 from textFile at TextFile.scala:30
17/01/13 22:39:02 INFO FileInputFormat: Total input paths to process : 1
17/01/13 22:39:02 INFO SparkContext: Starting job: sql at null:-2
17/01/13 22:39:02 INFO DAGScheduler: Registering RDD 73 (sql at null:-2)
17/01/13 22:39:02 INFO DAGScheduler: Got job 13 (sql at null:-2) with 1 output partitions
17/01/13 22:39:02 INFO DAGScheduler: Final stage: ResultStage 18 (sql at null:-2)
17/01/13 22:39:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)
17/01/13 22:39:02 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 17)
17/01/13 22:39:02 INFO DAGScheduler: Submitting ShuffleMapStage 17 (MapPartitionsRDD[73] at sql at null:-2), which has no missing parents
17/01/13 22:39:02 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 27.9 KB, free 25.6 MB)
17/01/13 22:39:02 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 11.2 KB, free 25.6 MB)
17/01/13 22:39:02 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on localhost:62928 (size: 11.2 KB, free: 486.6 MB)
17/01/13 22:39:02 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1006
17/01/13 22:39:02 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[73] at sql at null:-2)
17/01/13 22:39:02 INFO TaskSchedulerImpl: Adding task set 17.0 with 2 tasks
17/01/13 22:39:02 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 21, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:39:02 INFO TaskSetManager: Starting task 1.0 in stage 17.0 (TID 22, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:39:02 INFO Executor: Running task 0.0 in stage 17.0 (TID 21)
17/01/13 22:39:02 INFO Executor: Running task 1.0 in stage 17.0 (TID 22)
17/01/13 22:39:02 INFO CacheManager: Partition rdd_70_1 not found, computing it
17/01/13 22:39:02 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpEtqPHH/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv:3427780+3427781
17/01/13 22:39:02 INFO CacheManager: Partition rdd_70_0 not found, computing it
17/01/13 22:39:02 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpEtqPHH/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv:0+3427780
17/01/13 22:39:02 INFO GenerateUnsafeProjection: Code generated in 30.194321 ms
17/01/13 22:39:03 INFO ContextCleaner: Cleaned accumulator 53
17/01/13 22:39:03 INFO ContextCleaner: Cleaned accumulator 52
17/01/13 22:39:03 INFO BlockManagerInfo: Removed broadcast_24_piece0 on localhost:62928 in memory (size: 1944.0 B, free: 486.6 MB)
17/01/13 22:39:03 INFO ContextCleaner: Cleaned accumulator 55
17/01/13 22:39:03 INFO BlockManagerInfo: Removed broadcast_23_piece0 on localhost:62928 in memory (size: 19.3 KB, free: 486.6 MB)
17/01/13 22:39:03 INFO BlockManagerInfo: Removed broadcast_22_piece0 on localhost:62928 in memory (size: 1944.0 B, free: 486.6 MB)
17/01/13 22:39:03 INFO ContextCleaner: Cleaned accumulator 54
17/01/13 22:39:03 INFO BlockManagerInfo: Removed broadcast_21_piece0 on localhost:62928 in memory (size: 19.3 KB, free: 486.7 MB)
17/01/13 22:39:03 INFO BlockManagerInfo: Removed broadcast_20_piece0 on localhost:62928 in memory (size: 3.0 KB, free: 486.7 MB)
17/01/13 22:39:04 INFO MemoryStore: Block rdd_70_0 stored as values in memory (estimated size 1897.3 KB, free 27.0 MB)
17/01/13 22:39:04 INFO BlockManagerInfo: Added rdd_70_0 in memory on localhost:62928 (size: 1897.3 KB, free: 484.8 MB)
17/01/13 22:39:04 INFO Executor: Finished task 0.0 in stage 17.0 (TID 21). 9713 bytes result sent to driver
17/01/13 22:39:04 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 21) in 1233 ms on localhost (1/2)
17/01/13 22:39:04 INFO MemoryStore: Block rdd_70_1 stored as values in memory (estimated size 1699.8 KB, free 28.6 MB)
17/01/13 22:39:04 INFO BlockManagerInfo: Added rdd_70_1 in memory on localhost:62928 (size: 1699.8 KB, free: 483.2 MB)
17/01/13 22:39:04 INFO Executor: Finished task 1.0 in stage 17.0 (TID 22). 9862 bytes result sent to driver
17/01/13 22:39:04 INFO DAGScheduler: ShuffleMapStage 17 (sql at null:-2) finished in 1.263 s
17/01/13 22:39:04 INFO TaskSetManager: Finished task 1.0 in stage 17.0 (TID 22) in 1262 ms on localhost (2/2)
17/01/13 22:39:04 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:39:04 INFO DAGScheduler: running: Set()
17/01/13 22:39:04 INFO DAGScheduler: waiting: Set(ResultStage 18)
17/01/13 22:39:04 INFO DAGScheduler: failed: Set()
17/01/13 22:39:04 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
17/01/13 22:39:04 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[76] at sql at null:-2), which has no missing parents
17/01/13 22:39:04 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 9.3 KB, free 28.6 MB)
17/01/13 22:39:04 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 4.6 KB, free 28.6 MB)
17/01/13 22:39:04 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on localhost:62928 (size: 4.6 KB, free: 483.2 MB)
17/01/13 22:39:04 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1006
17/01/13 22:39:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[76] at sql at null:-2)
17/01/13 22:39:04 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks
17/01/13 22:39:04 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 23, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 22:39:04 INFO Executor: Running task 0.0 in stage 18.0 (TID 23)
17/01/13 22:39:04 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:39:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:39:04 INFO Executor: Finished task 0.0 in stage 18.0 (TID 23). 1830 bytes result sent to driver
17/01/13 22:39:04 INFO DAGScheduler: ResultStage 18 (sql at null:-2) finished in 0.011 s
17/01/13 22:39:04 INFO DAGScheduler: Job 13 finished: sql at null:-2, took 1.299155 s
17/01/13 22:39:04 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 23) in 10 ms on localhost (1/1)
17/01/13 22:39:04 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
17/01/13 22:39:04 INFO ParseDriver: Parsing command: SELECT count(*) FROM  `batting`
17/01/13 22:39:04 INFO ParseDriver: Parse Completed
17/01/13 22:39:04 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 22:39:04 INFO DAGScheduler: Registering RDD 80 (collect at utils.scala:195)
17/01/13 22:39:04 INFO DAGScheduler: Got job 14 (collect at utils.scala:195) with 1 output partitions
17/01/13 22:39:04 INFO DAGScheduler: Final stage: ResultStage 20 (collect at utils.scala:195)
17/01/13 22:39:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)
17/01/13 22:39:04 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 19)
17/01/13 22:39:04 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[80] at collect at utils.scala:195), which has no missing parents
17/01/13 22:39:04 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 28.0 KB, free 28.7 MB)
17/01/13 22:39:04 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 11.2 KB, free 28.7 MB)
17/01/13 22:39:04 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on localhost:62928 (size: 11.2 KB, free: 483.1 MB)
17/01/13 22:39:04 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1006
17/01/13 22:39:04 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[80] at collect at utils.scala:195)
17/01/13 22:39:04 INFO TaskSchedulerImpl: Adding task set 19.0 with 2 tasks
17/01/13 22:39:04 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 24, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:39:04 INFO TaskSetManager: Starting task 1.0 in stage 19.0 (TID 25, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:39:04 INFO Executor: Running task 0.0 in stage 19.0 (TID 24)
17/01/13 22:39:04 INFO Executor: Running task 1.0 in stage 19.0 (TID 25)
17/01/13 22:39:04 INFO BlockManager: Found block rdd_70_0 locally
17/01/13 22:39:04 INFO BlockManager: Found block rdd_70_1 locally
17/01/13 22:39:04 INFO Executor: Finished task 0.0 in stage 19.0 (TID 24). 2679 bytes result sent to driver
17/01/13 22:39:04 INFO Executor: Finished task 1.0 in stage 19.0 (TID 25). 2679 bytes result sent to driver
17/01/13 22:39:04 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 24) in 29 ms on localhost (1/2)
17/01/13 22:39:04 INFO TaskSetManager: Finished task 1.0 in stage 19.0 (TID 25) in 30 ms on localhost (2/2)
17/01/13 22:39:04 INFO DAGScheduler: ShuffleMapStage 19 (collect at utils.scala:195) finished in 0.031 s
17/01/13 22:39:04 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:39:04 INFO DAGScheduler: running: Set()
17/01/13 22:39:04 INFO DAGScheduler: waiting: Set(ResultStage 20)
17/01/13 22:39:04 INFO DAGScheduler: failed: Set()
17/01/13 22:39:04 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
17/01/13 22:39:04 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[83] at collect at utils.scala:195), which has no missing parents
17/01/13 22:39:04 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 9.4 KB, free 28.7 MB)
17/01/13 22:39:04 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 4.7 KB, free 28.7 MB)
17/01/13 22:39:04 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on localhost:62928 (size: 4.7 KB, free: 483.1 MB)
17/01/13 22:39:04 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1006
17/01/13 22:39:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[83] at collect at utils.scala:195)
17/01/13 22:39:04 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks
17/01/13 22:39:04 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 26, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 22:39:04 INFO Executor: Running task 0.0 in stage 20.0 (TID 26)
17/01/13 22:39:04 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:39:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/01/13 22:39:04 INFO Executor: Finished task 0.0 in stage 20.0 (TID 26). 1830 bytes result sent to driver
17/01/13 22:39:04 INFO DAGScheduler: ResultStage 20 (collect at utils.scala:195) finished in 0.009 s
17/01/13 22:39:04 INFO DAGScheduler: Job 14 finished: collect at utils.scala:195, took 0.054507 s
17/01/13 22:39:04 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 26) in 9 ms on localhost (1/1)
17/01/13 22:39:04 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
17/01/13 22:39:04 INFO ParseDriver: Parsing command: SELECT *
FROM `batting` AS `zzz3`
WHERE (0 = 1)
17/01/13 22:39:04 INFO ParseDriver: Parse Completed
17/01/13 22:39:04 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/01/13 22:39:04 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/01/13 22:39:04 INFO SparkContext: Starting job: collect at utils.scala:59
17/01/13 22:39:04 INFO DAGScheduler: Got job 15 (collect at utils.scala:59) with 1 output partitions
17/01/13 22:39:04 INFO DAGScheduler: Final stage: ResultStage 21 (collect at utils.scala:59)
17/01/13 22:39:04 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:39:04 INFO DAGScheduler: Missing parents: List()
17/01/13 22:39:04 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[87] at map at utils.scala:56), which has no missing parents
17/01/13 22:39:04 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 5.4 KB, free 28.7 MB)
17/01/13 22:39:04 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 3.0 KB, free 28.7 MB)
17/01/13 22:39:04 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on localhost:62928 (size: 3.0 KB, free: 483.1 MB)
17/01/13 22:39:04 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1006
17/01/13 22:39:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[87] at map at utils.scala:56)
17/01/13 22:39:04 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks
17/01/13 22:39:04 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 27, localhost, partition 0,PROCESS_LOCAL, 2728 bytes)
17/01/13 22:39:04 INFO Executor: Running task 0.0 in stage 21.0 (TID 27)
17/01/13 22:39:04 INFO Executor: Finished task 0.0 in stage 21.0 (TID 27). 1087 bytes result sent to driver
17/01/13 22:39:04 INFO DAGScheduler: ResultStage 21 (collect at utils.scala:59) finished in 0.007 s
17/01/13 22:39:04 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 27) in 6 ms on localhost (1/1)
17/01/13 22:39:04 INFO DAGScheduler: Job 15 finished: collect at utils.scala:59, took 0.013703 s
17/01/13 22:39:04 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
17/01/13 22:39:04 INFO ParseDriver: Parsing command: SELECT *
FROM `flights`
WHERE (`dep_delay` = 2.0)
17/01/13 22:39:04 INFO ParseDriver: Parse Completed
17/01/13 22:39:04 INFO InMemoryColumnarTableScan: Predicate (dep_delay#146 = 2.0) generates partition filter: ((dep_delay.lowerBound#1071 <= 2.0) && (2.0 <= dep_delay.upperBound#1070))
17/01/13 22:39:04 INFO SparkContext: Starting job: count at null:-2
17/01/13 22:39:04 INFO DAGScheduler: Registering RDD 92 (count at null:-2)
17/01/13 22:39:04 INFO DAGScheduler: Got job 16 (count at null:-2) with 1 output partitions
17/01/13 22:39:04 INFO DAGScheduler: Final stage: ResultStage 23 (count at null:-2)
17/01/13 22:39:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 22)
17/01/13 22:39:04 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 22)
17/01/13 22:39:04 INFO DAGScheduler: Submitting ShuffleMapStage 22 (MapPartitionsRDD[92] at count at null:-2), which has no missing parents
17/01/13 22:39:04 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 28.8 KB, free 28.7 MB)
17/01/13 22:39:04 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 11.7 KB, free 28.7 MB)
17/01/13 22:39:04 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on localhost:62928 (size: 11.7 KB, free: 483.1 MB)
17/01/13 22:39:04 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1006
17/01/13 22:39:04 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[92] at count at null:-2)
17/01/13 22:39:04 INFO TaskSchedulerImpl: Adding task set 22.0 with 2 tasks
17/01/13 22:39:04 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 28, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:39:04 INFO TaskSetManager: Starting task 1.0 in stage 22.0 (TID 29, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:39:04 INFO Executor: Running task 0.0 in stage 22.0 (TID 28)
17/01/13 22:39:04 INFO BlockManager: Found block rdd_42_0 locally
17/01/13 22:39:04 INFO Executor: Running task 1.0 in stage 22.0 (TID 29)
17/01/13 22:39:04 INFO BlockManager: Found block rdd_42_1 locally
17/01/13 22:39:04 INFO GeneratePredicate: Code generated in 35.49693 ms
17/01/13 22:39:04 INFO GenerateColumnAccessor: Code generated in 15.709426 ms
17/01/13 22:39:05 INFO GeneratePredicate: Code generated in 4.974929 ms
17/01/13 22:39:05 INFO Executor: Finished task 1.0 in stage 22.0 (TID 29). 2808 bytes result sent to driver
17/01/13 22:39:05 INFO TaskSetManager: Finished task 1.0 in stage 22.0 (TID 29) in 190 ms on localhost (1/2)
17/01/13 22:39:05 INFO Executor: Finished task 0.0 in stage 22.0 (TID 28). 2808 bytes result sent to driver
17/01/13 22:39:05 INFO DAGScheduler: ShuffleMapStage 22 (count at null:-2) finished in 0.210 s
17/01/13 22:39:05 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:39:05 INFO DAGScheduler: running: Set()
17/01/13 22:39:05 INFO DAGScheduler: waiting: Set(ResultStage 23)
17/01/13 22:39:05 INFO DAGScheduler: failed: Set()
17/01/13 22:39:05 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[95] at count at null:-2), which has no missing parents
17/01/13 22:39:05 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 10.8 KB, free 28.8 MB)
17/01/13 22:39:05 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 28) in 209 ms on localhost (2/2)
17/01/13 22:39:05 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
17/01/13 22:39:05 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 5.3 KB, free 28.8 MB)
17/01/13 22:39:05 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on localhost:62928 (size: 5.3 KB, free: 483.1 MB)
17/01/13 22:39:05 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1006
17/01/13 22:39:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[95] at count at null:-2)
17/01/13 22:39:05 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks
17/01/13 22:39:05 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 30, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 22:39:05 INFO Executor: Running task 0.0 in stage 23.0 (TID 30)
17/01/13 22:39:05 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:39:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
17/01/13 22:39:05 INFO Executor: Finished task 0.0 in stage 23.0 (TID 30). 1959 bytes result sent to driver
17/01/13 22:39:05 INFO DAGScheduler: ResultStage 23 (count at null:-2) finished in 0.013 s
17/01/13 22:39:05 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 30) in 12 ms on localhost (1/1)
17/01/13 22:39:05 INFO DAGScheduler: Job 16 finished: count at null:-2, took 0.237919 s
17/01/13 22:39:05 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
17/01/13 22:39:05 INFO ParseDriver: Parsing command: SELECT *
FROM `flights`
WHERE (`dep_delay` = 2.0)
17/01/13 22:39:05 INFO ParseDriver: Parse Completed
17/01/13 22:39:05 INFO InMemoryColumnarTableScan: Predicate (dep_delay#146 = 2.0) generates partition filter: ((dep_delay.lowerBound#1170 <= 2.0) && (2.0 <= dep_delay.upperBound#1169))
17/01/13 22:39:05 INFO SparkContext: Starting job: count at null:-2
17/01/13 22:39:05 INFO DAGScheduler: Registering RDD 100 (count at null:-2)
17/01/13 22:39:05 INFO DAGScheduler: Got job 17 (count at null:-2) with 1 output partitions
17/01/13 22:39:05 INFO DAGScheduler: Final stage: ResultStage 25 (count at null:-2)
17/01/13 22:39:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 24)
17/01/13 22:39:05 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 24)
17/01/13 22:39:05 INFO DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[100] at count at null:-2), which has no missing parents
17/01/13 22:39:05 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 28.8 KB, free 28.8 MB)
17/01/13 22:39:05 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 11.8 KB, free 28.8 MB)
17/01/13 22:39:05 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on localhost:62928 (size: 11.8 KB, free: 483.1 MB)
17/01/13 22:39:05 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1006
17/01/13 22:39:05 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[100] at count at null:-2)
17/01/13 22:39:05 INFO TaskSchedulerImpl: Adding task set 24.0 with 2 tasks
17/01/13 22:39:05 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 31, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:39:05 INFO TaskSetManager: Starting task 1.0 in stage 24.0 (TID 32, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:39:05 INFO Executor: Running task 0.0 in stage 24.0 (TID 31)
17/01/13 22:39:05 INFO Executor: Running task 1.0 in stage 24.0 (TID 32)
17/01/13 22:39:05 INFO BlockManager: Found block rdd_42_0 locally
17/01/13 22:39:05 INFO BlockManager: Found block rdd_42_1 locally
17/01/13 22:39:05 INFO Executor: Finished task 0.0 in stage 24.0 (TID 31). 2808 bytes result sent to driver
17/01/13 22:39:05 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 31) in 62 ms on localhost (1/2)
17/01/13 22:39:05 INFO Executor: Finished task 1.0 in stage 24.0 (TID 32). 2808 bytes result sent to driver
17/01/13 22:39:05 INFO DAGScheduler: ShuffleMapStage 24 (count at null:-2) finished in 0.075 s
17/01/13 22:39:05 INFO TaskSetManager: Finished task 1.0 in stage 24.0 (TID 32) in 74 ms on localhost (2/2)
17/01/13 22:39:05 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:39:05 INFO DAGScheduler: running: Set()
17/01/13 22:39:05 INFO DAGScheduler: waiting: Set(ResultStage 25)
17/01/13 22:39:05 INFO DAGScheduler: failed: Set()
17/01/13 22:39:05 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[103] at count at null:-2), which has no missing parents
17/01/13 22:39:05 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
17/01/13 22:39:05 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 10.8 KB, free 28.8 MB)
17/01/13 22:39:05 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 5.3 KB, free 28.8 MB)
17/01/13 22:39:05 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on localhost:62928 (size: 5.3 KB, free: 483.1 MB)
17/01/13 22:39:05 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1006
17/01/13 22:39:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[103] at count at null:-2)
17/01/13 22:39:05 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks
17/01/13 22:39:05 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 33, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 22:39:05 INFO Executor: Running task 0.0 in stage 25.0 (TID 33)
17/01/13 22:39:05 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:39:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:39:05 INFO Executor: Finished task 0.0 in stage 25.0 (TID 33). 1959 bytes result sent to driver
17/01/13 22:39:05 INFO DAGScheduler: ResultStage 25 (count at null:-2) finished in 0.009 s
17/01/13 22:39:05 INFO DAGScheduler: Job 17 finished: count at null:-2, took 0.095282 s
17/01/13 22:39:05 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 33) in 8 ms on localhost (1/1)
17/01/13 22:39:05 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
17/01/13 22:39:05 INFO ParseDriver: Parsing command: SELECT *
FROM (SELECT *
FROM `flights`
WHERE (`dep_delay` = 2.0)) `wqacuevvox`
LIMIT 10
17/01/13 22:39:05 INFO ParseDriver: Parse Completed
17/01/13 22:39:05 INFO InMemoryColumnarTableScan: Predicate (dep_delay#146 = 2.0) generates partition filter: ((dep_delay.lowerBound#1268 <= 2.0) && (2.0 <= dep_delay.upperBound#1267))
17/01/13 22:39:05 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 22:39:05 INFO DAGScheduler: Got job 18 (collect at utils.scala:195) with 1 output partitions
17/01/13 22:39:05 INFO DAGScheduler: Final stage: ResultStage 26 (collect at utils.scala:195)
17/01/13 22:39:05 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:39:05 INFO DAGScheduler: Missing parents: List()
17/01/13 22:39:05 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[107] at collect at utils.scala:195), which has no missing parents
17/01/13 22:39:05 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 25.3 KB, free 28.8 MB)
17/01/13 22:39:05 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 10.2 KB, free 28.9 MB)
17/01/13 22:39:05 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on localhost:62928 (size: 10.2 KB, free: 483.1 MB)
17/01/13 22:39:05 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1006
17/01/13 22:39:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[107] at collect at utils.scala:195)
17/01/13 22:39:05 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks
17/01/13 22:39:05 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 34, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 22:39:05 INFO Executor: Running task 0.0 in stage 26.0 (TID 34)
17/01/13 22:39:05 INFO BlockManager: Found block rdd_42_0 locally
17/01/13 22:39:05 INFO GenerateColumnAccessor: Code generated in 22.360301 ms
17/01/13 22:39:05 INFO GeneratePredicate: Code generated in 3.68085 ms
17/01/13 22:39:05 INFO GenerateSafeProjection: Code generated in 16.469746 ms
17/01/13 22:39:05 INFO Executor: Finished task 0.0 in stage 26.0 (TID 34). 5325 bytes result sent to driver
17/01/13 22:39:05 INFO DAGScheduler: ResultStage 26 (collect at utils.scala:195) finished in 0.082 s
17/01/13 22:39:05 INFO DAGScheduler: Job 18 finished: collect at utils.scala:195, took 0.088415 s
17/01/13 22:39:05 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 34) in 81 ms on localhost (1/1)
17/01/13 22:39:05 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
17/01/13 22:39:05 INFO ParseDriver: Parsing command: SELECT *
FROM (SELECT `tailnum`, count(*) AS `count`, AVG(`distance`) AS `dist`, AVG(`arr_delay`) AS `delay`
FROM `flights`
GROUP BY `tailnum`) `pkgjlrxkmz`
WHERE ((`count` > 20.0) AND (`dist` < 2000.0) AND (NOT((`delay`) IS NULL)))
17/01/13 22:39:05 INFO ParseDriver: Parse Completed
17/01/13 22:39:05 INFO BlockManagerInfo: Removed broadcast_35_piece0 on localhost:62928 in memory (size: 10.2 KB, free: 483.1 MB)
17/01/13 22:39:05 INFO ContextCleaner: Cleaned accumulator 107
17/01/13 22:39:05 INFO BlockManagerInfo: Removed broadcast_34_piece0 on localhost:62928 in memory (size: 5.3 KB, free: 483.1 MB)
17/01/13 22:39:05 INFO ContextCleaner: Cleaned accumulator 104
17/01/13 22:39:05 INFO BlockManagerInfo: Removed broadcast_33_piece0 on localhost:62928 in memory (size: 11.8 KB, free: 483.1 MB)
17/01/13 22:39:05 INFO ContextCleaner: Cleaned accumulator 103
17/01/13 22:39:05 INFO ContextCleaner: Cleaned shuffle 7
17/01/13 22:39:05 INFO ContextCleaner: Cleaned accumulator 102
17/01/13 22:39:05 INFO ContextCleaner: Cleaned accumulator 101
17/01/13 22:39:05 INFO ContextCleaner: Cleaned accumulator 100
17/01/13 22:39:05 INFO ContextCleaner: Cleaned accumulator 99
17/01/13 22:39:05 INFO ContextCleaner: Cleaned accumulator 98
17/01/13 22:39:05 INFO ContextCleaner: Cleaned accumulator 97
17/01/13 22:39:05 INFO ContextCleaner: Cleaned accumulator 96
17/01/13 22:39:05 INFO ContextCleaner: Cleaned accumulator 95
17/01/13 22:39:05 INFO ContextCleaner: Cleaned accumulator 94
17/01/13 22:39:05 INFO ContextCleaner: Cleaned accumulator 93
17/01/13 22:39:05 INFO ContextCleaner: Cleaned accumulator 92
17/01/13 22:39:05 INFO BlockManagerInfo: Removed broadcast_32_piece0 on localhost:62928 in memory (size: 5.3 KB, free: 483.1 MB)
17/01/13 22:39:05 INFO ContextCleaner: Cleaned accumulator 91
17/01/13 22:39:05 INFO BlockManagerInfo: Removed broadcast_31_piece0 on localhost:62928 in memory (size: 11.7 KB, free: 483.1 MB)
17/01/13 22:39:05 INFO ContextCleaner: Cleaned accumulator 90
17/01/13 22:39:05 INFO ContextCleaner: Cleaned shuffle 6
17/01/13 22:39:05 INFO ContextCleaner: Cleaned accumulator 89
17/01/13 22:39:05 INFO ContextCleaner: Cleaned accumulator 88
17/01/13 22:39:05 INFO ContextCleaner: Cleaned accumulator 87
17/01/13 22:39:05 INFO ContextCleaner: Cleaned accumulator 86
17/01/13 22:39:05 INFO ContextCleaner: Cleaned accumulator 85
17/01/13 22:39:05 INFO ContextCleaner: Cleaned accumulator 84
17/01/13 22:39:05 INFO ContextCleaner: Cleaned accumulator 83
17/01/13 22:39:05 INFO ContextCleaner: Cleaned accumulator 82
17/01/13 22:39:05 INFO ContextCleaner: Cleaned accumulator 81
17/01/13 22:39:05 INFO ContextCleaner: Cleaned accumulator 80
17/01/13 22:39:05 INFO ContextCleaner: Cleaned accumulator 79
17/01/13 22:39:05 INFO BlockManagerInfo: Removed broadcast_30_piece0 on localhost:62928 in memory (size: 3.0 KB, free: 483.1 MB)
17/01/13 22:39:05 INFO ContextCleaner: Cleaned accumulator 78
17/01/13 22:39:05 INFO ContextCleaner: Cleaned accumulator 77
17/01/13 22:39:05 INFO BlockManagerInfo: Removed broadcast_29_piece0 on localhost:62928 in memory (size: 4.7 KB, free: 483.1 MB)
17/01/13 22:39:05 INFO ContextCleaner: Cleaned accumulator 76
17/01/13 22:39:05 INFO BlockManagerInfo: Removed broadcast_28_piece0 on localhost:62928 in memory (size: 11.2 KB, free: 483.2 MB)
17/01/13 22:39:05 INFO ContextCleaner: Cleaned accumulator 75
17/01/13 22:39:05 INFO ContextCleaner: Cleaned shuffle 5
17/01/13 22:39:05 INFO BlockManagerInfo: Removed broadcast_27_piece0 on localhost:62928 in memory (size: 4.6 KB, free: 483.2 MB)
17/01/13 22:39:05 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 22:39:05 INFO DAGScheduler: Registering RDD 110 (collect at utils.scala:195)
17/01/13 22:39:05 INFO DAGScheduler: Got job 19 (collect at utils.scala:195) with 4 output partitions
17/01/13 22:39:05 INFO DAGScheduler: Final stage: ResultStage 28 (collect at utils.scala:195)
17/01/13 22:39:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)
17/01/13 22:39:05 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 27)
17/01/13 22:39:05 INFO DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[110] at collect at utils.scala:195), which has no missing parents
17/01/13 22:39:05 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 28.1 KB, free 28.7 MB)
17/01/13 22:39:05 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 11.4 KB, free 28.7 MB)
17/01/13 22:39:05 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on localhost:62928 (size: 11.4 KB, free: 483.1 MB)
17/01/13 22:39:05 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1006
17/01/13 22:39:05 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[110] at collect at utils.scala:195)
17/01/13 22:39:05 INFO TaskSchedulerImpl: Adding task set 27.0 with 2 tasks
17/01/13 22:39:05 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 35, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:39:05 INFO TaskSetManager: Starting task 1.0 in stage 27.0 (TID 36, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:39:05 INFO Executor: Running task 0.0 in stage 27.0 (TID 35)
17/01/13 22:39:05 INFO Executor: Running task 1.0 in stage 27.0 (TID 36)
17/01/13 22:39:05 INFO BlockManager: Found block rdd_42_0 locally
17/01/13 22:39:05 INFO BlockManager: Found block rdd_42_1 locally
17/01/13 22:39:05 INFO GenerateColumnAccessor: Code generated in 14.600094 ms
17/01/13 22:39:05 INFO GenerateMutableProjection: Code generated in 9.577378 ms
17/01/13 22:39:06 INFO GenerateMutableProjection: Code generated in 15.238387 ms
17/01/13 22:39:06 INFO GenerateUnsafeRowJoiner: Code generated in 7.769593 ms
17/01/13 22:39:06 INFO GenerateUnsafeProjection: Code generated in 6.918821 ms
17/01/13 22:39:06 INFO GenerateMutableProjection: Code generated in 7.150927 ms
17/01/13 22:39:06 INFO Executor: Finished task 1.0 in stage 27.0 (TID 36). 2682 bytes result sent to driver
17/01/13 22:39:06 INFO TaskSetManager: Finished task 1.0 in stage 27.0 (TID 36) in 370 ms on localhost (1/2)
17/01/13 22:39:06 INFO Executor: Finished task 0.0 in stage 27.0 (TID 35). 2682 bytes result sent to driver
17/01/13 22:39:06 INFO DAGScheduler: ShuffleMapStage 27 (collect at utils.scala:195) finished in 0.373 s
17/01/13 22:39:06 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 35) in 373 ms on localhost (2/2)
17/01/13 22:39:06 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:39:06 INFO DAGScheduler: running: Set()
17/01/13 22:39:06 INFO DAGScheduler: waiting: Set(ResultStage 28)
17/01/13 22:39:06 INFO DAGScheduler: failed: Set()
17/01/13 22:39:06 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[115] at collect at utils.scala:195), which has no missing parents
17/01/13 22:39:06 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 13.5 KB, free 28.7 MB)
17/01/13 22:39:06 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 6.2 KB, free 28.7 MB)
17/01/13 22:39:06 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
17/01/13 22:39:06 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on localhost:62928 (size: 6.2 KB, free: 483.1 MB)
17/01/13 22:39:06 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1006
17/01/13 22:39:06 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 28 (MapPartitionsRDD[115] at collect at utils.scala:195)
17/01/13 22:39:06 INFO TaskSchedulerImpl: Adding task set 28.0 with 4 tasks
17/01/13 22:39:06 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 37, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 22:39:06 INFO TaskSetManager: Starting task 1.0 in stage 28.0 (TID 38, localhost, partition 1,NODE_LOCAL, 2242 bytes)
17/01/13 22:39:06 INFO TaskSetManager: Starting task 2.0 in stage 28.0 (TID 39, localhost, partition 2,NODE_LOCAL, 2242 bytes)
17/01/13 22:39:06 INFO TaskSetManager: Starting task 3.0 in stage 28.0 (TID 40, localhost, partition 3,NODE_LOCAL, 2242 bytes)
17/01/13 22:39:06 INFO Executor: Running task 0.0 in stage 28.0 (TID 37)
17/01/13 22:39:06 INFO Executor: Running task 1.0 in stage 28.0 (TID 38)
17/01/13 22:39:06 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:39:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/01/13 22:39:06 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:39:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:39:06 INFO Executor: Running task 2.0 in stage 28.0 (TID 39)
17/01/13 22:39:06 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:39:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:39:06 INFO Executor: Running task 3.0 in stage 28.0 (TID 40)
17/01/13 22:39:06 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:39:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:39:06 INFO GenerateMutableProjection: Code generated in 44.946308 ms
17/01/13 22:39:06 INFO GenerateMutableProjection: Code generated in 10.520738 ms
17/01/13 22:39:06 INFO GenerateUnsafeProjection: Code generated in 8.209487 ms
17/01/13 22:39:06 INFO GeneratePredicate: Code generated in 18.389744 ms
17/01/13 22:39:06 INFO Executor: Finished task 3.0 in stage 28.0 (TID 40). 48431 bytes result sent to driver
17/01/13 22:39:06 INFO Executor: Finished task 1.0 in stage 28.0 (TID 38). 52153 bytes result sent to driver
17/01/13 22:39:06 INFO Executor: Finished task 2.0 in stage 28.0 (TID 39). 50586 bytes result sent to driver
17/01/13 22:39:06 INFO Executor: Finished task 0.0 in stage 28.0 (TID 37). 49931 bytes result sent to driver
17/01/13 22:39:06 INFO TaskSetManager: Finished task 1.0 in stage 28.0 (TID 38) in 149 ms on localhost (1/4)
17/01/13 22:39:06 INFO TaskSetManager: Finished task 3.0 in stage 28.0 (TID 40) in 149 ms on localhost (2/4)
17/01/13 22:39:06 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 37) in 151 ms on localhost (3/4)
17/01/13 22:39:06 INFO DAGScheduler: ResultStage 28 (collect at utils.scala:195) finished in 0.153 s
17/01/13 22:39:06 INFO DAGScheduler: Job 19 finished: collect at utils.scala:195, took 0.539668 s
17/01/13 22:39:06 INFO TaskSetManager: Finished task 2.0 in stage 28.0 (TID 39) in 152 ms on localhost (4/4)
17/01/13 22:39:06 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
17/01/13 22:39:07 INFO SparkContext: Invoking stop() from shutdown hook
17/01/13 22:39:07 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
17/01/13 22:39:07 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/01/13 22:39:07 INFO MemoryStore: MemoryStore cleared
17/01/13 22:39:07 INFO BlockManager: BlockManager stopped
17/01/13 22:39:07 INFO BlockManagerMaster: BlockManagerMaster stopped
17/01/13 22:39:07 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/01/13 22:39:07 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\Baoco\AppData\Local\Temp\spark-c537a403-ca67-411f-bcab-27aab16e4c1e\userFiles-9e5d75fe-be45-4911-a388-5568494c92a5
java.io.IOException: Failed to delete: C:\Users\Baoco\AppData\Local\Temp\spark-c537a403-ca67-411f-bcab-27aab16e4c1e\userFiles-9e5d75fe-be45-4911-a388-5568494c92a5
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:119)
	at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1219)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1755)
	at org.apache.spark.SparkContext$$anonfun$3.apply$mcV$sp(SparkContext.scala:596)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/01/13 22:39:07 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
17/01/13 22:39:07 INFO SparkContext: Successfully stopped SparkContext
17/01/13 22:39:07 INFO ShutdownHookManager: Shutdown hook called
17/01/13 22:39:07 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
17/01/13 22:39:07 INFO ShutdownHookManager: Deleting directory C:\Users\Baoco\AppData\Local\Temp\spark-55613523-ef35-4fdd-9ed0-2eb2b9f0487e
17/01/13 22:39:07 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
17/01/13 22:39:07 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\Baoco\AppData\Local\Temp\spark-55613523-ef35-4fdd-9ed0-2eb2b9f0487e
java.io.IOException: Failed to delete: C:\Users\Baoco\AppData\Local\Temp\spark-55613523-ef35-4fdd-9ed0-2eb2b9f0487e
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/01/13 22:39:07 INFO ShutdownHookManager: Deleting directory C:\Users\Baoco\AppData\Local\Temp\spark-c537a403-ca67-411f-bcab-27aab16e4c1e\httpd-f5ab8bfd-8903-4b03-9f58-a6b9dc4233a9
17/01/13 22:39:07 INFO ShutdownHookManager: Deleting directory C:\Users\Baoco\AppData\Local\Temp\spark-c537a403-ca67-411f-bcab-27aab16e4c1e
17/01/13 22:39:07 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\Baoco\AppData\Local\Temp\spark-c537a403-ca67-411f-bcab-27aab16e4c1e
java.io.IOException: Failed to delete: C:\Users\Baoco\AppData\Local\Temp\spark-c537a403-ca67-411f-bcab-27aab16e4c1e
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/01/13 22:39:07 INFO ShutdownHookManager: Deleting directory C:\Users\Baoco\AppData\Local\Temp\spark-c537a403-ca67-411f-bcab-27aab16e4c1e\userFiles-9e5d75fe-be45-4911-a388-5568494c92a5
17/01/13 22:39:07 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\Baoco\AppData\Local\Temp\spark-c537a403-ca67-411f-bcab-27aab16e4c1e\userFiles-9e5d75fe-be45-4911-a388-5568494c92a5
java.io.IOException: Failed to delete: C:\Users\Baoco\AppData\Local\Temp\spark-c537a403-ca67-411f-bcab-27aab16e4c1e\userFiles-9e5d75fe-be45-4911-a388-5568494c92a5
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/01/13 22:51:48 INFO SparkContext: Running Spark version 1.6.2
17/01/13 22:51:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/01/13 22:51:48 INFO SecurityManager: Changing view acls to: Baoco
17/01/13 22:51:48 INFO SecurityManager: Changing modify acls to: Baoco
17/01/13 22:51:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(Baoco); users with modify permissions: Set(Baoco)
17/01/13 22:51:49 INFO Utils: Successfully started service 'sparkDriver' on port 63324.
17/01/13 22:51:49 INFO Slf4jLogger: Slf4jLogger started
17/01/13 22:51:49 INFO Remoting: Starting remoting
17/01/13 22:51:49 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@127.0.0.1:63337]
17/01/13 22:51:49 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 63337.
17/01/13 22:51:49 INFO SparkEnv: Registering MapOutputTracker
17/01/13 22:51:49 INFO SparkEnv: Registering BlockManagerMaster
17/01/13 22:51:49 INFO DiskBlockManager: Created local directory at C:\Users\Baoco\AppData\Local\Temp\blockmgr-1f25b552-dbae-4fd4-a33b-f29aa2b3c4f8
17/01/13 22:51:49 INFO MemoryStore: MemoryStore started with capacity 511.1 MB
17/01/13 22:51:50 INFO SparkEnv: Registering OutputCommitCoordinator
17/01/13 22:51:50 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/01/13 22:51:50 INFO SparkUI: Started SparkUI at http://127.0.0.1:4040
17/01/13 22:51:50 INFO HttpFileServer: HTTP File server directory is C:\Users\Baoco\AppData\Local\Temp\spark-10b89bdd-6833-43c0-95ce-3710579b0b34\httpd-0b8099eb-234a-41df-9041-20aaeb1ec029
17/01/13 22:51:50 INFO HttpServer: Starting HTTP Server
17/01/13 22:51:50 INFO Utils: Successfully started service 'HTTP file server' on port 63340.
17/01/13 22:51:50 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/spark-csv_2.11-1.3.0.jar at http://127.0.0.1:63340/jars/spark-csv_2.11-1.3.0.jar with timestamp 1484376710315
17/01/13 22:51:50 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/commons-csv-1.1.jar at http://127.0.0.1:63340/jars/commons-csv-1.1.jar with timestamp 1484376710318
17/01/13 22:51:50 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/univocity-parsers-1.5.1.jar at http://127.0.0.1:63340/jars/univocity-parsers-1.5.1.jar with timestamp 1484376710320
17/01/13 22:51:50 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/sparklyr-1.6-2.10.jar at http://127.0.0.1:63340/jars/sparklyr-1.6-2.10.jar with timestamp 1484376710322
17/01/13 22:51:50 INFO Executor: Starting executor ID driver on host localhost
17/01/13 22:51:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63357.
17/01/13 22:51:50 INFO NettyBlockTransferService: Server created on 63357
17/01/13 22:51:50 INFO BlockManagerMaster: Trying to register BlockManager
17/01/13 22:51:50 INFO BlockManagerMasterEndpoint: Registering block manager localhost:63357 with 511.1 MB RAM, BlockManagerId(driver, localhost, 63357)
17/01/13 22:51:50 INFO BlockManagerMaster: Registered BlockManager
17/01/13 22:51:51 INFO HiveContext: Initializing execution hive, version 1.2.1
17/01/13 22:51:51 INFO ClientWrapper: Inspected Hadoop version: 2.6.0
17/01/13 22:51:51 INFO ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
17/01/13 22:51:51 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/01/13 22:51:51 INFO ObjectStore: ObjectStore, initialize called
17/01/13 22:51:52 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/01/13 22:51:52 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/01/13 22:51:52 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/01/13 22:51:52 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/01/13 22:51:59 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/01/13 22:52:00 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 22:52:00 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 22:52:07 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 22:52:07 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 22:52:10 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/01/13 22:52:10 INFO ObjectStore: Initialized ObjectStore
17/01/13 22:52:10 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/01/13 22:52:10 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/01/13 22:52:16 INFO HiveMetaStore: Added admin role in metastore
17/01/13 22:52:16 INFO HiveMetaStore: Added public role in metastore
17/01/13 22:52:17 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/01/13 22:52:17 INFO HiveMetaStore: 0: get_all_databases
17/01/13 22:52:17 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_all_databases	
17/01/13 22:52:17 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/01/13 22:52:17 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/01/13 22:52:17 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 22:52:20 INFO SessionState: Created local directory: C:/Users/Baoco/AppData/Local/Temp/0adcf0b8-69b2-477e-9400-434dc30da38c_resources
17/01/13 22:52:20 INFO SessionState: Created HDFS directory: C:/Users/Baoco/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/Baoco/0adcf0b8-69b2-477e-9400-434dc30da38c
17/01/13 22:52:20 INFO SessionState: Created local directory: C:/Users/Baoco/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/0adcf0b8-69b2-477e-9400-434dc30da38c
17/01/13 22:52:20 INFO SessionState: Created HDFS directory: C:/Users/Baoco/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/Baoco/0adcf0b8-69b2-477e-9400-434dc30da38c/_tmp_space.db
17/01/13 22:52:20 INFO HiveContext: default warehouse location is C:\Users\Baoco\AppData\Local\rstudio\spark\Cache\spark-1.6.2-bin-hadoop2.6\tmp\hive
17/01/13 22:52:20 INFO HiveContext: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/01/13 22:52:20 INFO ClientWrapper: Inspected Hadoop version: 2.6.0
17/01/13 22:52:20 INFO ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
17/01/13 22:52:21 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/01/13 22:52:21 INFO ObjectStore: ObjectStore, initialize called
17/01/13 22:52:21 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/01/13 22:52:21 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/01/13 22:52:21 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/01/13 22:52:21 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/01/13 22:52:22 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/01/13 22:52:23 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 22:52:23 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 22:52:24 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 22:52:24 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 22:52:24 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/01/13 22:52:24 INFO ObjectStore: Initialized ObjectStore
17/01/13 22:52:24 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/01/13 22:52:24 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/01/13 22:52:24 INFO HiveMetaStore: Added admin role in metastore
17/01/13 22:52:24 INFO HiveMetaStore: Added public role in metastore
17/01/13 22:52:24 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/01/13 22:52:24 INFO HiveMetaStore: 0: get_all_databases
17/01/13 22:52:24 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_all_databases	
17/01/13 22:52:24 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/01/13 22:52:24 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/01/13 22:52:24 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 22:52:25 INFO SessionState: Created local directory: C:/Users/Baoco/AppData/Local/Temp/49bfae9f-edd7-46cf-839b-186058a34218_resources
17/01/13 22:52:25 INFO SessionState: Created HDFS directory: C:/Users/Baoco/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/Baoco/49bfae9f-edd7-46cf-839b-186058a34218
17/01/13 22:52:25 INFO SessionState: Created local directory: C:/Users/Baoco/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/49bfae9f-edd7-46cf-839b-186058a34218
17/01/13 22:52:25 INFO SessionState: Created HDFS directory: C:/Users/Baoco/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/Baoco/49bfae9f-edd7-46cf-839b-186058a34218/_tmp_space.db
17/01/13 22:52:36 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/01/13 22:52:36 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/01/13 22:52:37 INFO SparkContext: Starting job: collect at utils.scala:59
17/01/13 22:52:37 INFO DAGScheduler: Got job 0 (collect at utils.scala:59) with 1 output partitions
17/01/13 22:52:37 INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:59)
17/01/13 22:52:37 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:52:37 INFO DAGScheduler: Missing parents: List()
17/01/13 22:52:37 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at map at utils.scala:56), which has no missing parents
17/01/13 22:52:37 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 5.4 KB, free 5.4 KB)
17/01/13 22:52:37 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.0 KB, free 8.4 KB)
17/01/13 22:52:37 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:63357 (size: 3.0 KB, free: 511.1 MB)
17/01/13 22:52:37 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
17/01/13 22:52:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at map at utils.scala:56)
17/01/13 22:52:37 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
17/01/13 22:52:37 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2348 bytes)
17/01/13 22:52:37 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/01/13 22:52:37 INFO Executor: Fetching http://127.0.0.1:63340/jars/sparklyr-1.6-2.10.jar with timestamp 1484376710322
17/01/13 22:52:37 INFO Utils: Fetching http://127.0.0.1:63340/jars/sparklyr-1.6-2.10.jar to C:\Users\Baoco\AppData\Local\Temp\spark-10b89bdd-6833-43c0-95ce-3710579b0b34\userFiles-23993f52-85b5-43a0-bb2f-45bfe38dc904\fetchFileTemp6142074330989414278.tmp
17/01/13 22:52:37 INFO Executor: Adding file:/C:/Users/Baoco/AppData/Local/Temp/spark-10b89bdd-6833-43c0-95ce-3710579b0b34/userFiles-23993f52-85b5-43a0-bb2f-45bfe38dc904/sparklyr-1.6-2.10.jar to class loader
17/01/13 22:52:37 INFO Executor: Fetching http://127.0.0.1:63340/jars/univocity-parsers-1.5.1.jar with timestamp 1484376710320
17/01/13 22:52:37 INFO Utils: Fetching http://127.0.0.1:63340/jars/univocity-parsers-1.5.1.jar to C:\Users\Baoco\AppData\Local\Temp\spark-10b89bdd-6833-43c0-95ce-3710579b0b34\userFiles-23993f52-85b5-43a0-bb2f-45bfe38dc904\fetchFileTemp4864492981456118348.tmp
17/01/13 22:52:37 INFO Executor: Adding file:/C:/Users/Baoco/AppData/Local/Temp/spark-10b89bdd-6833-43c0-95ce-3710579b0b34/userFiles-23993f52-85b5-43a0-bb2f-45bfe38dc904/univocity-parsers-1.5.1.jar to class loader
17/01/13 22:52:37 INFO Executor: Fetching http://127.0.0.1:63340/jars/spark-csv_2.11-1.3.0.jar with timestamp 1484376710315
17/01/13 22:52:37 INFO Utils: Fetching http://127.0.0.1:63340/jars/spark-csv_2.11-1.3.0.jar to C:\Users\Baoco\AppData\Local\Temp\spark-10b89bdd-6833-43c0-95ce-3710579b0b34\userFiles-23993f52-85b5-43a0-bb2f-45bfe38dc904\fetchFileTemp5098989802559702098.tmp
17/01/13 22:52:38 INFO Executor: Adding file:/C:/Users/Baoco/AppData/Local/Temp/spark-10b89bdd-6833-43c0-95ce-3710579b0b34/userFiles-23993f52-85b5-43a0-bb2f-45bfe38dc904/spark-csv_2.11-1.3.0.jar to class loader
17/01/13 22:52:38 INFO Executor: Fetching http://127.0.0.1:63340/jars/commons-csv-1.1.jar with timestamp 1484376710318
17/01/13 22:52:38 INFO Utils: Fetching http://127.0.0.1:63340/jars/commons-csv-1.1.jar to C:\Users\Baoco\AppData\Local\Temp\spark-10b89bdd-6833-43c0-95ce-3710579b0b34\userFiles-23993f52-85b5-43a0-bb2f-45bfe38dc904\fetchFileTemp4887026012915506373.tmp
17/01/13 22:52:38 INFO Executor: Adding file:/C:/Users/Baoco/AppData/Local/Temp/spark-10b89bdd-6833-43c0-95ce-3710579b0b34/userFiles-23993f52-85b5-43a0-bb2f-45bfe38dc904/commons-csv-1.1.jar to class loader
17/01/13 22:52:38 INFO GenerateUnsafeProjection: Code generated in 246.59691 ms
17/01/13 22:52:38 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1060 bytes result sent to driver
17/01/13 22:52:38 INFO DAGScheduler: ResultStage 0 (collect at utils.scala:59) finished in 1.021 s
17/01/13 22:52:38 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1005 ms on localhost (1/1)
17/01/13 22:52:38 INFO DAGScheduler: Job 0 finished: collect at utils.scala:59, took 1.264308 s
17/01/13 22:52:38 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/01/13 22:52:38 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 61.8 KB, free 70.2 KB)
17/01/13 22:52:38 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.3 KB, free 89.5 KB)
17/01/13 22:52:38 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:63357 (size: 19.3 KB, free: 511.1 MB)
17/01/13 22:52:38 INFO SparkContext: Created broadcast 1 from textFile at TextFile.scala:30
17/01/13 22:52:38 INFO FileInputFormat: Total input paths to process : 1
17/01/13 22:52:38 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/01/13 22:52:38 INFO DAGScheduler: Got job 1 (take at CsvRelation.scala:249) with 1 output partitions
17/01/13 22:52:38 INFO DAGScheduler: Final stage: ResultStage 1 (take at CsvRelation.scala:249)
17/01/13 22:52:38 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:52:38 INFO DAGScheduler: Missing parents: List()
17/01/13 22:52:38 INFO DAGScheduler: Submitting ResultStage 1 (C:\Users\Baoco\AppData\Local\Temp\RtmpGQdUfI/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv MapPartitionsRDD[5] at textFile at TextFile.scala:30), which has no missing parents
17/01/13 22:52:38 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 3.2 KB, free 92.7 KB)
17/01/13 22:52:38 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 1945.0 B, free 94.6 KB)
17/01/13 22:52:38 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:63357 (size: 1945.0 B, free: 511.1 MB)
17/01/13 22:52:38 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
17/01/13 22:52:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (C:\Users\Baoco\AppData\Local\Temp\RtmpGQdUfI/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv MapPartitionsRDD[5] at textFile at TextFile.scala:30)
17/01/13 22:52:38 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
17/01/13 22:52:38 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 22:52:38 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
17/01/13 22:52:38 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpGQdUfI/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv:0+2088
17/01/13 22:52:38 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
17/01/13 22:52:38 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
17/01/13 22:52:38 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
17/01/13 22:52:38 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
17/01/13 22:52:38 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
17/01/13 22:52:38 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2354 bytes result sent to driver
17/01/13 22:52:38 INFO DAGScheduler: ResultStage 1 (take at CsvRelation.scala:249) finished in 0.059 s
17/01/13 22:52:38 INFO DAGScheduler: Job 1 finished: take at CsvRelation.scala:249, took 0.071317 s
17/01/13 22:52:38 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 58 ms on localhost (1/1)
17/01/13 22:52:38 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/01/13 22:52:38 INFO ParseDriver: Parsing command: SELECT * FROM  `iris`
17/01/13 22:52:39 INFO ParseDriver: Parse Completed
17/01/13 22:52:39 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 208.5 KB, free 303.1 KB)
17/01/13 22:52:39 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 19.3 KB, free 322.5 KB)
17/01/13 22:52:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:63357 (size: 19.3 KB, free: 511.1 MB)
17/01/13 22:52:39 INFO SparkContext: Created broadcast 3 from textFile at TextFile.scala:30
17/01/13 22:52:39 INFO FileInputFormat: Total input paths to process : 1
17/01/13 22:52:39 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/01/13 22:52:39 INFO DAGScheduler: Got job 2 (take at CsvRelation.scala:249) with 1 output partitions
17/01/13 22:52:39 INFO DAGScheduler: Final stage: ResultStage 2 (take at CsvRelation.scala:249)
17/01/13 22:52:39 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:52:39 INFO DAGScheduler: Missing parents: List()
17/01/13 22:52:39 INFO DAGScheduler: Submitting ResultStage 2 (C:\Users\Baoco\AppData\Local\Temp\RtmpGQdUfI/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv MapPartitionsRDD[7] at textFile at TextFile.scala:30), which has no missing parents
17/01/13 22:52:39 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 3.2 KB, free 325.6 KB)
17/01/13 22:52:39 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 1946.0 B, free 327.5 KB)
17/01/13 22:52:39 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:63357 (size: 1946.0 B, free: 511.1 MB)
17/01/13 22:52:39 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1006
17/01/13 22:52:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (C:\Users\Baoco\AppData\Local\Temp\RtmpGQdUfI/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv MapPartitionsRDD[7] at textFile at TextFile.scala:30)
17/01/13 22:52:39 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
17/01/13 22:52:39 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 22:52:39 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
17/01/13 22:52:39 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpGQdUfI/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv:0+2088
17/01/13 22:52:39 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2354 bytes result sent to driver
17/01/13 22:52:39 INFO DAGScheduler: ResultStage 2 (take at CsvRelation.scala:249) finished in 0.022 s
17/01/13 22:52:39 INFO DAGScheduler: Job 2 finished: take at CsvRelation.scala:249, took 0.032131 s
17/01/13 22:52:39 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 21 ms on localhost (1/1)
17/01/13 22:52:39 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
17/01/13 22:52:39 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 208.5 KB, free 536.1 KB)
17/01/13 22:52:39 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 19.3 KB, free 555.4 KB)
17/01/13 22:52:39 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:63357 (size: 19.3 KB, free: 511.1 MB)
17/01/13 22:52:39 INFO SparkContext: Created broadcast 5 from textFile at TextFile.scala:30
17/01/13 22:52:40 INFO FileInputFormat: Total input paths to process : 1
17/01/13 22:52:40 INFO SparkContext: Starting job: sql at null:-2
17/01/13 22:52:40 INFO DAGScheduler: Registering RDD 17 (sql at null:-2)
17/01/13 22:52:40 INFO DAGScheduler: Got job 3 (sql at null:-2) with 1 output partitions
17/01/13 22:52:40 INFO DAGScheduler: Final stage: ResultStage 4 (sql at null:-2)
17/01/13 22:52:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
17/01/13 22:52:40 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
17/01/13 22:52:40 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[17] at sql at null:-2), which has no missing parents
17/01/13 22:52:40 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 18.3 KB, free 573.7 KB)
17/01/13 22:52:40 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 8.7 KB, free 582.3 KB)
17/01/13 22:52:40 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:63357 (size: 8.7 KB, free: 511.1 MB)
17/01/13 22:52:40 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1006
17/01/13 22:52:40 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[17] at sql at null:-2)
17/01/13 22:52:40 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks
17/01/13 22:52:40 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:52:40 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 4, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:52:40 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
17/01/13 22:52:40 INFO Executor: Running task 1.0 in stage 3.0 (TID 4)
17/01/13 22:52:40 INFO CacheManager: Partition rdd_14_1 not found, computing it
17/01/13 22:52:40 INFO CacheManager: Partition rdd_14_0 not found, computing it
17/01/13 22:52:40 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpGQdUfI/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv:0+2088
17/01/13 22:52:40 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpGQdUfI/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv:2088+2089
17/01/13 22:52:40 INFO GenerateUnsafeProjection: Code generated in 19.766597 ms
17/01/13 22:52:40 INFO MemoryStore: Block rdd_14_0 stored as values in memory (estimated size 3.2 KB, free 585.5 KB)
17/01/13 22:52:40 INFO MemoryStore: Block rdd_14_1 stored as values in memory (estimated size 3.1 KB, free 588.7 KB)
17/01/13 22:52:40 INFO BlockManagerInfo: Added rdd_14_0 in memory on localhost:63357 (size: 3.2 KB, free: 511.1 MB)
17/01/13 22:52:40 INFO BlockManagerInfo: Added rdd_14_1 in memory on localhost:63357 (size: 3.1 KB, free: 511.0 MB)
17/01/13 22:52:40 INFO GeneratePredicate: Code generated in 9.146018 ms
17/01/13 22:52:40 INFO GenerateColumnAccessor: Code generated in 19.618544 ms
17/01/13 22:52:40 INFO GenerateMutableProjection: Code generated in 10.047565 ms
17/01/13 22:52:40 INFO GenerateUnsafeProjection: Code generated in 9.104632 ms
17/01/13 22:52:40 INFO GenerateMutableProjection: Code generated in 11.926177 ms
17/01/13 22:52:40 INFO GenerateUnsafeRowJoiner: Code generated in 8.637006 ms
17/01/13 22:52:40 INFO GenerateUnsafeProjection: Code generated in 7.8289 ms
17/01/13 22:52:40 INFO Executor: Finished task 1.0 in stage 3.0 (TID 4). 3787 bytes result sent to driver
17/01/13 22:52:40 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3784 bytes result sent to driver
17/01/13 22:52:40 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 407 ms on localhost (1/2)
17/01/13 22:52:40 INFO DAGScheduler: ShuffleMapStage 3 (sql at null:-2) finished in 0.435 s
17/01/13 22:52:40 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:52:40 INFO DAGScheduler: running: Set()
17/01/13 22:52:40 INFO DAGScheduler: waiting: Set(ResultStage 4)
17/01/13 22:52:40 INFO DAGScheduler: failed: Set()
17/01/13 22:52:40 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[20] at sql at null:-2), which has no missing parents
17/01/13 22:52:40 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 9.3 KB, free 597.9 KB)
17/01/13 22:52:40 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 4.6 KB, free 602.5 KB)
17/01/13 22:52:40 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:63357 (size: 4.6 KB, free: 511.0 MB)
17/01/13 22:52:40 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1006
17/01/13 22:52:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[20] at sql at null:-2)
17/01/13 22:52:40 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
17/01/13 22:52:40 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 4) in 433 ms on localhost (2/2)
17/01/13 22:52:40 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
17/01/13 22:52:40 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 5, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 22:52:40 INFO Executor: Running task 0.0 in stage 4.0 (TID 5)
17/01/13 22:52:40 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:52:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
17/01/13 22:52:40 INFO GenerateMutableProjection: Code generated in 8.510286 ms
17/01/13 22:52:40 INFO GenerateMutableProjection: Code generated in 41.579912 ms
17/01/13 22:52:40 INFO Executor: Finished task 0.0 in stage 4.0 (TID 5). 1830 bytes result sent to driver
17/01/13 22:52:40 INFO DAGScheduler: ResultStage 4 (sql at null:-2) finished in 0.209 s
17/01/13 22:52:40 INFO DAGScheduler: Job 3 finished: sql at null:-2, took 0.706993 s
17/01/13 22:52:40 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 5) in 209 ms on localhost (1/1)
17/01/13 22:52:40 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
17/01/13 22:52:40 INFO ParseDriver: Parsing command: SELECT count(*) FROM  `iris`
17/01/13 22:52:40 INFO ParseDriver: Parse Completed
17/01/13 22:52:41 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 22:52:41 INFO DAGScheduler: Registering RDD 24 (collect at utils.scala:195)
17/01/13 22:52:41 INFO DAGScheduler: Got job 4 (collect at utils.scala:195) with 1 output partitions
17/01/13 22:52:41 INFO DAGScheduler: Final stage: ResultStage 6 (collect at utils.scala:195)
17/01/13 22:52:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
17/01/13 22:52:41 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 5)
17/01/13 22:52:41 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[24] at collect at utils.scala:195), which has no missing parents
17/01/13 22:52:41 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 18.4 KB, free 620.9 KB)
17/01/13 22:52:41 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.7 KB, free 629.6 KB)
17/01/13 22:52:41 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:63357 (size: 8.7 KB, free: 511.0 MB)
17/01/13 22:52:41 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1006
17/01/13 22:52:41 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[24] at collect at utils.scala:195)
17/01/13 22:52:41 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks
17/01/13 22:52:41 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:52:41 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 7, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:52:41 INFO Executor: Running task 0.0 in stage 5.0 (TID 6)
17/01/13 22:52:41 INFO Executor: Running task 1.0 in stage 5.0 (TID 7)
17/01/13 22:52:41 INFO BlockManager: Found block rdd_14_1 locally
17/01/13 22:52:41 INFO BlockManager: Found block rdd_14_0 locally
17/01/13 22:52:41 INFO Executor: Finished task 1.0 in stage 5.0 (TID 7). 2679 bytes result sent to driver
17/01/13 22:52:41 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 7) in 54 ms on localhost (1/2)
17/01/13 22:52:41 INFO Executor: Finished task 0.0 in stage 5.0 (TID 6). 2679 bytes result sent to driver
17/01/13 22:52:41 INFO DAGScheduler: ShuffleMapStage 5 (collect at utils.scala:195) finished in 0.065 s
17/01/13 22:52:41 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:52:41 INFO DAGScheduler: running: Set()
17/01/13 22:52:41 INFO DAGScheduler: waiting: Set(ResultStage 6)
17/01/13 22:52:41 INFO DAGScheduler: failed: Set()
17/01/13 22:52:41 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[27] at collect at utils.scala:195), which has no missing parents
17/01/13 22:52:41 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 9.4 KB, free 639.0 KB)
17/01/13 22:52:41 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 6) in 64 ms on localhost (2/2)
17/01/13 22:52:41 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
17/01/13 22:52:41 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.6 KB, free 643.6 KB)
17/01/13 22:52:41 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:63357 (size: 4.6 KB, free: 511.0 MB)
17/01/13 22:52:41 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1006
17/01/13 22:52:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[27] at collect at utils.scala:195)
17/01/13 22:52:41 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
17/01/13 22:52:41 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 22:52:41 INFO Executor: Running task 0.0 in stage 6.0 (TID 8)
17/01/13 22:52:41 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:52:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/01/13 22:52:41 INFO Executor: Finished task 0.0 in stage 6.0 (TID 8). 1830 bytes result sent to driver
17/01/13 22:52:41 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 12 ms on localhost (1/1)
17/01/13 22:52:41 INFO DAGScheduler: ResultStage 6 (collect at utils.scala:195) finished in 0.014 s
17/01/13 22:52:41 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
17/01/13 22:52:41 INFO DAGScheduler: Job 4 finished: collect at utils.scala:195, took 0.101067 s
17/01/13 22:52:41 INFO ParseDriver: Parsing command: SELECT *
FROM `iris` AS `zzz1`
WHERE (0 = 1)
17/01/13 22:52:41 INFO ParseDriver: Parse Completed
17/01/13 22:52:41 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/01/13 22:52:41 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/01/13 22:52:41 INFO SparkContext: Starting job: collect at utils.scala:59
17/01/13 22:52:41 INFO DAGScheduler: Got job 5 (collect at utils.scala:59) with 1 output partitions
17/01/13 22:52:41 INFO DAGScheduler: Final stage: ResultStage 7 (collect at utils.scala:59)
17/01/13 22:52:41 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:52:41 INFO DAGScheduler: Missing parents: List()
17/01/13 22:52:41 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[31] at map at utils.scala:56), which has no missing parents
17/01/13 22:52:41 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.4 KB, free 649.0 KB)
17/01/13 22:52:41 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.0 KB, free 652.0 KB)
17/01/13 22:52:41 INFO ContextCleaner: Cleaned accumulator 10
17/01/13 22:52:41 INFO ContextCleaner: Cleaned accumulator 9
17/01/13 22:52:41 INFO ContextCleaner: Cleaned accumulator 8
17/01/13 22:52:41 INFO ContextCleaner: Cleaned accumulator 7
17/01/13 22:52:41 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:63357 (size: 3.0 KB, free: 511.0 MB)
17/01/13 22:52:41 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1006
17/01/13 22:52:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[31] at map at utils.scala:56)
17/01/13 22:52:41 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks
17/01/13 22:52:41 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 9, localhost, partition 0,PROCESS_LOCAL, 2646 bytes)
17/01/13 22:52:41 INFO Executor: Running task 0.0 in stage 7.0 (TID 9)
17/01/13 22:52:41 INFO Executor: Finished task 0.0 in stage 7.0 (TID 9). 1067 bytes result sent to driver
17/01/13 22:52:41 INFO DAGScheduler: ResultStage 7 (collect at utils.scala:59) finished in 0.011 s
17/01/13 22:52:41 INFO BlockManagerInfo: Removed broadcast_4_piece0 on localhost:63357 in memory (size: 1946.0 B, free: 511.0 MB)
17/01/13 22:52:41 INFO DAGScheduler: Job 5 finished: collect at utils.scala:59, took 0.046749 s
17/01/13 22:52:41 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 9) in 11 ms on localhost (1/1)
17/01/13 22:52:41 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
17/01/13 22:52:41 INFO ContextCleaner: Cleaned accumulator 5
17/01/13 22:52:41 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:63357 in memory (size: 19.3 KB, free: 511.0 MB)
17/01/13 22:52:41 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:63357 in memory (size: 1945.0 B, free: 511.0 MB)
17/01/13 22:52:41 INFO ContextCleaner: Cleaned accumulator 4
17/01/13 22:52:41 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:63357 in memory (size: 19.3 KB, free: 511.1 MB)
17/01/13 22:52:41 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:63357 in memory (size: 3.0 KB, free: 511.1 MB)
17/01/13 22:52:41 INFO ContextCleaner: Cleaned accumulator 3
17/01/13 22:52:41 INFO ContextCleaner: Cleaned accumulator 2
17/01/13 22:52:41 INFO BlockManagerInfo: Removed broadcast_9_piece0 on localhost:63357 in memory (size: 4.6 KB, free: 511.1 MB)
17/01/13 22:52:41 INFO ContextCleaner: Cleaned accumulator 26
17/01/13 22:52:41 INFO BlockManagerInfo: Removed broadcast_8_piece0 on localhost:63357 in memory (size: 8.7 KB, free: 511.1 MB)
17/01/13 22:52:41 INFO ContextCleaner: Cleaned accumulator 25
17/01/13 22:52:41 INFO ContextCleaner: Cleaned shuffle 1
17/01/13 22:52:41 INFO BlockManagerInfo: Removed broadcast_7_piece0 on localhost:63357 in memory (size: 4.6 KB, free: 511.1 MB)
17/01/13 22:52:42 INFO ContextCleaner: Cleaned accumulator 16
17/01/13 22:52:42 INFO BlockManagerInfo: Removed broadcast_6_piece0 on localhost:63357 in memory (size: 8.7 KB, free: 511.1 MB)
17/01/13 22:52:42 INFO ContextCleaner: Cleaned accumulator 15
17/01/13 22:52:42 INFO ContextCleaner: Cleaned shuffle 0
17/01/13 22:52:42 INFO ContextCleaner: Cleaned accumulator 14
17/01/13 22:52:42 INFO ContextCleaner: Cleaned accumulator 13
17/01/13 22:52:42 INFO ContextCleaner: Cleaned accumulator 12
17/01/13 22:52:42 INFO ContextCleaner: Cleaned accumulator 11
17/01/13 22:52:51 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 208.5 KB, free 451.1 KB)
17/01/13 22:52:51 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 19.3 KB, free 470.4 KB)
17/01/13 22:52:51 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:63357 (size: 19.3 KB, free: 511.1 MB)
17/01/13 22:52:51 INFO SparkContext: Created broadcast 11 from textFile at TextFile.scala:30
17/01/13 22:52:51 INFO FileInputFormat: Total input paths to process : 1
17/01/13 22:52:51 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/01/13 22:52:51 INFO DAGScheduler: Got job 6 (take at CsvRelation.scala:249) with 1 output partitions
17/01/13 22:52:51 INFO DAGScheduler: Final stage: ResultStage 8 (take at CsvRelation.scala:249)
17/01/13 22:52:51 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:52:51 INFO DAGScheduler: Missing parents: List()
17/01/13 22:52:51 INFO DAGScheduler: Submitting ResultStage 8 (C:\Users\Baoco\AppData\Local\Temp\RtmpGQdUfI/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv MapPartitionsRDD[33] at textFile at TextFile.scala:30), which has no missing parents
17/01/13 22:52:51 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 3.2 KB, free 473.6 KB)
17/01/13 22:52:51 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 1944.0 B, free 475.5 KB)
17/01/13 22:52:51 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:63357 (size: 1944.0 B, free: 511.1 MB)
17/01/13 22:52:51 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1006
17/01/13 22:52:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (C:\Users\Baoco\AppData\Local\Temp\RtmpGQdUfI/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv MapPartitionsRDD[33] at textFile at TextFile.scala:30)
17/01/13 22:52:51 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks
17/01/13 22:52:51 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 10, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 22:52:51 INFO Executor: Running task 0.0 in stage 8.0 (TID 10)
17/01/13 22:52:51 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpGQdUfI/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv:0+16824941
17/01/13 22:52:51 INFO Executor: Finished task 0.0 in stage 8.0 (TID 10). 3117 bytes result sent to driver
17/01/13 22:52:51 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 10) in 6 ms on localhost (1/1)
17/01/13 22:52:51 INFO DAGScheduler: ResultStage 8 (take at CsvRelation.scala:249) finished in 0.006 s
17/01/13 22:52:51 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
17/01/13 22:52:51 INFO DAGScheduler: Job 6 finished: take at CsvRelation.scala:249, took 0.011834 s
17/01/13 22:52:51 INFO ParseDriver: Parsing command: SELECT * FROM  `flights`
17/01/13 22:52:51 INFO ParseDriver: Parse Completed
17/01/13 22:52:51 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 208.5 KB, free 684.0 KB)
17/01/13 22:52:51 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 19.3 KB, free 703.3 KB)
17/01/13 22:52:51 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on localhost:63357 (size: 19.3 KB, free: 511.1 MB)
17/01/13 22:52:51 INFO SparkContext: Created broadcast 13 from textFile at TextFile.scala:30
17/01/13 22:52:51 INFO FileInputFormat: Total input paths to process : 1
17/01/13 22:52:51 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/01/13 22:52:51 INFO DAGScheduler: Got job 7 (take at CsvRelation.scala:249) with 1 output partitions
17/01/13 22:52:51 INFO DAGScheduler: Final stage: ResultStage 9 (take at CsvRelation.scala:249)
17/01/13 22:52:51 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:52:51 INFO DAGScheduler: Missing parents: List()
17/01/13 22:52:51 INFO DAGScheduler: Submitting ResultStage 9 (C:\Users\Baoco\AppData\Local\Temp\RtmpGQdUfI/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv MapPartitionsRDD[35] at textFile at TextFile.scala:30), which has no missing parents
17/01/13 22:52:51 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 3.2 KB, free 706.5 KB)
17/01/13 22:52:51 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 1944.0 B, free 708.4 KB)
17/01/13 22:52:51 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on localhost:63357 (size: 1944.0 B, free: 511.1 MB)
17/01/13 22:52:51 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1006
17/01/13 22:52:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (C:\Users\Baoco\AppData\Local\Temp\RtmpGQdUfI/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv MapPartitionsRDD[35] at textFile at TextFile.scala:30)
17/01/13 22:52:51 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
17/01/13 22:52:51 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 11, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 22:52:51 INFO Executor: Running task 0.0 in stage 9.0 (TID 11)
17/01/13 22:52:51 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpGQdUfI/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv:0+16824941
17/01/13 22:52:51 INFO Executor: Finished task 0.0 in stage 9.0 (TID 11). 3117 bytes result sent to driver
17/01/13 22:52:51 INFO DAGScheduler: ResultStage 9 (take at CsvRelation.scala:249) finished in 0.009 s
17/01/13 22:52:51 INFO DAGScheduler: Job 7 finished: take at CsvRelation.scala:249, took 0.015184 s
17/01/13 22:52:51 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 11) in 9 ms on localhost (1/1)
17/01/13 22:52:51 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
17/01/13 22:52:51 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 208.5 KB, free 916.9 KB)
17/01/13 22:52:51 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 19.3 KB, free 936.3 KB)
17/01/13 22:52:51 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on localhost:63357 (size: 19.3 KB, free: 511.0 MB)
17/01/13 22:52:51 INFO SparkContext: Created broadcast 15 from textFile at TextFile.scala:30
17/01/13 22:52:51 INFO FileInputFormat: Total input paths to process : 1
17/01/13 22:52:51 INFO SparkContext: Starting job: sql at null:-2
17/01/13 22:52:51 INFO DAGScheduler: Registering RDD 45 (sql at null:-2)
17/01/13 22:52:51 INFO DAGScheduler: Got job 8 (sql at null:-2) with 1 output partitions
17/01/13 22:52:51 INFO DAGScheduler: Final stage: ResultStage 11 (sql at null:-2)
17/01/13 22:52:51 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
17/01/13 22:52:51 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 10)
17/01/13 22:52:51 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[45] at sql at null:-2), which has no missing parents
17/01/13 22:52:51 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 26.8 KB, free 963.1 KB)
17/01/13 22:52:51 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 11.0 KB, free 974.1 KB)
17/01/13 22:52:51 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on localhost:63357 (size: 11.0 KB, free: 511.0 MB)
17/01/13 22:52:51 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1006
17/01/13 22:52:51 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[45] at sql at null:-2)
17/01/13 22:52:51 INFO TaskSchedulerImpl: Adding task set 10.0 with 2 tasks
17/01/13 22:52:51 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 12, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:52:51 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 13, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:52:51 INFO Executor: Running task 0.0 in stage 10.0 (TID 12)
17/01/13 22:52:51 INFO Executor: Running task 1.0 in stage 10.0 (TID 13)
17/01/13 22:52:51 INFO CacheManager: Partition rdd_42_0 not found, computing it
17/01/13 22:52:51 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpGQdUfI/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv:0+16824941
17/01/13 22:52:51 INFO CacheManager: Partition rdd_42_1 not found, computing it
17/01/13 22:52:51 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpGQdUfI/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv:16824941+16824942
17/01/13 22:52:51 INFO GenerateUnsafeProjection: Code generated in 28.319976 ms
17/01/13 22:52:52 INFO BlockManagerInfo: Removed broadcast_14_piece0 on localhost:63357 in memory (size: 1944.0 B, free: 511.0 MB)
17/01/13 22:52:52 INFO ContextCleaner: Cleaned accumulator 30
17/01/13 22:52:52 INFO BlockManagerInfo: Removed broadcast_13_piece0 on localhost:63357 in memory (size: 19.3 KB, free: 511.0 MB)
17/01/13 22:52:52 INFO BlockManagerInfo: Removed broadcast_12_piece0 on localhost:63357 in memory (size: 1944.0 B, free: 511.0 MB)
17/01/13 22:52:52 INFO ContextCleaner: Cleaned accumulator 29
17/01/13 22:52:52 INFO BlockManagerInfo: Removed broadcast_11_piece0 on localhost:63357 in memory (size: 19.3 KB, free: 511.1 MB)
17/01/13 22:52:52 INFO BlockManagerInfo: Removed broadcast_10_piece0 on localhost:63357 in memory (size: 3.0 KB, free: 511.1 MB)
17/01/13 22:52:52 INFO ContextCleaner: Cleaned accumulator 28
17/01/13 22:52:52 INFO ContextCleaner: Cleaned accumulator 27
17/01/13 22:52:57 INFO MemoryStore: Block rdd_42_0 stored as values in memory (estimated size 12.2 MB, free 12.7 MB)
17/01/13 22:52:57 INFO BlockManagerInfo: Added rdd_42_0 in memory on localhost:63357 (size: 12.2 MB, free: 498.9 MB)
17/01/13 22:52:57 INFO MemoryStore: Block rdd_42_1 stored as values in memory (estimated size 12.2 MB, free 24.9 MB)
17/01/13 22:52:57 INFO BlockManagerInfo: Added rdd_42_1 in memory on localhost:63357 (size: 12.2 MB, free: 486.7 MB)
17/01/13 22:52:57 INFO Executor: Finished task 0.0 in stage 10.0 (TID 12). 21077 bytes result sent to driver
17/01/13 22:52:57 INFO Executor: Finished task 1.0 in stage 10.0 (TID 13). 21132 bytes result sent to driver
17/01/13 22:52:57 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 12) in 5229 ms on localhost (1/2)
17/01/13 22:52:57 INFO DAGScheduler: ShuffleMapStage 10 (sql at null:-2) finished in 5.234 s
17/01/13 22:52:57 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:52:57 INFO DAGScheduler: running: Set()
17/01/13 22:52:57 INFO DAGScheduler: waiting: Set(ResultStage 11)
17/01/13 22:52:57 INFO DAGScheduler: failed: Set()
17/01/13 22:52:57 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[48] at sql at null:-2), which has no missing parents
17/01/13 22:52:57 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 9.3 KB, free 24.9 MB)
17/01/13 22:52:57 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 13) in 5233 ms on localhost (2/2)
17/01/13 22:52:57 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
17/01/13 22:52:57 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 4.6 KB, free 24.9 MB)
17/01/13 22:52:57 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on localhost:63357 (size: 4.6 KB, free: 486.7 MB)
17/01/13 22:52:57 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1006
17/01/13 22:52:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[48] at sql at null:-2)
17/01/13 22:52:57 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks
17/01/13 22:52:57 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 14, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 22:52:57 INFO Executor: Running task 0.0 in stage 11.0 (TID 14)
17/01/13 22:52:57 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:52:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:52:57 INFO Executor: Finished task 0.0 in stage 11.0 (TID 14). 1830 bytes result sent to driver
17/01/13 22:52:57 INFO DAGScheduler: ResultStage 11 (sql at null:-2) finished in 0.011 s
17/01/13 22:52:57 INFO DAGScheduler: Job 8 finished: sql at null:-2, took 5.259614 s
17/01/13 22:52:57 INFO ParseDriver: Parsing command: SELECT count(*) FROM  `flights`
17/01/13 22:52:57 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 14) in 10 ms on localhost (1/1)
17/01/13 22:52:57 INFO ParseDriver: Parse Completed
17/01/13 22:52:57 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
17/01/13 22:52:57 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 22:52:57 INFO DAGScheduler: Registering RDD 52 (collect at utils.scala:195)
17/01/13 22:52:57 INFO DAGScheduler: Got job 9 (collect at utils.scala:195) with 1 output partitions
17/01/13 22:52:57 INFO DAGScheduler: Final stage: ResultStage 13 (collect at utils.scala:195)
17/01/13 22:52:57 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)
17/01/13 22:52:57 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 12)
17/01/13 22:52:57 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[52] at collect at utils.scala:195), which has no missing parents
17/01/13 22:52:57 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 26.9 KB, free 24.9 MB)
17/01/13 22:52:57 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 11.0 KB, free 24.9 MB)
17/01/13 22:52:57 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on localhost:63357 (size: 11.0 KB, free: 486.7 MB)
17/01/13 22:52:57 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1006
17/01/13 22:52:57 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[52] at collect at utils.scala:195)
17/01/13 22:52:57 INFO TaskSchedulerImpl: Adding task set 12.0 with 2 tasks
17/01/13 22:52:57 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 15, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:52:57 INFO TaskSetManager: Starting task 1.0 in stage 12.0 (TID 16, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:52:57 INFO Executor: Running task 0.0 in stage 12.0 (TID 15)
17/01/13 22:52:57 INFO Executor: Running task 1.0 in stage 12.0 (TID 16)
17/01/13 22:52:57 INFO BlockManager: Found block rdd_42_1 locally
17/01/13 22:52:57 INFO BlockManager: Found block rdd_42_0 locally
17/01/13 22:52:57 INFO Executor: Finished task 0.0 in stage 12.0 (TID 15). 2679 bytes result sent to driver
17/01/13 22:52:57 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 15) in 55 ms on localhost (1/2)
17/01/13 22:52:57 INFO Executor: Finished task 1.0 in stage 12.0 (TID 16). 2679 bytes result sent to driver
17/01/13 22:52:57 INFO DAGScheduler: ShuffleMapStage 12 (collect at utils.scala:195) finished in 0.057 s
17/01/13 22:52:57 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:52:57 INFO DAGScheduler: running: Set()
17/01/13 22:52:57 INFO DAGScheduler: waiting: Set(ResultStage 13)
17/01/13 22:52:57 INFO DAGScheduler: failed: Set()
17/01/13 22:52:57 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[55] at collect at utils.scala:195), which has no missing parents
17/01/13 22:52:57 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 9.4 KB, free 24.9 MB)
17/01/13 22:52:57 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 4.6 KB, free 24.9 MB)
17/01/13 22:52:57 INFO TaskSetManager: Finished task 1.0 in stage 12.0 (TID 16) in 56 ms on localhost (2/2)
17/01/13 22:52:57 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
17/01/13 22:52:57 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on localhost:63357 (size: 4.6 KB, free: 486.7 MB)
17/01/13 22:52:57 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1006
17/01/13 22:52:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[55] at collect at utils.scala:195)
17/01/13 22:52:57 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks
17/01/13 22:52:57 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 17, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 22:52:57 INFO Executor: Running task 0.0 in stage 13.0 (TID 17)
17/01/13 22:52:57 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:52:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:52:57 INFO Executor: Finished task 0.0 in stage 13.0 (TID 17). 1830 bytes result sent to driver
17/01/13 22:52:57 INFO DAGScheduler: ResultStage 13 (collect at utils.scala:195) finished in 0.010 s
17/01/13 22:52:57 INFO DAGScheduler: Job 9 finished: collect at utils.scala:195, took 0.084094 s
17/01/13 22:52:57 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 17) in 10 ms on localhost (1/1)
17/01/13 22:52:57 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
17/01/13 22:52:57 INFO ParseDriver: Parsing command: SELECT *
FROM `flights` AS `zzz2`
WHERE (0 = 1)
17/01/13 22:52:57 INFO ParseDriver: Parse Completed
17/01/13 22:52:57 INFO BlockManagerInfo: Removed broadcast_17_piece0 on localhost:63357 in memory (size: 4.6 KB, free: 486.7 MB)
17/01/13 22:52:57 INFO ContextCleaner: Cleaned shuffle 3
17/01/13 22:52:57 INFO ContextCleaner: Cleaned accumulator 51
17/01/13 22:52:57 INFO ContextCleaner: Cleaned accumulator 50
17/01/13 22:52:57 INFO BlockManagerInfo: Removed broadcast_18_piece0 on localhost:63357 in memory (size: 11.0 KB, free: 486.7 MB)
17/01/13 22:52:57 INFO BlockManagerInfo: Removed broadcast_19_piece0 on localhost:63357 in memory (size: 4.6 KB, free: 486.7 MB)
17/01/13 22:52:58 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/01/13 22:52:58 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/01/13 22:52:58 INFO SparkContext: Starting job: collect at utils.scala:59
17/01/13 22:52:58 INFO DAGScheduler: Got job 10 (collect at utils.scala:59) with 1 output partitions
17/01/13 22:52:58 INFO DAGScheduler: Final stage: ResultStage 14 (collect at utils.scala:59)
17/01/13 22:52:58 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:52:58 INFO DAGScheduler: Missing parents: List()
17/01/13 22:52:58 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[59] at map at utils.scala:56), which has no missing parents
17/01/13 22:52:58 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 5.4 KB, free 24.9 MB)
17/01/13 22:52:58 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 3.0 KB, free 24.9 MB)
17/01/13 22:52:58 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on localhost:63357 (size: 3.0 KB, free: 486.7 MB)
17/01/13 22:52:58 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1006
17/01/13 22:52:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[59] at map at utils.scala:56)
17/01/13 22:52:58 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks
17/01/13 22:52:58 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 18, localhost, partition 0,PROCESS_LOCAL, 2687 bytes)
17/01/13 22:52:58 INFO Executor: Running task 0.0 in stage 14.0 (TID 18)
17/01/13 22:52:58 INFO Executor: Finished task 0.0 in stage 14.0 (TID 18). 1077 bytes result sent to driver
17/01/13 22:52:58 INFO DAGScheduler: ResultStage 14 (collect at utils.scala:59) finished in 0.004 s
17/01/13 22:52:58 INFO DAGScheduler: Job 10 finished: collect at utils.scala:59, took 0.010453 s
17/01/13 22:52:58 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 18) in 4 ms on localhost (1/1)
17/01/13 22:52:58 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
17/01/13 22:52:59 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 208.5 KB, free 25.1 MB)
17/01/13 22:52:59 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 19.3 KB, free 25.1 MB)
17/01/13 22:52:59 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on localhost:63357 (size: 19.3 KB, free: 486.7 MB)
17/01/13 22:52:59 INFO SparkContext: Created broadcast 21 from textFile at TextFile.scala:30
17/01/13 22:52:59 INFO FileInputFormat: Total input paths to process : 1
17/01/13 22:52:59 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/01/13 22:52:59 INFO DAGScheduler: Got job 11 (take at CsvRelation.scala:249) with 1 output partitions
17/01/13 22:52:59 INFO DAGScheduler: Final stage: ResultStage 15 (take at CsvRelation.scala:249)
17/01/13 22:52:59 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:52:59 INFO DAGScheduler: Missing parents: List()
17/01/13 22:52:59 INFO DAGScheduler: Submitting ResultStage 15 (C:\Users\Baoco\AppData\Local\Temp\RtmpGQdUfI/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv MapPartitionsRDD[61] at textFile at TextFile.scala:30), which has no missing parents
17/01/13 22:52:59 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 3.2 KB, free 25.1 MB)
17/01/13 22:52:59 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 1944.0 B, free 25.1 MB)
17/01/13 22:52:59 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on localhost:63357 (size: 1944.0 B, free: 486.7 MB)
17/01/13 22:52:59 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1006
17/01/13 22:52:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (C:\Users\Baoco\AppData\Local\Temp\RtmpGQdUfI/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv MapPartitionsRDD[61] at textFile at TextFile.scala:30)
17/01/13 22:52:59 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks
17/01/13 22:52:59 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 19, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 22:52:59 INFO Executor: Running task 0.0 in stage 15.0 (TID 19)
17/01/13 22:52:59 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpGQdUfI/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv:0+3427780
17/01/13 22:52:59 INFO Executor: Finished task 0.0 in stage 15.0 (TID 19). 2768 bytes result sent to driver
17/01/13 22:52:59 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 19) in 6 ms on localhost (1/1)
17/01/13 22:52:59 INFO DAGScheduler: ResultStage 15 (take at CsvRelation.scala:249) finished in 0.006 s
17/01/13 22:52:59 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
17/01/13 22:52:59 INFO DAGScheduler: Job 11 finished: take at CsvRelation.scala:249, took 0.010746 s
17/01/13 22:52:59 INFO ParseDriver: Parsing command: SELECT * FROM  `batting`
17/01/13 22:52:59 INFO ParseDriver: Parse Completed
17/01/13 22:52:59 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 208.5 KB, free 25.3 MB)
17/01/13 22:52:59 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 19.3 KB, free 25.3 MB)
17/01/13 22:52:59 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on localhost:63357 (size: 19.3 KB, free: 486.7 MB)
17/01/13 22:52:59 INFO SparkContext: Created broadcast 23 from textFile at TextFile.scala:30
17/01/13 22:52:59 INFO FileInputFormat: Total input paths to process : 1
17/01/13 22:52:59 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/01/13 22:52:59 INFO DAGScheduler: Got job 12 (take at CsvRelation.scala:249) with 1 output partitions
17/01/13 22:52:59 INFO DAGScheduler: Final stage: ResultStage 16 (take at CsvRelation.scala:249)
17/01/13 22:52:59 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:52:59 INFO DAGScheduler: Missing parents: List()
17/01/13 22:52:59 INFO DAGScheduler: Submitting ResultStage 16 (C:\Users\Baoco\AppData\Local\Temp\RtmpGQdUfI/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv MapPartitionsRDD[63] at textFile at TextFile.scala:30), which has no missing parents
17/01/13 22:52:59 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 3.2 KB, free 25.3 MB)
17/01/13 22:52:59 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 1944.0 B, free 25.3 MB)
17/01/13 22:52:59 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on localhost:63357 (size: 1944.0 B, free: 486.7 MB)
17/01/13 22:52:59 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1006
17/01/13 22:52:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (C:\Users\Baoco\AppData\Local\Temp\RtmpGQdUfI/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv MapPartitionsRDD[63] at textFile at TextFile.scala:30)
17/01/13 22:52:59 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks
17/01/13 22:52:59 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 20, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 22:52:59 INFO Executor: Running task 0.0 in stage 16.0 (TID 20)
17/01/13 22:52:59 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpGQdUfI/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv:0+3427780
17/01/13 22:52:59 INFO Executor: Finished task 0.0 in stage 16.0 (TID 20). 2768 bytes result sent to driver
17/01/13 22:52:59 INFO DAGScheduler: ResultStage 16 (take at CsvRelation.scala:249) finished in 0.006 s
17/01/13 22:52:59 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 20) in 6 ms on localhost (1/1)
17/01/13 22:52:59 INFO DAGScheduler: Job 12 finished: take at CsvRelation.scala:249, took 0.010076 s
17/01/13 22:52:59 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
17/01/13 22:52:59 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 208.5 KB, free 25.5 MB)
17/01/13 22:52:59 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 19.3 KB, free 25.5 MB)
17/01/13 22:52:59 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on localhost:63357 (size: 19.3 KB, free: 486.6 MB)
17/01/13 22:52:59 INFO SparkContext: Created broadcast 25 from textFile at TextFile.scala:30
17/01/13 22:52:59 INFO FileInputFormat: Total input paths to process : 1
17/01/13 22:52:59 INFO SparkContext: Starting job: sql at null:-2
17/01/13 22:52:59 INFO DAGScheduler: Registering RDD 73 (sql at null:-2)
17/01/13 22:52:59 INFO DAGScheduler: Got job 13 (sql at null:-2) with 1 output partitions
17/01/13 22:52:59 INFO DAGScheduler: Final stage: ResultStage 18 (sql at null:-2)
17/01/13 22:52:59 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)
17/01/13 22:52:59 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 17)
17/01/13 22:52:59 INFO DAGScheduler: Submitting ShuffleMapStage 17 (MapPartitionsRDD[73] at sql at null:-2), which has no missing parents
17/01/13 22:52:59 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 27.9 KB, free 25.6 MB)
17/01/13 22:52:59 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 11.2 KB, free 25.6 MB)
17/01/13 22:52:59 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on localhost:63357 (size: 11.2 KB, free: 486.6 MB)
17/01/13 22:52:59 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1006
17/01/13 22:52:59 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[73] at sql at null:-2)
17/01/13 22:52:59 INFO TaskSchedulerImpl: Adding task set 17.0 with 2 tasks
17/01/13 22:52:59 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 21, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:52:59 INFO TaskSetManager: Starting task 1.0 in stage 17.0 (TID 22, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:52:59 INFO Executor: Running task 0.0 in stage 17.0 (TID 21)
17/01/13 22:52:59 INFO Executor: Running task 1.0 in stage 17.0 (TID 22)
17/01/13 22:52:59 INFO CacheManager: Partition rdd_70_0 not found, computing it
17/01/13 22:52:59 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpGQdUfI/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv:0+3427780
17/01/13 22:52:59 INFO GenerateUnsafeProjection: Code generated in 21.649048 ms
17/01/13 22:52:59 INFO CacheManager: Partition rdd_70_1 not found, computing it
17/01/13 22:52:59 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpGQdUfI/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv:3427780+3427781
17/01/13 22:53:00 INFO BlockManagerInfo: Removed broadcast_20_piece0 on localhost:63357 in memory (size: 3.0 KB, free: 486.6 MB)
17/01/13 22:53:00 INFO ContextCleaner: Cleaned accumulator 53
17/01/13 22:53:00 INFO ContextCleaner: Cleaned accumulator 52
17/01/13 22:53:00 INFO BlockManagerInfo: Removed broadcast_24_piece0 on localhost:63357 in memory (size: 1944.0 B, free: 486.6 MB)
17/01/13 22:53:00 INFO ContextCleaner: Cleaned accumulator 55
17/01/13 22:53:00 INFO BlockManagerInfo: Removed broadcast_23_piece0 on localhost:63357 in memory (size: 19.3 KB, free: 486.6 MB)
17/01/13 22:53:00 INFO BlockManagerInfo: Removed broadcast_22_piece0 on localhost:63357 in memory (size: 1944.0 B, free: 486.7 MB)
17/01/13 22:53:00 INFO ContextCleaner: Cleaned accumulator 54
17/01/13 22:53:00 INFO BlockManagerInfo: Removed broadcast_21_piece0 on localhost:63357 in memory (size: 19.3 KB, free: 486.7 MB)
17/01/13 22:53:00 INFO MemoryStore: Block rdd_70_0 stored as values in memory (estimated size 1897.3 KB, free 27.0 MB)
17/01/13 22:53:00 INFO BlockManagerInfo: Added rdd_70_0 in memory on localhost:63357 (size: 1897.3 KB, free: 484.8 MB)
17/01/13 22:53:00 INFO Executor: Finished task 0.0 in stage 17.0 (TID 21). 9713 bytes result sent to driver
17/01/13 22:53:00 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 21) in 899 ms on localhost (1/2)
17/01/13 22:53:00 INFO MemoryStore: Block rdd_70_1 stored as values in memory (estimated size 1699.8 KB, free 28.6 MB)
17/01/13 22:53:00 INFO BlockManagerInfo: Added rdd_70_1 in memory on localhost:63357 (size: 1699.8 KB, free: 483.2 MB)
17/01/13 22:53:00 INFO Executor: Finished task 1.0 in stage 17.0 (TID 22). 9862 bytes result sent to driver
17/01/13 22:53:00 INFO DAGScheduler: ShuffleMapStage 17 (sql at null:-2) finished in 0.940 s
17/01/13 22:53:00 INFO TaskSetManager: Finished task 1.0 in stage 17.0 (TID 22) in 938 ms on localhost (2/2)
17/01/13 22:53:00 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:53:00 INFO DAGScheduler: running: Set()
17/01/13 22:53:00 INFO DAGScheduler: waiting: Set(ResultStage 18)
17/01/13 22:53:00 INFO DAGScheduler: failed: Set()
17/01/13 22:53:00 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[76] at sql at null:-2), which has no missing parents
17/01/13 22:53:00 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
17/01/13 22:53:00 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 9.3 KB, free 28.6 MB)
17/01/13 22:53:00 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 4.6 KB, free 28.6 MB)
17/01/13 22:53:00 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on localhost:63357 (size: 4.6 KB, free: 483.2 MB)
17/01/13 22:53:00 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1006
17/01/13 22:53:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[76] at sql at null:-2)
17/01/13 22:53:00 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks
17/01/13 22:53:00 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 23, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 22:53:00 INFO Executor: Running task 0.0 in stage 18.0 (TID 23)
17/01/13 22:53:00 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:53:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:53:00 INFO Executor: Finished task 0.0 in stage 18.0 (TID 23). 1830 bytes result sent to driver
17/01/13 22:53:00 INFO DAGScheduler: ResultStage 18 (sql at null:-2) finished in 0.008 s
17/01/13 22:53:00 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 23) in 8 ms on localhost (1/1)
17/01/13 22:53:00 INFO DAGScheduler: Job 13 finished: sql at null:-2, took 0.960900 s
17/01/13 22:53:00 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
17/01/13 22:53:00 INFO ParseDriver: Parsing command: SELECT count(*) FROM  `batting`
17/01/13 22:53:00 INFO ParseDriver: Parse Completed
17/01/13 22:53:00 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 22:53:00 INFO DAGScheduler: Registering RDD 80 (collect at utils.scala:195)
17/01/13 22:53:00 INFO DAGScheduler: Got job 14 (collect at utils.scala:195) with 1 output partitions
17/01/13 22:53:00 INFO DAGScheduler: Final stage: ResultStage 20 (collect at utils.scala:195)
17/01/13 22:53:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)
17/01/13 22:53:00 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 19)
17/01/13 22:53:00 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[80] at collect at utils.scala:195), which has no missing parents
17/01/13 22:53:00 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 28.0 KB, free 28.7 MB)
17/01/13 22:53:00 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 11.2 KB, free 28.7 MB)
17/01/13 22:53:00 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on localhost:63357 (size: 11.2 KB, free: 483.1 MB)
17/01/13 22:53:00 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1006
17/01/13 22:53:00 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[80] at collect at utils.scala:195)
17/01/13 22:53:00 INFO TaskSchedulerImpl: Adding task set 19.0 with 2 tasks
17/01/13 22:53:00 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 24, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:53:00 INFO TaskSetManager: Starting task 1.0 in stage 19.0 (TID 25, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:53:00 INFO Executor: Running task 0.0 in stage 19.0 (TID 24)
17/01/13 22:53:00 INFO Executor: Running task 1.0 in stage 19.0 (TID 25)
17/01/13 22:53:00 INFO BlockManager: Found block rdd_70_0 locally
17/01/13 22:53:00 INFO Executor: Finished task 0.0 in stage 19.0 (TID 24). 2679 bytes result sent to driver
17/01/13 22:53:00 INFO BlockManager: Found block rdd_70_1 locally
17/01/13 22:53:00 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 24) in 22 ms on localhost (1/2)
17/01/13 22:53:00 INFO Executor: Finished task 1.0 in stage 19.0 (TID 25). 2679 bytes result sent to driver
17/01/13 22:53:00 INFO TaskSetManager: Finished task 1.0 in stage 19.0 (TID 25) in 32 ms on localhost (2/2)
17/01/13 22:53:00 INFO DAGScheduler: ShuffleMapStage 19 (collect at utils.scala:195) finished in 0.033 s
17/01/13 22:53:00 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:53:00 INFO DAGScheduler: running: Set()
17/01/13 22:53:00 INFO DAGScheduler: waiting: Set(ResultStage 20)
17/01/13 22:53:00 INFO DAGScheduler: failed: Set()
17/01/13 22:53:00 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[83] at collect at utils.scala:195), which has no missing parents
17/01/13 22:53:00 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
17/01/13 22:53:00 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 9.4 KB, free 28.7 MB)
17/01/13 22:53:00 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 4.7 KB, free 28.7 MB)
17/01/13 22:53:00 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on localhost:63357 (size: 4.7 KB, free: 483.1 MB)
17/01/13 22:53:00 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1006
17/01/13 22:53:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[83] at collect at utils.scala:195)
17/01/13 22:53:00 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks
17/01/13 22:53:00 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 26, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 22:53:00 INFO Executor: Running task 0.0 in stage 20.0 (TID 26)
17/01/13 22:53:00 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:53:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:53:00 INFO Executor: Finished task 0.0 in stage 20.0 (TID 26). 1830 bytes result sent to driver
17/01/13 22:53:00 INFO DAGScheduler: ResultStage 20 (collect at utils.scala:195) finished in 0.010 s
17/01/13 22:53:00 INFO DAGScheduler: Job 14 finished: collect at utils.scala:195, took 0.052901 s
17/01/13 22:53:00 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 26) in 9 ms on localhost (1/1)
17/01/13 22:53:00 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
17/01/13 22:53:00 INFO ParseDriver: Parsing command: SELECT *
FROM `batting` AS `zzz3`
WHERE (0 = 1)
17/01/13 22:53:00 INFO ParseDriver: Parse Completed
17/01/13 22:53:00 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/01/13 22:53:00 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/01/13 22:53:01 INFO SparkContext: Starting job: collect at utils.scala:59
17/01/13 22:53:01 INFO DAGScheduler: Got job 15 (collect at utils.scala:59) with 1 output partitions
17/01/13 22:53:01 INFO DAGScheduler: Final stage: ResultStage 21 (collect at utils.scala:59)
17/01/13 22:53:01 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:53:01 INFO DAGScheduler: Missing parents: List()
17/01/13 22:53:01 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[87] at map at utils.scala:56), which has no missing parents
17/01/13 22:53:01 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 5.4 KB, free 28.7 MB)
17/01/13 22:53:01 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 3.0 KB, free 28.7 MB)
17/01/13 22:53:01 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on localhost:63357 (size: 3.0 KB, free: 483.1 MB)
17/01/13 22:53:01 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1006
17/01/13 22:53:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[87] at map at utils.scala:56)
17/01/13 22:53:01 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks
17/01/13 22:53:01 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 27, localhost, partition 0,PROCESS_LOCAL, 2728 bytes)
17/01/13 22:53:01 INFO Executor: Running task 0.0 in stage 21.0 (TID 27)
17/01/13 22:53:01 INFO Executor: Finished task 0.0 in stage 21.0 (TID 27). 1087 bytes result sent to driver
17/01/13 22:53:01 INFO DAGScheduler: ResultStage 21 (collect at utils.scala:59) finished in 0.004 s
17/01/13 22:53:01 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 27) in 4 ms on localhost (1/1)
17/01/13 22:53:01 INFO DAGScheduler: Job 15 finished: collect at utils.scala:59, took 0.009246 s
17/01/13 22:53:01 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
17/01/13 22:53:01 INFO ParseDriver: Parsing command: SELECT *
FROM `flights`
WHERE (`dep_delay` = 2.0)
17/01/13 22:53:01 INFO ParseDriver: Parse Completed
17/01/13 22:53:01 INFO InMemoryColumnarTableScan: Predicate (dep_delay#146 = 2.0) generates partition filter: ((dep_delay.lowerBound#1071 <= 2.0) && (2.0 <= dep_delay.upperBound#1070))
17/01/13 22:53:01 INFO SparkContext: Starting job: count at null:-2
17/01/13 22:53:01 INFO DAGScheduler: Registering RDD 92 (count at null:-2)
17/01/13 22:53:01 INFO DAGScheduler: Got job 16 (count at null:-2) with 1 output partitions
17/01/13 22:53:01 INFO DAGScheduler: Final stage: ResultStage 23 (count at null:-2)
17/01/13 22:53:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 22)
17/01/13 22:53:01 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 22)
17/01/13 22:53:01 INFO DAGScheduler: Submitting ShuffleMapStage 22 (MapPartitionsRDD[92] at count at null:-2), which has no missing parents
17/01/13 22:53:01 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 28.8 KB, free 28.7 MB)
17/01/13 22:53:01 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 11.7 KB, free 28.7 MB)
17/01/13 22:53:01 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on localhost:63357 (size: 11.7 KB, free: 483.1 MB)
17/01/13 22:53:01 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1006
17/01/13 22:53:01 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[92] at count at null:-2)
17/01/13 22:53:01 INFO TaskSchedulerImpl: Adding task set 22.0 with 2 tasks
17/01/13 22:53:01 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 28, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:53:01 INFO TaskSetManager: Starting task 1.0 in stage 22.0 (TID 29, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:53:01 INFO Executor: Running task 0.0 in stage 22.0 (TID 28)
17/01/13 22:53:01 INFO Executor: Running task 1.0 in stage 22.0 (TID 29)
17/01/13 22:53:01 INFO BlockManager: Found block rdd_42_1 locally
17/01/13 22:53:01 INFO BlockManager: Found block rdd_42_0 locally
17/01/13 22:53:01 INFO GeneratePredicate: Code generated in 17.595292 ms
17/01/13 22:53:01 INFO GenerateColumnAccessor: Code generated in 15.363401 ms
17/01/13 22:53:01 INFO GeneratePredicate: Code generated in 3.878824 ms
17/01/13 22:53:01 INFO Executor: Finished task 0.0 in stage 22.0 (TID 28). 2808 bytes result sent to driver
17/01/13 22:53:01 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 28) in 132 ms on localhost (1/2)
17/01/13 22:53:01 INFO Executor: Finished task 1.0 in stage 22.0 (TID 29). 2808 bytes result sent to driver
17/01/13 22:53:01 INFO TaskSetManager: Finished task 1.0 in stage 22.0 (TID 29) in 135 ms on localhost (2/2)
17/01/13 22:53:01 INFO DAGScheduler: ShuffleMapStage 22 (count at null:-2) finished in 0.136 s
17/01/13 22:53:01 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
17/01/13 22:53:01 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:53:01 INFO DAGScheduler: running: Set()
17/01/13 22:53:01 INFO DAGScheduler: waiting: Set(ResultStage 23)
17/01/13 22:53:01 INFO DAGScheduler: failed: Set()
17/01/13 22:53:01 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[95] at count at null:-2), which has no missing parents
17/01/13 22:53:01 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 10.8 KB, free 28.8 MB)
17/01/13 22:53:01 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 5.3 KB, free 28.8 MB)
17/01/13 22:53:01 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on localhost:63357 (size: 5.3 KB, free: 483.1 MB)
17/01/13 22:53:01 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1006
17/01/13 22:53:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[95] at count at null:-2)
17/01/13 22:53:01 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks
17/01/13 22:53:01 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 30, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 22:53:01 INFO Executor: Running task 0.0 in stage 23.0 (TID 30)
17/01/13 22:53:01 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:53:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/01/13 22:53:01 INFO Executor: Finished task 0.0 in stage 23.0 (TID 30). 1959 bytes result sent to driver
17/01/13 22:53:01 INFO DAGScheduler: ResultStage 23 (count at null:-2) finished in 0.012 s
17/01/13 22:53:01 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 30) in 12 ms on localhost (1/1)
17/01/13 22:53:01 INFO DAGScheduler: Job 16 finished: count at null:-2, took 0.160171 s
17/01/13 22:53:01 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
17/01/13 22:53:01 INFO ParseDriver: Parsing command: SELECT *
FROM `flights`
WHERE (`dep_delay` = 2.0)
17/01/13 22:53:01 INFO ParseDriver: Parse Completed
17/01/13 22:53:01 INFO InMemoryColumnarTableScan: Predicate (dep_delay#146 = 2.0) generates partition filter: ((dep_delay.lowerBound#1170 <= 2.0) && (2.0 <= dep_delay.upperBound#1169))
17/01/13 22:53:01 INFO SparkContext: Starting job: count at null:-2
17/01/13 22:53:01 INFO DAGScheduler: Registering RDD 100 (count at null:-2)
17/01/13 22:53:01 INFO DAGScheduler: Got job 17 (count at null:-2) with 1 output partitions
17/01/13 22:53:01 INFO DAGScheduler: Final stage: ResultStage 25 (count at null:-2)
17/01/13 22:53:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 24)
17/01/13 22:53:01 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 24)
17/01/13 22:53:01 INFO DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[100] at count at null:-2), which has no missing parents
17/01/13 22:53:01 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 28.8 KB, free 28.8 MB)
17/01/13 22:53:01 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 11.8 KB, free 28.8 MB)
17/01/13 22:53:01 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on localhost:63357 (size: 11.8 KB, free: 483.1 MB)
17/01/13 22:53:01 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1006
17/01/13 22:53:01 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[100] at count at null:-2)
17/01/13 22:53:01 INFO TaskSchedulerImpl: Adding task set 24.0 with 2 tasks
17/01/13 22:53:01 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 31, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:53:01 INFO TaskSetManager: Starting task 1.0 in stage 24.0 (TID 32, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:53:01 INFO Executor: Running task 1.0 in stage 24.0 (TID 32)
17/01/13 22:53:01 INFO Executor: Running task 0.0 in stage 24.0 (TID 31)
17/01/13 22:53:01 INFO BlockManager: Found block rdd_42_0 locally
17/01/13 22:53:01 INFO BlockManager: Found block rdd_42_1 locally
17/01/13 22:53:01 INFO Executor: Finished task 0.0 in stage 24.0 (TID 31). 2808 bytes result sent to driver
17/01/13 22:53:01 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 31) in 35 ms on localhost (1/2)
17/01/13 22:53:01 INFO Executor: Finished task 1.0 in stage 24.0 (TID 32). 2808 bytes result sent to driver
17/01/13 22:53:01 INFO TaskSetManager: Finished task 1.0 in stage 24.0 (TID 32) in 39 ms on localhost (2/2)
17/01/13 22:53:01 INFO DAGScheduler: ShuffleMapStage 24 (count at null:-2) finished in 0.039 s
17/01/13 22:53:01 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
17/01/13 22:53:01 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:53:01 INFO DAGScheduler: running: Set()
17/01/13 22:53:01 INFO DAGScheduler: waiting: Set(ResultStage 25)
17/01/13 22:53:01 INFO DAGScheduler: failed: Set()
17/01/13 22:53:01 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[103] at count at null:-2), which has no missing parents
17/01/13 22:53:01 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 10.8 KB, free 28.8 MB)
17/01/13 22:53:01 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 5.3 KB, free 28.8 MB)
17/01/13 22:53:01 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on localhost:63357 (size: 5.3 KB, free: 483.1 MB)
17/01/13 22:53:01 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1006
17/01/13 22:53:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[103] at count at null:-2)
17/01/13 22:53:01 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks
17/01/13 22:53:01 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 33, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 22:53:01 INFO Executor: Running task 0.0 in stage 25.0 (TID 33)
17/01/13 22:53:01 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:53:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:53:01 INFO Executor: Finished task 0.0 in stage 25.0 (TID 33). 1959 bytes result sent to driver
17/01/13 22:53:01 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 33) in 9 ms on localhost (1/1)
17/01/13 22:53:01 INFO DAGScheduler: ResultStage 25 (count at null:-2) finished in 0.010 s
17/01/13 22:53:01 INFO DAGScheduler: Job 17 finished: count at null:-2, took 0.057205 s
17/01/13 22:53:01 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
17/01/13 22:53:01 INFO ParseDriver: Parsing command: SELECT *
FROM (SELECT *
FROM `flights`
WHERE (`dep_delay` = 2.0)) `usbgtyvibt`
LIMIT 10
17/01/13 22:53:01 INFO ParseDriver: Parse Completed
17/01/13 22:53:01 INFO InMemoryColumnarTableScan: Predicate (dep_delay#146 = 2.0) generates partition filter: ((dep_delay.lowerBound#1268 <= 2.0) && (2.0 <= dep_delay.upperBound#1267))
17/01/13 22:53:01 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 22:53:01 INFO DAGScheduler: Got job 18 (collect at utils.scala:195) with 1 output partitions
17/01/13 22:53:01 INFO DAGScheduler: Final stage: ResultStage 26 (collect at utils.scala:195)
17/01/13 22:53:01 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:53:01 INFO DAGScheduler: Missing parents: List()
17/01/13 22:53:01 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[107] at collect at utils.scala:195), which has no missing parents
17/01/13 22:53:01 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 25.3 KB, free 28.8 MB)
17/01/13 22:53:01 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 10.2 KB, free 28.9 MB)
17/01/13 22:53:01 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on localhost:63357 (size: 10.2 KB, free: 483.1 MB)
17/01/13 22:53:01 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1006
17/01/13 22:53:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[107] at collect at utils.scala:195)
17/01/13 22:53:01 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks
17/01/13 22:53:01 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 34, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 22:53:01 INFO Executor: Running task 0.0 in stage 26.0 (TID 34)
17/01/13 22:53:01 INFO BlockManager: Found block rdd_42_0 locally
17/01/13 22:53:01 INFO GenerateColumnAccessor: Code generated in 27.078804 ms
17/01/13 22:53:01 INFO GeneratePredicate: Code generated in 4.201809 ms
17/01/13 22:53:01 INFO GenerateSafeProjection: Code generated in 17.059825 ms
17/01/13 22:53:01 INFO Executor: Finished task 0.0 in stage 26.0 (TID 34). 5325 bytes result sent to driver
17/01/13 22:53:01 INFO DAGScheduler: ResultStage 26 (collect at utils.scala:195) finished in 0.083 s
17/01/13 22:53:01 INFO DAGScheduler: Job 18 finished: collect at utils.scala:195, took 0.088567 s
17/01/13 22:53:01 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 34) in 82 ms on localhost (1/1)
17/01/13 22:53:01 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
17/01/13 22:53:01 INFO ParseDriver: Parsing command: SELECT *
FROM (SELECT `tailnum`, count(*) AS `count`, AVG(`distance`) AS `dist`, AVG(`arr_delay`) AS `delay`
FROM `flights`
GROUP BY `tailnum`) `qyqnjfagjv`
WHERE ((`count` > 20.0) AND (`dist` < 2000.0) AND (NOT((`delay`) IS NULL)))
17/01/13 22:53:01 INFO ParseDriver: Parse Completed
17/01/13 22:53:01 INFO ContextCleaner: Cleaned accumulator 79
17/01/13 22:53:01 INFO BlockManagerInfo: Removed broadcast_30_piece0 on localhost:63357 in memory (size: 3.0 KB, free: 483.1 MB)
17/01/13 22:53:01 INFO ContextCleaner: Cleaned accumulator 78
17/01/13 22:53:01 INFO ContextCleaner: Cleaned accumulator 102
17/01/13 22:53:01 INFO ContextCleaner: Cleaned accumulator 101
17/01/13 22:53:01 INFO ContextCleaner: Cleaned accumulator 100
17/01/13 22:53:01 INFO ContextCleaner: Cleaned accumulator 99
17/01/13 22:53:01 INFO ContextCleaner: Cleaned accumulator 98
17/01/13 22:53:01 INFO ContextCleaner: Cleaned accumulator 97
17/01/13 22:53:01 INFO ContextCleaner: Cleaned accumulator 96
17/01/13 22:53:01 INFO ContextCleaner: Cleaned accumulator 95
17/01/13 22:53:01 INFO ContextCleaner: Cleaned accumulator 94
17/01/13 22:53:01 INFO ContextCleaner: Cleaned accumulator 93
17/01/13 22:53:01 INFO ContextCleaner: Cleaned accumulator 92
17/01/13 22:53:01 INFO BlockManagerInfo: Removed broadcast_32_piece0 on localhost:63357 in memory (size: 5.3 KB, free: 483.1 MB)
17/01/13 22:53:01 INFO ContextCleaner: Cleaned accumulator 91
17/01/13 22:53:01 INFO BlockManagerInfo: Removed broadcast_31_piece0 on localhost:63357 in memory (size: 11.7 KB, free: 483.1 MB)
17/01/13 22:53:01 INFO ContextCleaner: Cleaned accumulator 90
17/01/13 22:53:01 INFO ContextCleaner: Cleaned shuffle 6
17/01/13 22:53:01 INFO ContextCleaner: Cleaned accumulator 89
17/01/13 22:53:01 INFO ContextCleaner: Cleaned accumulator 88
17/01/13 22:53:01 INFO ContextCleaner: Cleaned accumulator 87
17/01/13 22:53:01 INFO ContextCleaner: Cleaned accumulator 86
17/01/13 22:53:01 INFO ContextCleaner: Cleaned accumulator 85
17/01/13 22:53:01 INFO ContextCleaner: Cleaned accumulator 84
17/01/13 22:53:01 INFO ContextCleaner: Cleaned accumulator 83
17/01/13 22:53:01 INFO ContextCleaner: Cleaned accumulator 82
17/01/13 22:53:01 INFO ContextCleaner: Cleaned accumulator 81
17/01/13 22:53:01 INFO ContextCleaner: Cleaned accumulator 80
17/01/13 22:53:01 INFO ContextCleaner: Cleaned accumulator 77
17/01/13 22:53:01 INFO BlockManagerInfo: Removed broadcast_29_piece0 on localhost:63357 in memory (size: 4.7 KB, free: 483.1 MB)
17/01/13 22:53:01 INFO ContextCleaner: Cleaned accumulator 76
17/01/13 22:53:01 INFO BlockManagerInfo: Removed broadcast_28_piece0 on localhost:63357 in memory (size: 11.2 KB, free: 483.1 MB)
17/01/13 22:53:01 INFO ContextCleaner: Cleaned accumulator 75
17/01/13 22:53:01 INFO ContextCleaner: Cleaned shuffle 5
17/01/13 22:53:01 INFO BlockManagerInfo: Removed broadcast_27_piece0 on localhost:63357 in memory (size: 4.6 KB, free: 483.1 MB)
17/01/13 22:53:01 INFO BlockManagerInfo: Removed broadcast_35_piece0 on localhost:63357 in memory (size: 10.2 KB, free: 483.1 MB)
17/01/13 22:53:01 INFO ContextCleaner: Cleaned accumulator 107
17/01/13 22:53:01 INFO BlockManagerInfo: Removed broadcast_34_piece0 on localhost:63357 in memory (size: 5.3 KB, free: 483.1 MB)
17/01/13 22:53:01 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 22:53:01 INFO ContextCleaner: Cleaned accumulator 104
17/01/13 22:53:01 INFO DAGScheduler: Registering RDD 110 (collect at utils.scala:195)
17/01/13 22:53:01 INFO DAGScheduler: Got job 19 (collect at utils.scala:195) with 4 output partitions
17/01/13 22:53:01 INFO DAGScheduler: Final stage: ResultStage 28 (collect at utils.scala:195)
17/01/13 22:53:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)
17/01/13 22:53:01 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 27)
17/01/13 22:53:01 INFO DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[110] at collect at utils.scala:195), which has no missing parents
17/01/13 22:53:01 INFO BlockManagerInfo: Removed broadcast_33_piece0 on localhost:63357 in memory (size: 11.8 KB, free: 483.2 MB)
17/01/13 22:53:01 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 28.1 KB, free 28.7 MB)
17/01/13 22:53:01 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 11.4 KB, free 28.7 MB)
17/01/13 22:53:01 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on localhost:63357 (size: 11.4 KB, free: 483.1 MB)
17/01/13 22:53:01 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1006
17/01/13 22:53:01 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[110] at collect at utils.scala:195)
17/01/13 22:53:01 INFO TaskSchedulerImpl: Adding task set 27.0 with 2 tasks
17/01/13 22:53:01 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 35, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:53:01 INFO TaskSetManager: Starting task 1.0 in stage 27.0 (TID 36, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:53:01 INFO Executor: Running task 0.0 in stage 27.0 (TID 35)
17/01/13 22:53:01 INFO Executor: Running task 1.0 in stage 27.0 (TID 36)
17/01/13 22:53:01 INFO ContextCleaner: Cleaned accumulator 103
17/01/13 22:53:01 INFO BlockManager: Found block rdd_42_0 locally
17/01/13 22:53:01 INFO ContextCleaner: Cleaned shuffle 7
17/01/13 22:53:01 INFO BlockManager: Found block rdd_42_1 locally
17/01/13 22:53:01 INFO GenerateColumnAccessor: Code generated in 23.004141 ms
17/01/13 22:53:01 INFO GenerateMutableProjection: Code generated in 6.340262 ms
17/01/13 22:53:02 INFO GenerateMutableProjection: Code generated in 23.34206 ms
17/01/13 22:53:02 INFO GenerateUnsafeRowJoiner: Code generated in 6.162768 ms
17/01/13 22:53:02 INFO GenerateUnsafeProjection: Code generated in 10.286925 ms
17/01/13 22:53:02 INFO GenerateMutableProjection: Code generated in 13.749322 ms
17/01/13 22:53:02 INFO Executor: Finished task 1.0 in stage 27.0 (TID 36). 2682 bytes result sent to driver
17/01/13 22:53:02 INFO TaskSetManager: Finished task 1.0 in stage 27.0 (TID 36) in 452 ms on localhost (1/2)
17/01/13 22:53:02 INFO Executor: Finished task 0.0 in stage 27.0 (TID 35). 2682 bytes result sent to driver
17/01/13 22:53:02 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 35) in 493 ms on localhost (2/2)
17/01/13 22:53:02 INFO DAGScheduler: ShuffleMapStage 27 (collect at utils.scala:195) finished in 0.493 s
17/01/13 22:53:02 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
17/01/13 22:53:02 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:53:02 INFO DAGScheduler: running: Set()
17/01/13 22:53:02 INFO DAGScheduler: waiting: Set(ResultStage 28)
17/01/13 22:53:02 INFO DAGScheduler: failed: Set()
17/01/13 22:53:02 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[115] at collect at utils.scala:195), which has no missing parents
17/01/13 22:53:02 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 13.5 KB, free 28.7 MB)
17/01/13 22:53:02 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 6.2 KB, free 28.7 MB)
17/01/13 22:53:02 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on localhost:63357 (size: 6.2 KB, free: 483.1 MB)
17/01/13 22:53:02 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1006
17/01/13 22:53:02 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 28 (MapPartitionsRDD[115] at collect at utils.scala:195)
17/01/13 22:53:02 INFO TaskSchedulerImpl: Adding task set 28.0 with 4 tasks
17/01/13 22:53:02 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 37, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 22:53:02 INFO TaskSetManager: Starting task 1.0 in stage 28.0 (TID 38, localhost, partition 1,NODE_LOCAL, 2242 bytes)
17/01/13 22:53:02 INFO TaskSetManager: Starting task 2.0 in stage 28.0 (TID 39, localhost, partition 2,NODE_LOCAL, 2242 bytes)
17/01/13 22:53:02 INFO TaskSetManager: Starting task 3.0 in stage 28.0 (TID 40, localhost, partition 3,NODE_LOCAL, 2242 bytes)
17/01/13 22:53:02 INFO Executor: Running task 0.0 in stage 28.0 (TID 37)
17/01/13 22:53:02 INFO Executor: Running task 1.0 in stage 28.0 (TID 38)
17/01/13 22:53:02 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:53:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:53:02 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:53:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:53:02 INFO Executor: Running task 2.0 in stage 28.0 (TID 39)
17/01/13 22:53:02 INFO Executor: Running task 3.0 in stage 28.0 (TID 40)
17/01/13 22:53:02 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:53:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:53:02 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:53:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:53:02 INFO GenerateMutableProjection: Code generated in 11.953483 ms
17/01/13 22:53:02 INFO GenerateMutableProjection: Code generated in 8.124153 ms
17/01/13 22:53:02 INFO GenerateUnsafeProjection: Code generated in 7.32074 ms
17/01/13 22:53:02 INFO GeneratePredicate: Code generated in 23.022487 ms
17/01/13 22:53:02 INFO Executor: Finished task 3.0 in stage 28.0 (TID 40). 48431 bytes result sent to driver
17/01/13 22:53:02 INFO Executor: Finished task 0.0 in stage 28.0 (TID 37). 49931 bytes result sent to driver
17/01/13 22:53:02 INFO TaskSetManager: Finished task 3.0 in stage 28.0 (TID 40) in 115 ms on localhost (1/4)
17/01/13 22:53:02 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 37) in 117 ms on localhost (2/4)
17/01/13 22:53:02 INFO Executor: Finished task 2.0 in stage 28.0 (TID 39). 50586 bytes result sent to driver
17/01/13 22:53:02 INFO Executor: Finished task 1.0 in stage 28.0 (TID 38). 52153 bytes result sent to driver
17/01/13 22:53:02 INFO TaskSetManager: Finished task 2.0 in stage 28.0 (TID 39) in 121 ms on localhost (3/4)
17/01/13 22:53:02 INFO DAGScheduler: ResultStage 28 (collect at utils.scala:195) finished in 0.125 s
17/01/13 22:53:02 INFO DAGScheduler: Job 19 finished: collect at utils.scala:195, took 0.632880 s
17/01/13 22:53:02 INFO TaskSetManager: Finished task 1.0 in stage 28.0 (TID 38) in 125 ms on localhost (4/4)
17/01/13 22:53:02 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
17/01/13 22:53:06 INFO ParseDriver: Parsing command: SELECT `playerID`, `yearID`, `teamID`, `G`, `AB`, `R`, `H`
FROM (SELECT `playerID`, `yearID`, `teamID`, `G`, `AB`, `R`, `H`, rank() OVER (PARTITION BY `playerID` ORDER BY `H` DESC) AS `zzz4`
FROM (SELECT *
FROM (SELECT `playerID` AS `playerID`, `yearID` AS `yearID`, `teamID` AS `teamID`, `G` AS `G`, `AB` AS `AB`, `R` AS `R`, `H` AS `H`
FROM `batting`) `pgyqrqmgco`
ORDER BY `playerID`, `yearID`, `teamID`) `tlclzziefh`) `qqsgagygxs`
WHERE (`zzz4` <= 2.0 AND `H` > 0.0)
17/01/13 22:53:06 INFO ParseDriver: Parse Completed
17/01/13 22:53:07 INFO SparkContext: Starting job: count at null:-2
17/01/13 22:53:07 INFO DAGScheduler: Got job 20 (count at null:-2) with 2 output partitions
17/01/13 22:53:07 INFO DAGScheduler: Final stage: ResultStage 29 (count at null:-2)
17/01/13 22:53:07 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:53:07 INFO DAGScheduler: Missing parents: List()
17/01/13 22:53:07 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[121] at count at null:-2), which has no missing parents
17/01/13 22:53:07 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 26.4 KB, free 28.7 MB)
17/01/13 22:53:07 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 10.5 KB, free 28.7 MB)
17/01/13 22:53:07 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on localhost:63357 (size: 10.5 KB, free: 483.1 MB)
17/01/13 22:53:07 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1006
17/01/13 22:53:07 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 29 (MapPartitionsRDD[121] at count at null:-2)
17/01/13 22:53:07 INFO TaskSchedulerImpl: Adding task set 29.0 with 2 tasks
17/01/13 22:53:07 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 41, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 22:53:07 INFO TaskSetManager: Starting task 1.0 in stage 29.0 (TID 42, localhost, partition 1,PROCESS_LOCAL, 2481 bytes)
17/01/13 22:53:07 INFO Executor: Running task 0.0 in stage 29.0 (TID 41)
17/01/13 22:53:07 INFO Executor: Running task 1.0 in stage 29.0 (TID 42)
17/01/13 22:53:07 INFO BlockManager: Found block rdd_70_0 locally
17/01/13 22:53:07 INFO BlockManager: Found block rdd_70_1 locally
17/01/13 22:53:07 INFO GenerateColumnAccessor: Code generated in 8.786339 ms
17/01/13 22:53:07 INFO GenerateUnsafeProjection: Code generated in 7.232848 ms
17/01/13 22:53:07 INFO GenerateSafeProjection: Code generated in 7.35786 ms
17/01/13 22:53:07 INFO Executor: Finished task 1.0 in stage 29.0 (TID 42). 13552 bytes result sent to driver
17/01/13 22:53:07 INFO TaskSetManager: Finished task 1.0 in stage 29.0 (TID 42) in 148 ms on localhost (1/2)
17/01/13 22:53:07 INFO Executor: Finished task 0.0 in stage 29.0 (TID 41). 13905 bytes result sent to driver
17/01/13 22:53:07 INFO DAGScheduler: ResultStage 29 (count at null:-2) finished in 0.155 s
17/01/13 22:53:07 INFO DAGScheduler: Job 20 finished: count at null:-2, took 0.161182 s
17/01/13 22:53:07 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 41) in 155 ms on localhost (2/2)
17/01/13 22:53:07 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool 
17/01/13 22:53:07 INFO SparkContext: Starting job: count at null:-2
17/01/13 22:53:07 INFO DAGScheduler: Registering RDD 122 (count at null:-2)
17/01/13 22:53:07 INFO DAGScheduler: Registering RDD 126 (count at null:-2)
17/01/13 22:53:07 INFO DAGScheduler: Registering RDD 133 (count at null:-2)
17/01/13 22:53:07 INFO DAGScheduler: Got job 21 (count at null:-2) with 1 output partitions
17/01/13 22:53:07 INFO DAGScheduler: Final stage: ResultStage 33 (count at null:-2)
17/01/13 22:53:07 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 32)
17/01/13 22:53:07 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 32)
17/01/13 22:53:07 INFO DAGScheduler: Submitting ShuffleMapStage 30 (MapPartitionsRDD[122] at count at null:-2), which has no missing parents
17/01/13 22:53:07 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 28.9 KB, free 28.8 MB)
17/01/13 22:53:07 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 11.8 KB, free 28.8 MB)
17/01/13 22:53:07 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on localhost:63357 (size: 11.8 KB, free: 483.1 MB)
17/01/13 22:53:07 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1006
17/01/13 22:53:07 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 30 (MapPartitionsRDD[122] at count at null:-2)
17/01/13 22:53:07 INFO TaskSchedulerImpl: Adding task set 30.0 with 2 tasks
17/01/13 22:53:07 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 43, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:53:07 INFO TaskSetManager: Starting task 1.0 in stage 30.0 (TID 44, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:53:07 INFO Executor: Running task 0.0 in stage 30.0 (TID 43)
17/01/13 22:53:07 INFO Executor: Running task 1.0 in stage 30.0 (TID 44)
17/01/13 22:53:07 INFO BlockManager: Found block rdd_70_0 locally
17/01/13 22:53:07 INFO BlockManager: Found block rdd_70_1 locally
17/01/13 22:53:08 INFO Executor: Finished task 0.0 in stage 30.0 (TID 43). 2580 bytes result sent to driver
17/01/13 22:53:08 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 43) in 725 ms on localhost (1/2)
17/01/13 22:53:08 INFO Executor: Finished task 1.0 in stage 30.0 (TID 44). 2580 bytes result sent to driver
17/01/13 22:53:08 INFO DAGScheduler: ShuffleMapStage 30 (count at null:-2) finished in 0.736 s
17/01/13 22:53:08 INFO TaskSetManager: Finished task 1.0 in stage 30.0 (TID 44) in 735 ms on localhost (2/2)
17/01/13 22:53:08 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:53:08 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool 
17/01/13 22:53:08 INFO DAGScheduler: running: Set()
17/01/13 22:53:08 INFO DAGScheduler: waiting: Set(ResultStage 33, ShuffleMapStage 31, ShuffleMapStage 32)
17/01/13 22:53:08 INFO DAGScheduler: failed: Set()
17/01/13 22:53:08 INFO DAGScheduler: Submitting ShuffleMapStage 31 (MapPartitionsRDD[126] at count at null:-2), which has no missing parents
17/01/13 22:53:08 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 10.3 KB, free 28.8 MB)
17/01/13 22:53:08 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 5.3 KB, free 28.8 MB)
17/01/13 22:53:08 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on localhost:63357 (size: 5.3 KB, free: 483.1 MB)
17/01/13 22:53:08 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1006
17/01/13 22:53:08 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 31 (MapPartitionsRDD[126] at count at null:-2)
17/01/13 22:53:08 INFO TaskSchedulerImpl: Adding task set 31.0 with 4 tasks
17/01/13 22:53:08 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 45, localhost, partition 0,NODE_LOCAL, 2231 bytes)
17/01/13 22:53:08 INFO TaskSetManager: Starting task 1.0 in stage 31.0 (TID 46, localhost, partition 1,NODE_LOCAL, 2231 bytes)
17/01/13 22:53:08 INFO TaskSetManager: Starting task 2.0 in stage 31.0 (TID 47, localhost, partition 2,NODE_LOCAL, 2231 bytes)
17/01/13 22:53:08 INFO TaskSetManager: Starting task 3.0 in stage 31.0 (TID 48, localhost, partition 3,NODE_LOCAL, 2231 bytes)
17/01/13 22:53:08 INFO Executor: Running task 1.0 in stage 31.0 (TID 46)
17/01/13 22:53:08 INFO Executor: Running task 3.0 in stage 31.0 (TID 48)
17/01/13 22:53:08 INFO Executor: Running task 2.0 in stage 31.0 (TID 47)
17/01/13 22:53:08 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:53:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:53:08 INFO Executor: Running task 0.0 in stage 31.0 (TID 45)
17/01/13 22:53:08 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:53:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:53:08 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:53:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:53:08 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:53:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:53:08 INFO GenerateUnsafeProjection: Code generated in 7.722234 ms
17/01/13 22:53:08 INFO GenerateOrdering: Code generated in 34.774157 ms
17/01/13 22:53:08 INFO GenerateUnsafeProjection: Code generated in 4.383143 ms
17/01/13 22:53:09 INFO Executor: Finished task 2.0 in stage 31.0 (TID 47). 1660 bytes result sent to driver
17/01/13 22:53:09 INFO TaskSetManager: Finished task 2.0 in stage 31.0 (TID 47) in 705 ms on localhost (1/4)
17/01/13 22:53:09 INFO Executor: Finished task 3.0 in stage 31.0 (TID 48). 1660 bytes result sent to driver
17/01/13 22:53:09 INFO TaskSetManager: Finished task 3.0 in stage 31.0 (TID 48) in 719 ms on localhost (2/4)
17/01/13 22:53:09 INFO Executor: Finished task 1.0 in stage 31.0 (TID 46). 1660 bytes result sent to driver
17/01/13 22:53:09 INFO TaskSetManager: Finished task 1.0 in stage 31.0 (TID 46) in 779 ms on localhost (3/4)
17/01/13 22:53:09 INFO Executor: Finished task 0.0 in stage 31.0 (TID 45). 1660 bytes result sent to driver
17/01/13 22:53:09 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 45) in 795 ms on localhost (4/4)
17/01/13 22:53:09 INFO DAGScheduler: ShuffleMapStage 31 (count at null:-2) finished in 0.795 s
17/01/13 22:53:09 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool 
17/01/13 22:53:09 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:53:09 INFO DAGScheduler: running: Set()
17/01/13 22:53:09 INFO DAGScheduler: waiting: Set(ResultStage 33, ShuffleMapStage 32)
17/01/13 22:53:09 INFO DAGScheduler: failed: Set()
17/01/13 22:53:09 INFO DAGScheduler: Submitting ShuffleMapStage 32 (MapPartitionsRDD[133] at count at null:-2), which has no missing parents
17/01/13 22:53:09 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 17.9 KB, free 28.8 MB)
17/01/13 22:53:09 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 8.1 KB, free 28.8 MB)
17/01/13 22:53:09 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on localhost:63357 (size: 8.1 KB, free: 483.1 MB)
17/01/13 22:53:09 INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1006
17/01/13 22:53:09 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 32 (MapPartitionsRDD[133] at count at null:-2)
17/01/13 22:53:09 INFO TaskSchedulerImpl: Adding task set 32.0 with 4 tasks
17/01/13 22:53:09 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 49, localhost, partition 0,NODE_LOCAL, 2231 bytes)
17/01/13 22:53:09 INFO TaskSetManager: Starting task 1.0 in stage 32.0 (TID 50, localhost, partition 1,NODE_LOCAL, 2231 bytes)
17/01/13 22:53:09 INFO TaskSetManager: Starting task 2.0 in stage 32.0 (TID 51, localhost, partition 2,NODE_LOCAL, 2231 bytes)
17/01/13 22:53:09 INFO TaskSetManager: Starting task 3.0 in stage 32.0 (TID 52, localhost, partition 3,NODE_LOCAL, 2231 bytes)
17/01/13 22:53:09 INFO Executor: Running task 0.0 in stage 32.0 (TID 49)
17/01/13 22:53:09 INFO Executor: Running task 1.0 in stage 32.0 (TID 50)
17/01/13 22:53:09 INFO Executor: Running task 2.0 in stage 32.0 (TID 51)
17/01/13 22:53:09 INFO Executor: Running task 3.0 in stage 32.0 (TID 52)
17/01/13 22:53:09 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/01/13 22:53:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:53:09 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/01/13 22:53:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/01/13 22:53:09 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/01/13 22:53:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:53:09 INFO GenerateOrdering: Code generated in 11.350604 ms
17/01/13 22:53:09 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/01/13 22:53:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:53:09 INFO GenerateMutableProjection: Code generated in 10.044151 ms
17/01/13 22:53:09 INFO GeneratePredicate: Code generated in 9.548792 ms
17/01/13 22:53:09 INFO Executor: Finished task 2.0 in stage 32.0 (TID 51). 1965 bytes result sent to driver
17/01/13 22:53:09 INFO TaskSetManager: Finished task 2.0 in stage 32.0 (TID 51) in 486 ms on localhost (1/4)
17/01/13 22:53:09 INFO Executor: Finished task 1.0 in stage 32.0 (TID 50). 1965 bytes result sent to driver
17/01/13 22:53:09 INFO TaskSetManager: Finished task 1.0 in stage 32.0 (TID 50) in 499 ms on localhost (2/4)
17/01/13 22:53:09 INFO Executor: Finished task 0.0 in stage 32.0 (TID 49). 1965 bytes result sent to driver
17/01/13 22:53:09 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 49) in 517 ms on localhost (3/4)
17/01/13 22:53:09 INFO Executor: Finished task 3.0 in stage 32.0 (TID 52). 1965 bytes result sent to driver
17/01/13 22:53:09 INFO DAGScheduler: ShuffleMapStage 32 (count at null:-2) finished in 0.535 s
17/01/13 22:53:09 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:53:09 INFO DAGScheduler: running: Set()
17/01/13 22:53:09 INFO DAGScheduler: waiting: Set(ResultStage 33)
17/01/13 22:53:09 INFO DAGScheduler: failed: Set()
17/01/13 22:53:09 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[136] at count at null:-2), which has no missing parents
17/01/13 22:53:09 INFO TaskSetManager: Finished task 3.0 in stage 32.0 (TID 52) in 534 ms on localhost (4/4)
17/01/13 22:53:09 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
17/01/13 22:53:09 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 15.8 KB, free 28.8 MB)
17/01/13 22:53:09 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 7.2 KB, free 28.8 MB)
17/01/13 22:53:09 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on localhost:63357 (size: 7.2 KB, free: 483.1 MB)
17/01/13 22:53:09 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1006
17/01/13 22:53:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[136] at count at null:-2)
17/01/13 22:53:09 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks
17/01/13 22:53:09 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 53, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 22:53:09 INFO Executor: Running task 0.0 in stage 33.0 (TID 53)
17/01/13 22:53:09 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/01/13 22:53:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:53:09 INFO Executor: Finished task 0.0 in stage 33.0 (TID 53). 2174 bytes result sent to driver
17/01/13 22:53:09 INFO DAGScheduler: ResultStage 33 (count at null:-2) finished in 0.010 s
17/01/13 22:53:09 INFO DAGScheduler: Job 21 finished: count at null:-2, took 2.100706 s
17/01/13 22:53:09 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 53) in 10 ms on localhost (1/1)
17/01/13 22:53:09 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool 
17/01/13 22:53:09 INFO ParseDriver: Parsing command: SELECT `playerID`, `yearID`, `teamID`, `G`, `AB`, `R`, `H`
FROM (SELECT `playerID`, `yearID`, `teamID`, `G`, `AB`, `R`, `H`, rank() OVER (PARTITION BY `playerID` ORDER BY `H` DESC) AS `zzz5`
FROM (SELECT *
FROM (SELECT `playerID` AS `playerID`, `yearID` AS `yearID`, `teamID` AS `teamID`, `G` AS `G`, `AB` AS `AB`, `R` AS `R`, `H` AS `H`
FROM `batting`) `epffegqynn`
ORDER BY `playerID`, `yearID`, `teamID`) `hivrqumuob`) `feltpcyaka`
WHERE (`zzz5` <= 2.0 AND `H` > 0.0)
17/01/13 22:53:09 INFO ParseDriver: Parse Completed
17/01/13 22:53:09 INFO SparkContext: Starting job: count at null:-2
17/01/13 22:53:09 INFO DAGScheduler: Got job 22 (count at null:-2) with 2 output partitions
17/01/13 22:53:09 INFO DAGScheduler: Final stage: ResultStage 34 (count at null:-2)
17/01/13 22:53:09 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:53:09 INFO DAGScheduler: Missing parents: List()
17/01/13 22:53:09 INFO DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[142] at count at null:-2), which has no missing parents
17/01/13 22:53:09 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 26.4 KB, free 28.9 MB)
17/01/13 22:53:10 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 10.4 KB, free 28.9 MB)
17/01/13 22:53:10 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on localhost:63357 (size: 10.4 KB, free: 483.1 MB)
17/01/13 22:53:10 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1006
17/01/13 22:53:10 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 34 (MapPartitionsRDD[142] at count at null:-2)
17/01/13 22:53:10 INFO TaskSchedulerImpl: Adding task set 34.0 with 2 tasks
17/01/13 22:53:10 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 54, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 22:53:10 INFO TaskSetManager: Starting task 1.0 in stage 34.0 (TID 55, localhost, partition 1,PROCESS_LOCAL, 2481 bytes)
17/01/13 22:53:10 INFO Executor: Running task 1.0 in stage 34.0 (TID 55)
17/01/13 22:53:10 INFO Executor: Running task 0.0 in stage 34.0 (TID 54)
17/01/13 22:53:10 INFO BlockManager: Found block rdd_70_0 locally
17/01/13 22:53:10 INFO BlockManager: Found block rdd_70_1 locally
17/01/13 22:53:10 INFO Executor: Finished task 1.0 in stage 34.0 (TID 55). 13661 bytes result sent to driver
17/01/13 22:53:10 INFO TaskSetManager: Finished task 1.0 in stage 34.0 (TID 55) in 67 ms on localhost (1/2)
17/01/13 22:53:10 INFO Executor: Finished task 0.0 in stage 34.0 (TID 54). 13877 bytes result sent to driver
17/01/13 22:53:10 INFO DAGScheduler: ResultStage 34 (count at null:-2) finished in 0.077 s
17/01/13 22:53:10 INFO DAGScheduler: Job 22 finished: count at null:-2, took 0.082705 s
17/01/13 22:53:10 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 54) in 76 ms on localhost (2/2)
17/01/13 22:53:10 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool 
17/01/13 22:53:10 INFO SparkContext: Starting job: count at null:-2
17/01/13 22:53:10 INFO DAGScheduler: Registering RDD 143 (count at null:-2)
17/01/13 22:53:10 INFO DAGScheduler: Registering RDD 147 (count at null:-2)
17/01/13 22:53:10 INFO DAGScheduler: Registering RDD 154 (count at null:-2)
17/01/13 22:53:10 INFO DAGScheduler: Got job 23 (count at null:-2) with 1 output partitions
17/01/13 22:53:10 INFO DAGScheduler: Final stage: ResultStage 38 (count at null:-2)
17/01/13 22:53:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 37)
17/01/13 22:53:10 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 37)
17/01/13 22:53:10 INFO DAGScheduler: Submitting ShuffleMapStage 35 (MapPartitionsRDD[143] at count at null:-2), which has no missing parents
17/01/13 22:53:10 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 28.9 KB, free 28.9 MB)
17/01/13 22:53:10 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 11.7 KB, free 28.9 MB)
17/01/13 22:53:10 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on localhost:63357 (size: 11.7 KB, free: 483.1 MB)
17/01/13 22:53:10 INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1006
17/01/13 22:53:10 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 35 (MapPartitionsRDD[143] at count at null:-2)
17/01/13 22:53:10 INFO TaskSchedulerImpl: Adding task set 35.0 with 2 tasks
17/01/13 22:53:10 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 56, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:53:10 INFO TaskSetManager: Starting task 1.0 in stage 35.0 (TID 57, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:53:10 INFO Executor: Running task 0.0 in stage 35.0 (TID 56)
17/01/13 22:53:10 INFO Executor: Running task 1.0 in stage 35.0 (TID 57)
17/01/13 22:53:10 INFO BlockManager: Found block rdd_70_0 locally
17/01/13 22:53:10 INFO BlockManager: Found block rdd_70_1 locally
17/01/13 22:53:10 INFO Executor: Finished task 1.0 in stage 35.0 (TID 57). 2580 bytes result sent to driver
17/01/13 22:53:10 INFO TaskSetManager: Finished task 1.0 in stage 35.0 (TID 57) in 211 ms on localhost (1/2)
17/01/13 22:53:10 INFO Executor: Finished task 0.0 in stage 35.0 (TID 56). 2580 bytes result sent to driver
17/01/13 22:53:10 INFO DAGScheduler: ShuffleMapStage 35 (count at null:-2) finished in 0.216 s
17/01/13 22:53:10 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 56) in 215 ms on localhost (2/2)
17/01/13 22:53:10 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:53:10 INFO DAGScheduler: running: Set()
17/01/13 22:53:10 INFO DAGScheduler: waiting: Set(ShuffleMapStage 37, ResultStage 38, ShuffleMapStage 36)
17/01/13 22:53:10 INFO DAGScheduler: failed: Set()
17/01/13 22:53:10 INFO DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[147] at count at null:-2), which has no missing parents
17/01/13 22:53:10 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool 
17/01/13 22:53:10 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 10.3 KB, free 28.9 MB)
17/01/13 22:53:10 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 5.3 KB, free 28.9 MB)
17/01/13 22:53:10 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on localhost:63357 (size: 5.3 KB, free: 483.1 MB)
17/01/13 22:53:10 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1006
17/01/13 22:53:10 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[147] at count at null:-2)
17/01/13 22:53:10 INFO TaskSchedulerImpl: Adding task set 36.0 with 4 tasks
17/01/13 22:53:10 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 58, localhost, partition 0,NODE_LOCAL, 2231 bytes)
17/01/13 22:53:10 INFO TaskSetManager: Starting task 1.0 in stage 36.0 (TID 59, localhost, partition 1,NODE_LOCAL, 2231 bytes)
17/01/13 22:53:10 INFO TaskSetManager: Starting task 2.0 in stage 36.0 (TID 60, localhost, partition 2,NODE_LOCAL, 2231 bytes)
17/01/13 22:53:10 INFO TaskSetManager: Starting task 3.0 in stage 36.0 (TID 61, localhost, partition 3,NODE_LOCAL, 2231 bytes)
17/01/13 22:53:10 INFO Executor: Running task 0.0 in stage 36.0 (TID 58)
17/01/13 22:53:10 INFO Executor: Running task 1.0 in stage 36.0 (TID 59)
17/01/13 22:53:10 INFO Executor: Running task 2.0 in stage 36.0 (TID 60)
17/01/13 22:53:10 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:53:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:53:10 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:53:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/01/13 22:53:10 INFO Executor: Running task 3.0 in stage 36.0 (TID 61)
17/01/13 22:53:10 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:53:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:53:10 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:53:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:53:10 INFO Executor: Finished task 3.0 in stage 36.0 (TID 61). 1660 bytes result sent to driver
17/01/13 22:53:10 INFO TaskSetManager: Finished task 3.0 in stage 36.0 (TID 61) in 370 ms on localhost (1/4)
17/01/13 22:53:10 INFO Executor: Finished task 0.0 in stage 36.0 (TID 58). 1660 bytes result sent to driver
17/01/13 22:53:10 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 58) in 430 ms on localhost (2/4)
17/01/13 22:53:10 INFO Executor: Finished task 2.0 in stage 36.0 (TID 60). 1660 bytes result sent to driver
17/01/13 22:53:10 INFO TaskSetManager: Finished task 2.0 in stage 36.0 (TID 60) in 493 ms on localhost (3/4)
17/01/13 22:53:10 INFO Executor: Finished task 1.0 in stage 36.0 (TID 59). 1660 bytes result sent to driver
17/01/13 22:53:10 INFO TaskSetManager: Finished task 1.0 in stage 36.0 (TID 59) in 543 ms on localhost (4/4)
17/01/13 22:53:10 INFO DAGScheduler: ShuffleMapStage 36 (count at null:-2) finished in 0.544 s
17/01/13 22:53:10 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:53:10 INFO DAGScheduler: running: Set()
17/01/13 22:53:10 INFO DAGScheduler: waiting: Set(ShuffleMapStage 37, ResultStage 38)
17/01/13 22:53:10 INFO DAGScheduler: failed: Set()
17/01/13 22:53:10 INFO DAGScheduler: Submitting ShuffleMapStage 37 (MapPartitionsRDD[154] at count at null:-2), which has no missing parents
17/01/13 22:53:10 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool 
17/01/13 22:53:10 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 17.9 KB, free 28.9 MB)
17/01/13 22:53:10 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 8.1 KB, free 28.9 MB)
17/01/13 22:53:10 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on localhost:63357 (size: 8.1 KB, free: 483.1 MB)
17/01/13 22:53:10 INFO SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1006
17/01/13 22:53:10 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 37 (MapPartitionsRDD[154] at count at null:-2)
17/01/13 22:53:10 INFO TaskSchedulerImpl: Adding task set 37.0 with 4 tasks
17/01/13 22:53:10 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 62, localhost, partition 0,NODE_LOCAL, 2231 bytes)
17/01/13 22:53:10 INFO TaskSetManager: Starting task 1.0 in stage 37.0 (TID 63, localhost, partition 1,NODE_LOCAL, 2231 bytes)
17/01/13 22:53:10 INFO TaskSetManager: Starting task 2.0 in stage 37.0 (TID 64, localhost, partition 2,NODE_LOCAL, 2231 bytes)
17/01/13 22:53:10 INFO TaskSetManager: Starting task 3.0 in stage 37.0 (TID 65, localhost, partition 3,NODE_LOCAL, 2231 bytes)
17/01/13 22:53:10 INFO Executor: Running task 0.0 in stage 37.0 (TID 62)
17/01/13 22:53:10 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/01/13 22:53:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:53:10 INFO Executor: Running task 1.0 in stage 37.0 (TID 63)
17/01/13 22:53:10 INFO Executor: Running task 2.0 in stage 37.0 (TID 64)
17/01/13 22:53:10 INFO Executor: Running task 3.0 in stage 37.0 (TID 65)
17/01/13 22:53:10 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/01/13 22:53:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:53:10 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/01/13 22:53:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:53:10 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/01/13 22:53:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:53:11 INFO Executor: Finished task 2.0 in stage 37.0 (TID 64). 1965 bytes result sent to driver
17/01/13 22:53:11 INFO TaskSetManager: Finished task 2.0 in stage 37.0 (TID 64) in 205 ms on localhost (1/4)
17/01/13 22:53:11 INFO BlockManagerInfo: Removed broadcast_42_piece0 on localhost:63357 in memory (size: 7.2 KB, free: 483.1 MB)
17/01/13 22:53:11 INFO ContextCleaner: Cleaned accumulator 141
17/01/13 22:53:11 INFO BlockManagerInfo: Removed broadcast_45_piece0 on localhost:63357 in memory (size: 5.3 KB, free: 483.1 MB)
17/01/13 22:53:11 INFO BlockManagerInfo: Removed broadcast_44_piece0 on localhost:63357 in memory (size: 11.7 KB, free: 483.1 MB)
17/01/13 22:53:11 INFO BlockManagerInfo: Removed broadcast_43_piece0 on localhost:63357 in memory (size: 10.4 KB, free: 483.1 MB)
17/01/13 22:53:11 INFO ContextCleaner: Cleaned accumulator 158
17/01/13 22:53:11 INFO Executor: Finished task 3.0 in stage 37.0 (TID 65). 1965 bytes result sent to driver
17/01/13 22:53:11 INFO Executor: Finished task 1.0 in stage 37.0 (TID 63). 1965 bytes result sent to driver
17/01/13 22:53:11 INFO TaskSetManager: Finished task 3.0 in stage 37.0 (TID 65) in 266 ms on localhost (2/4)
17/01/13 22:53:11 INFO Executor: Finished task 0.0 in stage 37.0 (TID 62). 1965 bytes result sent to driver
17/01/13 22:53:11 INFO TaskSetManager: Finished task 1.0 in stage 37.0 (TID 63) in 268 ms on localhost (3/4)
17/01/13 22:53:11 INFO DAGScheduler: ShuffleMapStage 37 (count at null:-2) finished in 0.270 s
17/01/13 22:53:11 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 62) in 270 ms on localhost (4/4)
17/01/13 22:53:11 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:53:11 INFO DAGScheduler: running: Set()
17/01/13 22:53:11 INFO DAGScheduler: waiting: Set(ResultStage 38)
17/01/13 22:53:11 INFO DAGScheduler: failed: Set()
17/01/13 22:53:11 INFO DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[157] at count at null:-2), which has no missing parents
17/01/13 22:53:11 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool 
17/01/13 22:53:11 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 15.8 KB, free 28.8 MB)
17/01/13 22:53:11 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 7.3 KB, free 28.9 MB)
17/01/13 22:53:11 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on localhost:63357 (size: 7.3 KB, free: 483.1 MB)
17/01/13 22:53:11 INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1006
17/01/13 22:53:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 38 (MapPartitionsRDD[157] at count at null:-2)
17/01/13 22:53:11 INFO TaskSchedulerImpl: Adding task set 38.0 with 1 tasks
17/01/13 22:53:11 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 66, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 22:53:11 INFO Executor: Running task 0.0 in stage 38.0 (TID 66)
17/01/13 22:53:11 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/01/13 22:53:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:53:11 INFO Executor: Finished task 0.0 in stage 38.0 (TID 66). 2174 bytes result sent to driver
17/01/13 22:53:11 INFO DAGScheduler: ResultStage 38 (count at null:-2) finished in 0.009 s
17/01/13 22:53:11 INFO DAGScheduler: Job 23 finished: count at null:-2, took 1.057192 s
17/01/13 22:53:11 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 66) in 9 ms on localhost (1/1)
17/01/13 22:53:11 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool 
17/01/13 22:53:11 INFO ParseDriver: Parsing command: SELECT *
FROM (SELECT `playerID`, `yearID`, `teamID`, `G`, `AB`, `R`, `H`
FROM (SELECT `playerID`, `yearID`, `teamID`, `G`, `AB`, `R`, `H`, rank() OVER (PARTITION BY `playerID` ORDER BY `H` DESC) AS `zzz6`
FROM (SELECT *
FROM (SELECT `playerID` AS `playerID`, `yearID` AS `yearID`, `teamID` AS `teamID`, `G` AS `G`, `AB` AS `AB`, `R` AS `R`, `H` AS `H`
FROM `batting`) `fuhopfkwvw`
ORDER BY `playerID`, `yearID`, `teamID`) `cwhzjaclbo`) `tlntwdizrt`
WHERE (`zzz6` <= 2.0 AND `H` > 0.0)) `boucnzfvgz`
LIMIT 10
17/01/13 22:53:11 INFO ParseDriver: Parse Completed
17/01/13 22:53:11 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 22:53:11 INFO DAGScheduler: Got job 24 (collect at utils.scala:195) with 2 output partitions
17/01/13 22:53:11 INFO DAGScheduler: Final stage: ResultStage 39 (collect at utils.scala:195)
17/01/13 22:53:11 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:53:11 INFO DAGScheduler: Missing parents: List()
17/01/13 22:53:11 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[163] at collect at utils.scala:195), which has no missing parents
17/01/13 22:53:11 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 26.4 KB, free 28.9 MB)
17/01/13 22:53:11 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 10.4 KB, free 28.9 MB)
17/01/13 22:53:11 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on localhost:63357 (size: 10.4 KB, free: 483.1 MB)
17/01/13 22:53:11 INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1006
17/01/13 22:53:11 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 39 (MapPartitionsRDD[163] at collect at utils.scala:195)
17/01/13 22:53:11 INFO TaskSchedulerImpl: Adding task set 39.0 with 2 tasks
17/01/13 22:53:11 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 67, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 22:53:11 INFO TaskSetManager: Starting task 1.0 in stage 39.0 (TID 68, localhost, partition 1,PROCESS_LOCAL, 2481 bytes)
17/01/13 22:53:11 INFO Executor: Running task 0.0 in stage 39.0 (TID 67)
17/01/13 22:53:11 INFO Executor: Running task 1.0 in stage 39.0 (TID 68)
17/01/13 22:53:11 INFO BlockManager: Found block rdd_70_0 locally
17/01/13 22:53:11 INFO BlockManager: Found block rdd_70_1 locally
17/01/13 22:53:11 INFO Executor: Finished task 0.0 in stage 39.0 (TID 67). 13904 bytes result sent to driver
17/01/13 22:53:11 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 67) in 47 ms on localhost (1/2)
17/01/13 22:53:11 INFO Executor: Finished task 1.0 in stage 39.0 (TID 68). 13625 bytes result sent to driver
17/01/13 22:53:11 INFO TaskSetManager: Finished task 1.0 in stage 39.0 (TID 68) in 54 ms on localhost (2/2)
17/01/13 22:53:11 INFO DAGScheduler: ResultStage 39 (collect at utils.scala:195) finished in 0.056 s
17/01/13 22:53:11 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool 
17/01/13 22:53:11 INFO DAGScheduler: Job 24 finished: collect at utils.scala:195, took 0.060917 s
17/01/13 22:53:11 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 22:53:11 INFO DAGScheduler: Registering RDD 164 (collect at utils.scala:195)
17/01/13 22:53:11 INFO DAGScheduler: Registering RDD 168 (collect at utils.scala:195)
17/01/13 22:53:11 INFO DAGScheduler: Got job 25 (collect at utils.scala:195) with 1 output partitions
17/01/13 22:53:11 INFO DAGScheduler: Final stage: ResultStage 42 (collect at utils.scala:195)
17/01/13 22:53:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 41)
17/01/13 22:53:11 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 41)
17/01/13 22:53:11 INFO DAGScheduler: Submitting ShuffleMapStage 40 (MapPartitionsRDD[164] at collect at utils.scala:195), which has no missing parents
17/01/13 22:53:11 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 28.9 KB, free 28.9 MB)
17/01/13 22:53:11 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 11.7 KB, free 28.9 MB)
17/01/13 22:53:11 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on localhost:63357 (size: 11.7 KB, free: 483.1 MB)
17/01/13 22:53:11 INFO SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1006
17/01/13 22:53:11 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 40 (MapPartitionsRDD[164] at collect at utils.scala:195)
17/01/13 22:53:11 INFO TaskSchedulerImpl: Adding task set 40.0 with 2 tasks
17/01/13 22:53:11 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 69, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:53:11 INFO TaskSetManager: Starting task 1.0 in stage 40.0 (TID 70, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:53:11 INFO Executor: Running task 1.0 in stage 40.0 (TID 70)
17/01/13 22:53:11 INFO Executor: Running task 0.0 in stage 40.0 (TID 69)
17/01/13 22:53:11 INFO BlockManager: Found block rdd_70_1 locally
17/01/13 22:53:11 INFO BlockManager: Found block rdd_70_0 locally
17/01/13 22:53:11 INFO Executor: Finished task 0.0 in stage 40.0 (TID 69). 2580 bytes result sent to driver
17/01/13 22:53:11 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 69) in 176 ms on localhost (1/2)
17/01/13 22:53:11 INFO Executor: Finished task 1.0 in stage 40.0 (TID 70). 2580 bytes result sent to driver
17/01/13 22:53:11 INFO TaskSetManager: Finished task 1.0 in stage 40.0 (TID 70) in 201 ms on localhost (2/2)
17/01/13 22:53:11 INFO DAGScheduler: ShuffleMapStage 40 (collect at utils.scala:195) finished in 0.202 s
17/01/13 22:53:11 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool 
17/01/13 22:53:11 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:53:11 INFO DAGScheduler: running: Set()
17/01/13 22:53:11 INFO DAGScheduler: waiting: Set(ResultStage 42, ShuffleMapStage 41)
17/01/13 22:53:11 INFO DAGScheduler: failed: Set()
17/01/13 22:53:11 INFO DAGScheduler: Submitting ShuffleMapStage 41 (MapPartitionsRDD[168] at collect at utils.scala:195), which has no missing parents
17/01/13 22:53:11 INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 10.3 KB, free 28.9 MB)
17/01/13 22:53:11 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 5.3 KB, free 28.9 MB)
17/01/13 22:53:11 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on localhost:63357 (size: 5.3 KB, free: 483.1 MB)
17/01/13 22:53:11 INFO SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:1006
17/01/13 22:53:11 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 41 (MapPartitionsRDD[168] at collect at utils.scala:195)
17/01/13 22:53:11 INFO TaskSchedulerImpl: Adding task set 41.0 with 4 tasks
17/01/13 22:53:11 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 71, localhost, partition 0,NODE_LOCAL, 2231 bytes)
17/01/13 22:53:11 INFO TaskSetManager: Starting task 1.0 in stage 41.0 (TID 72, localhost, partition 1,NODE_LOCAL, 2231 bytes)
17/01/13 22:53:11 INFO TaskSetManager: Starting task 2.0 in stage 41.0 (TID 73, localhost, partition 2,NODE_LOCAL, 2231 bytes)
17/01/13 22:53:11 INFO TaskSetManager: Starting task 3.0 in stage 41.0 (TID 74, localhost, partition 3,NODE_LOCAL, 2231 bytes)
17/01/13 22:53:11 INFO Executor: Running task 0.0 in stage 41.0 (TID 71)
17/01/13 22:53:11 INFO Executor: Running task 1.0 in stage 41.0 (TID 72)
17/01/13 22:53:11 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:53:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:53:11 INFO Executor: Running task 2.0 in stage 41.0 (TID 73)
17/01/13 22:53:11 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:53:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:53:11 INFO Executor: Running task 3.0 in stage 41.0 (TID 74)
17/01/13 22:53:11 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:53:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:53:11 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:53:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:53:11 INFO Executor: Finished task 1.0 in stage 41.0 (TID 72). 1660 bytes result sent to driver
17/01/13 22:53:11 INFO TaskSetManager: Finished task 1.0 in stage 41.0 (TID 72) in 280 ms on localhost (1/4)
17/01/13 22:53:11 INFO Executor: Finished task 0.0 in stage 41.0 (TID 71). 1660 bytes result sent to driver
17/01/13 22:53:11 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 71) in 309 ms on localhost (2/4)
17/01/13 22:53:11 INFO Executor: Finished task 3.0 in stage 41.0 (TID 74). 1660 bytes result sent to driver
17/01/13 22:53:11 INFO TaskSetManager: Finished task 3.0 in stage 41.0 (TID 74) in 344 ms on localhost (3/4)
17/01/13 22:53:12 INFO Executor: Finished task 2.0 in stage 41.0 (TID 73). 1660 bytes result sent to driver
17/01/13 22:53:12 INFO TaskSetManager: Finished task 2.0 in stage 41.0 (TID 73) in 366 ms on localhost (4/4)
17/01/13 22:53:12 INFO DAGScheduler: ShuffleMapStage 41 (collect at utils.scala:195) finished in 0.367 s
17/01/13 22:53:12 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool 
17/01/13 22:53:12 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:53:12 INFO DAGScheduler: running: Set()
17/01/13 22:53:12 INFO DAGScheduler: waiting: Set(ResultStage 42)
17/01/13 22:53:12 INFO DAGScheduler: failed: Set()
17/01/13 22:53:12 INFO DAGScheduler: Submitting ResultStage 42 (MapPartitionsRDD[175] at collect at utils.scala:195), which has no missing parents
17/01/13 22:53:12 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 15.6 KB, free 29.0 MB)
17/01/13 22:53:12 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 7.2 KB, free 29.0 MB)
17/01/13 22:53:12 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on localhost:63357 (size: 7.2 KB, free: 483.1 MB)
17/01/13 22:53:12 INFO SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1006
17/01/13 22:53:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (MapPartitionsRDD[175] at collect at utils.scala:195)
17/01/13 22:53:12 INFO TaskSchedulerImpl: Adding task set 42.0 with 1 tasks
17/01/13 22:53:12 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 75, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 22:53:12 INFO Executor: Running task 0.0 in stage 42.0 (TID 75)
17/01/13 22:53:12 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/01/13 22:53:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:53:12 INFO Executor: Finished task 0.0 in stage 42.0 (TID 75). 2884 bytes result sent to driver
17/01/13 22:53:12 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 75) in 28 ms on localhost (1/1)
17/01/13 22:53:12 INFO DAGScheduler: ResultStage 42 (collect at utils.scala:195) finished in 0.028 s
17/01/13 22:53:12 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool 
17/01/13 22:53:12 INFO DAGScheduler: Job 25 finished: collect at utils.scala:195, took 0.609966 s
17/01/13 22:53:12 INFO ParseDriver: Parsing command: SELECT * FROM iris WHERE Species = 'virginica' ORDER BY Sepal.Length LIMIT 10;
17/01/13 22:53:13 INFO SparkContext: Invoking stop() from shutdown hook
17/01/13 22:53:13 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
17/01/13 22:53:13 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/01/13 22:53:13 INFO MemoryStore: MemoryStore cleared
17/01/13 22:53:13 INFO BlockManager: BlockManager stopped
17/01/13 22:53:13 INFO BlockManagerMaster: BlockManagerMaster stopped
17/01/13 22:53:13 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/01/13 22:53:13 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\Baoco\AppData\Local\Temp\spark-10b89bdd-6833-43c0-95ce-3710579b0b34\userFiles-23993f52-85b5-43a0-bb2f-45bfe38dc904
java.io.IOException: Failed to delete: C:\Users\Baoco\AppData\Local\Temp\spark-10b89bdd-6833-43c0-95ce-3710579b0b34\userFiles-23993f52-85b5-43a0-bb2f-45bfe38dc904
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:119)
	at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1219)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1755)
	at org.apache.spark.SparkContext$$anonfun$3.apply$mcV$sp(SparkContext.scala:596)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/01/13 22:53:13 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
17/01/13 22:53:13 INFO SparkContext: Successfully stopped SparkContext
17/01/13 22:53:13 INFO ShutdownHookManager: Shutdown hook called
17/01/13 22:53:13 INFO ShutdownHookManager: Deleting directory C:\Users\Baoco\AppData\Local\Temp\spark-10b89bdd-6833-43c0-95ce-3710579b0b34\httpd-0b8099eb-234a-41df-9041-20aaeb1ec029
17/01/13 22:53:13 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
17/01/13 22:53:13 INFO ShutdownHookManager: Deleting directory C:\Users\Baoco\AppData\Local\Temp\spark-10b89bdd-6833-43c0-95ce-3710579b0b34
17/01/13 22:53:13 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\Baoco\AppData\Local\Temp\spark-10b89bdd-6833-43c0-95ce-3710579b0b34
java.io.IOException: Failed to delete: C:\Users\Baoco\AppData\Local\Temp\spark-10b89bdd-6833-43c0-95ce-3710579b0b34
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/01/13 22:53:13 INFO ShutdownHookManager: Deleting directory C:\Users\Baoco\AppData\Local\Temp\spark-10b89bdd-6833-43c0-95ce-3710579b0b34\userFiles-23993f52-85b5-43a0-bb2f-45bfe38dc904
17/01/13 22:53:13 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\Baoco\AppData\Local\Temp\spark-10b89bdd-6833-43c0-95ce-3710579b0b34\userFiles-23993f52-85b5-43a0-bb2f-45bfe38dc904
java.io.IOException: Failed to delete: C:\Users\Baoco\AppData\Local\Temp\spark-10b89bdd-6833-43c0-95ce-3710579b0b34\userFiles-23993f52-85b5-43a0-bb2f-45bfe38dc904
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/01/13 22:53:13 INFO ShutdownHookManager: Deleting directory C:\Users\Baoco\AppData\Local\Temp\spark-2e9dfe8d-912a-454c-b7f5-5c82dc485bc2
17/01/13 22:53:13 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
17/01/13 22:53:13 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\Baoco\AppData\Local\Temp\spark-2e9dfe8d-912a-454c-b7f5-5c82dc485bc2
java.io.IOException: Failed to delete: C:\Users\Baoco\AppData\Local\Temp\spark-2e9dfe8d-912a-454c-b7f5-5c82dc485bc2
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/01/13 22:56:57 INFO SparkContext: Running Spark version 1.6.2
17/01/13 22:56:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/01/13 22:56:58 INFO SecurityManager: Changing view acls to: Baoco
17/01/13 22:56:58 INFO SecurityManager: Changing modify acls to: Baoco
17/01/13 22:56:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(Baoco); users with modify permissions: Set(Baoco)
17/01/13 22:56:58 INFO Utils: Successfully started service 'sparkDriver' on port 63413.
17/01/13 22:56:58 INFO Slf4jLogger: Slf4jLogger started
17/01/13 22:56:58 INFO Remoting: Starting remoting
17/01/13 22:56:59 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@127.0.0.1:63426]
17/01/13 22:56:59 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 63426.
17/01/13 22:56:59 INFO SparkEnv: Registering MapOutputTracker
17/01/13 22:56:59 INFO SparkEnv: Registering BlockManagerMaster
17/01/13 22:56:59 INFO DiskBlockManager: Created local directory at C:\Users\Baoco\AppData\Local\Temp\blockmgr-2abc26c7-8672-4a59-9590-29f2833fb243
17/01/13 22:56:59 INFO MemoryStore: MemoryStore started with capacity 511.1 MB
17/01/13 22:56:59 INFO SparkEnv: Registering OutputCommitCoordinator
17/01/13 22:56:59 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/01/13 22:56:59 INFO SparkUI: Started SparkUI at http://127.0.0.1:4040
17/01/13 22:56:59 INFO HttpFileServer: HTTP File server directory is C:\Users\Baoco\AppData\Local\Temp\spark-124892fd-52e1-4df7-9712-e85eb2df212d\httpd-1aa40b97-6cf8-4abf-9d8c-97ec1cf43c00
17/01/13 22:56:59 INFO HttpServer: Starting HTTP Server
17/01/13 22:56:59 INFO Utils: Successfully started service 'HTTP file server' on port 63429.
17/01/13 22:56:59 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/spark-csv_2.11-1.3.0.jar at http://127.0.0.1:63429/jars/spark-csv_2.11-1.3.0.jar with timestamp 1484377019486
17/01/13 22:56:59 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/commons-csv-1.1.jar at http://127.0.0.1:63429/jars/commons-csv-1.1.jar with timestamp 1484377019489
17/01/13 22:56:59 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/univocity-parsers-1.5.1.jar at http://127.0.0.1:63429/jars/univocity-parsers-1.5.1.jar with timestamp 1484377019491
17/01/13 22:56:59 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/sparklyr-1.6-2.10.jar at http://127.0.0.1:63429/jars/sparklyr-1.6-2.10.jar with timestamp 1484377019493
17/01/13 22:56:59 INFO Executor: Starting executor ID driver on host localhost
17/01/13 22:56:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63446.
17/01/13 22:56:59 INFO NettyBlockTransferService: Server created on 63446
17/01/13 22:56:59 INFO BlockManagerMaster: Trying to register BlockManager
17/01/13 22:56:59 INFO BlockManagerMasterEndpoint: Registering block manager localhost:63446 with 511.1 MB RAM, BlockManagerId(driver, localhost, 63446)
17/01/13 22:56:59 INFO BlockManagerMaster: Registered BlockManager
17/01/13 22:57:00 INFO HiveContext: Initializing execution hive, version 1.2.1
17/01/13 22:57:00 INFO ClientWrapper: Inspected Hadoop version: 2.6.0
17/01/13 22:57:00 INFO ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
17/01/13 22:57:00 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/01/13 22:57:00 INFO ObjectStore: ObjectStore, initialize called
17/01/13 22:57:01 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/01/13 22:57:01 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/01/13 22:57:01 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/01/13 22:57:01 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/01/13 22:57:08 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/01/13 22:57:10 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 22:57:10 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 22:57:16 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 22:57:16 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 22:57:17 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/01/13 22:57:17 INFO ObjectStore: Initialized ObjectStore
17/01/13 22:57:18 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/01/13 22:57:18 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/01/13 22:57:24 INFO HiveMetaStore: Added admin role in metastore
17/01/13 22:57:24 INFO HiveMetaStore: Added public role in metastore
17/01/13 22:57:24 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/01/13 22:57:24 INFO HiveMetaStore: 0: get_all_databases
17/01/13 22:57:24 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_all_databases	
17/01/13 22:57:24 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/01/13 22:57:24 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/01/13 22:57:24 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 22:57:26 INFO SessionState: Created local directory: C:/Users/Baoco/AppData/Local/Temp/790afe8b-0ed0-46f6-b653-0d761f672ab8_resources
17/01/13 22:57:26 INFO SessionState: Created HDFS directory: C:/Users/Baoco/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/Baoco/790afe8b-0ed0-46f6-b653-0d761f672ab8
17/01/13 22:57:26 INFO SessionState: Created local directory: C:/Users/Baoco/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/790afe8b-0ed0-46f6-b653-0d761f672ab8
17/01/13 22:57:26 INFO SessionState: Created HDFS directory: C:/Users/Baoco/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/Baoco/790afe8b-0ed0-46f6-b653-0d761f672ab8/_tmp_space.db
17/01/13 22:57:26 INFO HiveContext: default warehouse location is C:\Users\Baoco\AppData\Local\rstudio\spark\Cache\spark-1.6.2-bin-hadoop2.6\tmp\hive
17/01/13 22:57:26 INFO HiveContext: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/01/13 22:57:26 INFO ClientWrapper: Inspected Hadoop version: 2.6.0
17/01/13 22:57:26 INFO ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
17/01/13 22:57:27 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/01/13 22:57:27 INFO ObjectStore: ObjectStore, initialize called
17/01/13 22:57:27 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/01/13 22:57:27 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/01/13 22:57:27 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/01/13 22:57:27 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/01/13 22:57:28 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/01/13 22:57:29 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 22:57:29 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 22:57:30 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 22:57:30 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 22:57:30 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/01/13 22:57:30 INFO ObjectStore: Initialized ObjectStore
17/01/13 22:57:30 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/01/13 22:57:30 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/01/13 22:57:30 INFO HiveMetaStore: Added admin role in metastore
17/01/13 22:57:30 INFO HiveMetaStore: Added public role in metastore
17/01/13 22:57:30 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/01/13 22:57:30 INFO HiveMetaStore: 0: get_all_databases
17/01/13 22:57:30 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_all_databases	
17/01/13 22:57:30 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/01/13 22:57:30 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/01/13 22:57:30 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 22:57:30 INFO SessionState: Created local directory: C:/Users/Baoco/AppData/Local/Temp/0ba146f3-de70-4cc7-89f9-3df99d143285_resources
17/01/13 22:57:30 INFO SessionState: Created HDFS directory: C:/Users/Baoco/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/Baoco/0ba146f3-de70-4cc7-89f9-3df99d143285
17/01/13 22:57:30 INFO SessionState: Created local directory: C:/Users/Baoco/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/0ba146f3-de70-4cc7-89f9-3df99d143285
17/01/13 22:57:31 INFO SessionState: Created HDFS directory: C:/Users/Baoco/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/Baoco/0ba146f3-de70-4cc7-89f9-3df99d143285/_tmp_space.db
17/01/13 22:57:41 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/01/13 22:57:41 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/01/13 22:57:42 INFO SparkContext: Starting job: collect at utils.scala:59
17/01/13 22:57:42 INFO DAGScheduler: Got job 0 (collect at utils.scala:59) with 1 output partitions
17/01/13 22:57:42 INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:59)
17/01/13 22:57:42 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:57:42 INFO DAGScheduler: Missing parents: List()
17/01/13 22:57:42 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at map at utils.scala:56), which has no missing parents
17/01/13 22:57:42 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 5.4 KB, free 5.4 KB)
17/01/13 22:57:42 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.0 KB, free 8.4 KB)
17/01/13 22:57:42 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:63446 (size: 3.0 KB, free: 511.1 MB)
17/01/13 22:57:42 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
17/01/13 22:57:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at map at utils.scala:56)
17/01/13 22:57:42 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
17/01/13 22:57:42 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2348 bytes)
17/01/13 22:57:42 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/01/13 22:57:42 INFO Executor: Fetching http://127.0.0.1:63429/jars/commons-csv-1.1.jar with timestamp 1484377019489
17/01/13 22:57:42 INFO Utils: Fetching http://127.0.0.1:63429/jars/commons-csv-1.1.jar to C:\Users\Baoco\AppData\Local\Temp\spark-124892fd-52e1-4df7-9712-e85eb2df212d\userFiles-241599d2-58eb-4122-9c12-6b033c2d3a34\fetchFileTemp6265406371462227369.tmp
17/01/13 22:57:42 INFO Executor: Adding file:/C:/Users/Baoco/AppData/Local/Temp/spark-124892fd-52e1-4df7-9712-e85eb2df212d/userFiles-241599d2-58eb-4122-9c12-6b033c2d3a34/commons-csv-1.1.jar to class loader
17/01/13 22:57:42 INFO Executor: Fetching http://127.0.0.1:63429/jars/univocity-parsers-1.5.1.jar with timestamp 1484377019491
17/01/13 22:57:42 INFO Utils: Fetching http://127.0.0.1:63429/jars/univocity-parsers-1.5.1.jar to C:\Users\Baoco\AppData\Local\Temp\spark-124892fd-52e1-4df7-9712-e85eb2df212d\userFiles-241599d2-58eb-4122-9c12-6b033c2d3a34\fetchFileTemp8061733033352494098.tmp
17/01/13 22:57:42 INFO Executor: Adding file:/C:/Users/Baoco/AppData/Local/Temp/spark-124892fd-52e1-4df7-9712-e85eb2df212d/userFiles-241599d2-58eb-4122-9c12-6b033c2d3a34/univocity-parsers-1.5.1.jar to class loader
17/01/13 22:57:42 INFO Executor: Fetching http://127.0.0.1:63429/jars/spark-csv_2.11-1.3.0.jar with timestamp 1484377019486
17/01/13 22:57:42 INFO Utils: Fetching http://127.0.0.1:63429/jars/spark-csv_2.11-1.3.0.jar to C:\Users\Baoco\AppData\Local\Temp\spark-124892fd-52e1-4df7-9712-e85eb2df212d\userFiles-241599d2-58eb-4122-9c12-6b033c2d3a34\fetchFileTemp5994471423305286316.tmp
17/01/13 22:57:42 INFO Executor: Adding file:/C:/Users/Baoco/AppData/Local/Temp/spark-124892fd-52e1-4df7-9712-e85eb2df212d/userFiles-241599d2-58eb-4122-9c12-6b033c2d3a34/spark-csv_2.11-1.3.0.jar to class loader
17/01/13 22:57:42 INFO Executor: Fetching http://127.0.0.1:63429/jars/sparklyr-1.6-2.10.jar with timestamp 1484377019493
17/01/13 22:57:42 INFO Utils: Fetching http://127.0.0.1:63429/jars/sparklyr-1.6-2.10.jar to C:\Users\Baoco\AppData\Local\Temp\spark-124892fd-52e1-4df7-9712-e85eb2df212d\userFiles-241599d2-58eb-4122-9c12-6b033c2d3a34\fetchFileTemp5890083487001810722.tmp
17/01/13 22:57:43 INFO Executor: Adding file:/C:/Users/Baoco/AppData/Local/Temp/spark-124892fd-52e1-4df7-9712-e85eb2df212d/userFiles-241599d2-58eb-4122-9c12-6b033c2d3a34/sparklyr-1.6-2.10.jar to class loader
17/01/13 22:57:43 INFO GenerateUnsafeProjection: Code generated in 186.809014 ms
17/01/13 22:57:43 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1060 bytes result sent to driver
17/01/13 22:57:43 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 914 ms on localhost (1/1)
17/01/13 22:57:43 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/01/13 22:57:43 INFO DAGScheduler: ResultStage 0 (collect at utils.scala:59) finished in 0.929 s
17/01/13 22:57:43 INFO DAGScheduler: Job 0 finished: collect at utils.scala:59, took 1.299221 s
17/01/13 22:57:43 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 61.8 KB, free 70.2 KB)
17/01/13 22:57:43 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.3 KB, free 89.5 KB)
17/01/13 22:57:43 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:63446 (size: 19.3 KB, free: 511.1 MB)
17/01/13 22:57:43 INFO SparkContext: Created broadcast 1 from textFile at TextFile.scala:30
17/01/13 22:57:43 INFO FileInputFormat: Total input paths to process : 1
17/01/13 22:57:43 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/01/13 22:57:43 INFO DAGScheduler: Got job 1 (take at CsvRelation.scala:249) with 1 output partitions
17/01/13 22:57:43 INFO DAGScheduler: Final stage: ResultStage 1 (take at CsvRelation.scala:249)
17/01/13 22:57:43 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:57:43 INFO DAGScheduler: Missing parents: List()
17/01/13 22:57:43 INFO DAGScheduler: Submitting ResultStage 1 (C:\Users\Baoco\AppData\Local\Temp\RtmpwBbCGg/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv MapPartitionsRDD[5] at textFile at TextFile.scala:30), which has no missing parents
17/01/13 22:57:43 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 3.2 KB, free 92.7 KB)
17/01/13 22:57:43 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 1945.0 B, free 94.6 KB)
17/01/13 22:57:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:63446 (size: 1945.0 B, free: 511.1 MB)
17/01/13 22:57:43 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
17/01/13 22:57:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (C:\Users\Baoco\AppData\Local\Temp\RtmpwBbCGg/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv MapPartitionsRDD[5] at textFile at TextFile.scala:30)
17/01/13 22:57:43 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
17/01/13 22:57:43 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 22:57:43 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
17/01/13 22:57:43 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpwBbCGg/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv:0+2088
17/01/13 22:57:43 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
17/01/13 22:57:43 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
17/01/13 22:57:43 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
17/01/13 22:57:43 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
17/01/13 22:57:43 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
17/01/13 22:57:43 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2354 bytes result sent to driver
17/01/13 22:57:43 INFO DAGScheduler: ResultStage 1 (take at CsvRelation.scala:249) finished in 0.058 s
17/01/13 22:57:43 INFO DAGScheduler: Job 1 finished: take at CsvRelation.scala:249, took 0.069793 s
17/01/13 22:57:43 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 58 ms on localhost (1/1)
17/01/13 22:57:43 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/01/13 22:57:43 INFO ParseDriver: Parsing command: SELECT * FROM  `iris`
17/01/13 22:57:44 INFO ParseDriver: Parse Completed
17/01/13 22:57:44 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 208.5 KB, free 303.1 KB)
17/01/13 22:57:44 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 19.3 KB, free 322.5 KB)
17/01/13 22:57:44 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:63446 (size: 19.3 KB, free: 511.1 MB)
17/01/13 22:57:44 INFO SparkContext: Created broadcast 3 from textFile at TextFile.scala:30
17/01/13 22:57:44 INFO FileInputFormat: Total input paths to process : 1
17/01/13 22:57:44 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/01/13 22:57:44 INFO DAGScheduler: Got job 2 (take at CsvRelation.scala:249) with 1 output partitions
17/01/13 22:57:44 INFO DAGScheduler: Final stage: ResultStage 2 (take at CsvRelation.scala:249)
17/01/13 22:57:44 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:57:44 INFO DAGScheduler: Missing parents: List()
17/01/13 22:57:44 INFO DAGScheduler: Submitting ResultStage 2 (C:\Users\Baoco\AppData\Local\Temp\RtmpwBbCGg/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv MapPartitionsRDD[7] at textFile at TextFile.scala:30), which has no missing parents
17/01/13 22:57:44 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 3.2 KB, free 325.6 KB)
17/01/13 22:57:44 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 1946.0 B, free 327.5 KB)
17/01/13 22:57:44 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:63446 (size: 1946.0 B, free: 511.1 MB)
17/01/13 22:57:44 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1006
17/01/13 22:57:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (C:\Users\Baoco\AppData\Local\Temp\RtmpwBbCGg/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv MapPartitionsRDD[7] at textFile at TextFile.scala:30)
17/01/13 22:57:44 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
17/01/13 22:57:44 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 22:57:44 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
17/01/13 22:57:44 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpwBbCGg/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv:0+2088
17/01/13 22:57:44 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2354 bytes result sent to driver
17/01/13 22:57:44 INFO DAGScheduler: ResultStage 2 (take at CsvRelation.scala:249) finished in 0.013 s
17/01/13 22:57:44 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 13 ms on localhost (1/1)
17/01/13 22:57:44 INFO DAGScheduler: Job 2 finished: take at CsvRelation.scala:249, took 0.021766 s
17/01/13 22:57:44 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
17/01/13 22:57:44 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 208.5 KB, free 536.1 KB)
17/01/13 22:57:44 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 19.3 KB, free 555.4 KB)
17/01/13 22:57:44 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:63446 (size: 19.3 KB, free: 511.1 MB)
17/01/13 22:57:44 INFO SparkContext: Created broadcast 5 from textFile at TextFile.scala:30
17/01/13 22:57:44 INFO FileInputFormat: Total input paths to process : 1
17/01/13 22:57:44 INFO SparkContext: Starting job: sql at null:-2
17/01/13 22:57:44 INFO DAGScheduler: Registering RDD 17 (sql at null:-2)
17/01/13 22:57:44 INFO DAGScheduler: Got job 3 (sql at null:-2) with 1 output partitions
17/01/13 22:57:44 INFO DAGScheduler: Final stage: ResultStage 4 (sql at null:-2)
17/01/13 22:57:44 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
17/01/13 22:57:44 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
17/01/13 22:57:44 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[17] at sql at null:-2), which has no missing parents
17/01/13 22:57:44 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 18.3 KB, free 573.7 KB)
17/01/13 22:57:44 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 8.7 KB, free 582.3 KB)
17/01/13 22:57:44 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:63446 (size: 8.7 KB, free: 511.1 MB)
17/01/13 22:57:44 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1006
17/01/13 22:57:44 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[17] at sql at null:-2)
17/01/13 22:57:44 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks
17/01/13 22:57:44 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:57:44 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 4, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:57:44 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
17/01/13 22:57:44 INFO Executor: Running task 1.0 in stage 3.0 (TID 4)
17/01/13 22:57:44 INFO CacheManager: Partition rdd_14_1 not found, computing it
17/01/13 22:57:44 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpwBbCGg/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv:2088+2089
17/01/13 22:57:44 INFO CacheManager: Partition rdd_14_0 not found, computing it
17/01/13 22:57:44 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpwBbCGg/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv:0+2088
17/01/13 22:57:44 INFO GenerateUnsafeProjection: Code generated in 17.198066 ms
17/01/13 22:57:44 INFO MemoryStore: Block rdd_14_1 stored as values in memory (estimated size 3.1 KB, free 585.5 KB)
17/01/13 22:57:44 INFO BlockManagerInfo: Added rdd_14_1 in memory on localhost:63446 (size: 3.1 KB, free: 511.1 MB)
17/01/13 22:57:44 INFO MemoryStore: Block rdd_14_0 stored as values in memory (estimated size 3.2 KB, free 588.7 KB)
17/01/13 22:57:44 INFO BlockManagerInfo: Added rdd_14_0 in memory on localhost:63446 (size: 3.2 KB, free: 511.0 MB)
17/01/13 22:57:44 INFO GeneratePredicate: Code generated in 7.010128 ms
17/01/13 22:57:44 INFO GenerateColumnAccessor: Code generated in 16.457373 ms
17/01/13 22:57:44 INFO GenerateMutableProjection: Code generated in 4.944636 ms
17/01/13 22:57:44 INFO GenerateUnsafeProjection: Code generated in 6.891088 ms
17/01/13 22:57:44 INFO GenerateMutableProjection: Code generated in 14.746868 ms
17/01/13 22:57:44 INFO GenerateUnsafeRowJoiner: Code generated in 7.159033 ms
17/01/13 22:57:44 INFO GenerateUnsafeProjection: Code generated in 6.767355 ms
17/01/13 22:57:45 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3784 bytes result sent to driver
17/01/13 22:57:45 INFO Executor: Finished task 1.0 in stage 3.0 (TID 4). 3787 bytes result sent to driver
17/01/13 22:57:45 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 4) in 326 ms on localhost (1/2)
17/01/13 22:57:45 INFO DAGScheduler: ShuffleMapStage 3 (sql at null:-2) finished in 0.336 s
17/01/13 22:57:45 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:57:45 INFO DAGScheduler: running: Set()
17/01/13 22:57:45 INFO DAGScheduler: waiting: Set(ResultStage 4)
17/01/13 22:57:45 INFO DAGScheduler: failed: Set()
17/01/13 22:57:45 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 335 ms on localhost (2/2)
17/01/13 22:57:45 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
17/01/13 22:57:45 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[20] at sql at null:-2), which has no missing parents
17/01/13 22:57:45 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 9.3 KB, free 597.9 KB)
17/01/13 22:57:45 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 4.6 KB, free 602.6 KB)
17/01/13 22:57:45 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:63446 (size: 4.6 KB, free: 511.0 MB)
17/01/13 22:57:45 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1006
17/01/13 22:57:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[20] at sql at null:-2)
17/01/13 22:57:45 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
17/01/13 22:57:45 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 5, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 22:57:45 INFO Executor: Running task 0.0 in stage 4.0 (TID 5)
17/01/13 22:57:45 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:57:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
17/01/13 22:57:45 INFO GenerateMutableProjection: Code generated in 8.523513 ms
17/01/13 22:57:45 INFO GenerateMutableProjection: Code generated in 7.171407 ms
17/01/13 22:57:45 INFO Executor: Finished task 0.0 in stage 4.0 (TID 5). 1830 bytes result sent to driver
17/01/13 22:57:45 INFO DAGScheduler: ResultStage 4 (sql at null:-2) finished in 0.112 s
17/01/13 22:57:45 INFO DAGScheduler: Job 3 finished: sql at null:-2, took 0.489383 s
17/01/13 22:57:45 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 5) in 112 ms on localhost (1/1)
17/01/13 22:57:45 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
17/01/13 22:57:45 INFO ParseDriver: Parsing command: SELECT count(*) FROM  `iris`
17/01/13 22:57:45 INFO ParseDriver: Parse Completed
17/01/13 22:57:45 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 22:57:45 INFO DAGScheduler: Registering RDD 24 (collect at utils.scala:195)
17/01/13 22:57:45 INFO DAGScheduler: Got job 4 (collect at utils.scala:195) with 1 output partitions
17/01/13 22:57:45 INFO DAGScheduler: Final stage: ResultStage 6 (collect at utils.scala:195)
17/01/13 22:57:45 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
17/01/13 22:57:45 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 5)
17/01/13 22:57:45 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[24] at collect at utils.scala:195), which has no missing parents
17/01/13 22:57:45 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 18.4 KB, free 620.9 KB)
17/01/13 22:57:45 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.7 KB, free 629.6 KB)
17/01/13 22:57:45 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:63446 (size: 8.7 KB, free: 511.0 MB)
17/01/13 22:57:45 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1006
17/01/13 22:57:45 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[24] at collect at utils.scala:195)
17/01/13 22:57:45 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks
17/01/13 22:57:45 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:57:45 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 7, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:57:45 INFO Executor: Running task 0.0 in stage 5.0 (TID 6)
17/01/13 22:57:45 INFO Executor: Running task 1.0 in stage 5.0 (TID 7)
17/01/13 22:57:45 INFO BlockManager: Found block rdd_14_0 locally
17/01/13 22:57:45 INFO BlockManager: Found block rdd_14_1 locally
17/01/13 22:57:45 INFO Executor: Finished task 0.0 in stage 5.0 (TID 6). 2679 bytes result sent to driver
17/01/13 22:57:45 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 6) in 40 ms on localhost (1/2)
17/01/13 22:57:45 INFO Executor: Finished task 1.0 in stage 5.0 (TID 7). 2679 bytes result sent to driver
17/01/13 22:57:45 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 7) in 41 ms on localhost (2/2)
17/01/13 22:57:45 INFO DAGScheduler: ShuffleMapStage 5 (collect at utils.scala:195) finished in 0.042 s
17/01/13 22:57:45 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:57:45 INFO DAGScheduler: running: Set()
17/01/13 22:57:45 INFO DAGScheduler: waiting: Set(ResultStage 6)
17/01/13 22:57:45 INFO DAGScheduler: failed: Set()
17/01/13 22:57:45 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
17/01/13 22:57:45 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[27] at collect at utils.scala:195), which has no missing parents
17/01/13 22:57:45 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 9.4 KB, free 639.0 KB)
17/01/13 22:57:45 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.6 KB, free 643.6 KB)
17/01/13 22:57:45 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:63446 (size: 4.6 KB, free: 511.0 MB)
17/01/13 22:57:45 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1006
17/01/13 22:57:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[27] at collect at utils.scala:195)
17/01/13 22:57:45 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
17/01/13 22:57:45 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 22:57:45 INFO Executor: Running task 0.0 in stage 6.0 (TID 8)
17/01/13 22:57:45 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:57:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/01/13 22:57:45 INFO Executor: Finished task 0.0 in stage 6.0 (TID 8). 1830 bytes result sent to driver
17/01/13 22:57:45 INFO DAGScheduler: ResultStage 6 (collect at utils.scala:195) finished in 0.013 s
17/01/13 22:57:45 INFO DAGScheduler: Job 4 finished: collect at utils.scala:195, took 0.074313 s
17/01/13 22:57:45 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 12 ms on localhost (1/1)
17/01/13 22:57:45 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
17/01/13 22:57:45 INFO ParseDriver: Parsing command: SELECT *
FROM `iris` AS `zzz1`
WHERE (0 = 1)
17/01/13 22:57:45 INFO ParseDriver: Parse Completed
17/01/13 22:57:45 INFO ContextCleaner: Cleaned accumulator 10
17/01/13 22:57:45 INFO ContextCleaner: Cleaned accumulator 9
17/01/13 22:57:45 INFO ContextCleaner: Cleaned accumulator 8
17/01/13 22:57:45 INFO ContextCleaner: Cleaned accumulator 7
17/01/13 22:57:45 INFO BlockManagerInfo: Removed broadcast_4_piece0 on localhost:63446 in memory (size: 1946.0 B, free: 511.0 MB)
17/01/13 22:57:45 INFO ContextCleaner: Cleaned accumulator 5
17/01/13 22:57:45 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:63446 in memory (size: 19.3 KB, free: 511.1 MB)
17/01/13 22:57:45 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:63446 in memory (size: 1945.0 B, free: 511.1 MB)
17/01/13 22:57:45 INFO ContextCleaner: Cleaned accumulator 4
17/01/13 22:57:45 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:63446 in memory (size: 19.3 KB, free: 511.1 MB)
17/01/13 22:57:45 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:63446 in memory (size: 3.0 KB, free: 511.1 MB)
17/01/13 22:57:45 INFO ContextCleaner: Cleaned accumulator 3
17/01/13 22:57:45 INFO ContextCleaner: Cleaned accumulator 2
17/01/13 22:57:45 INFO BlockManagerInfo: Removed broadcast_9_piece0 on localhost:63446 in memory (size: 4.6 KB, free: 511.1 MB)
17/01/13 22:57:45 INFO ContextCleaner: Cleaned accumulator 26
17/01/13 22:57:45 INFO BlockManagerInfo: Removed broadcast_8_piece0 on localhost:63446 in memory (size: 8.7 KB, free: 511.1 MB)
17/01/13 22:57:45 INFO ContextCleaner: Cleaned accumulator 25
17/01/13 22:57:45 INFO ContextCleaner: Cleaned shuffle 1
17/01/13 22:57:45 INFO BlockManagerInfo: Removed broadcast_7_piece0 on localhost:63446 in memory (size: 4.6 KB, free: 511.1 MB)
17/01/13 22:57:45 INFO ContextCleaner: Cleaned accumulator 16
17/01/13 22:57:45 INFO BlockManagerInfo: Removed broadcast_6_piece0 on localhost:63446 in memory (size: 8.7 KB, free: 511.1 MB)
17/01/13 22:57:45 INFO ContextCleaner: Cleaned accumulator 15
17/01/13 22:57:45 INFO ContextCleaner: Cleaned shuffle 0
17/01/13 22:57:45 INFO ContextCleaner: Cleaned accumulator 14
17/01/13 22:57:45 INFO ContextCleaner: Cleaned accumulator 13
17/01/13 22:57:45 INFO ContextCleaner: Cleaned accumulator 12
17/01/13 22:57:45 INFO ContextCleaner: Cleaned accumulator 11
17/01/13 22:57:46 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/01/13 22:57:46 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/01/13 22:57:46 INFO SparkContext: Starting job: collect at utils.scala:59
17/01/13 22:57:46 INFO DAGScheduler: Got job 5 (collect at utils.scala:59) with 1 output partitions
17/01/13 22:57:46 INFO DAGScheduler: Final stage: ResultStage 7 (collect at utils.scala:59)
17/01/13 22:57:46 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:57:46 INFO DAGScheduler: Missing parents: List()
17/01/13 22:57:46 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[31] at map at utils.scala:56), which has no missing parents
17/01/13 22:57:46 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.4 KB, free 239.6 KB)
17/01/13 22:57:46 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.0 KB, free 242.6 KB)
17/01/13 22:57:46 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:63446 (size: 3.0 KB, free: 511.1 MB)
17/01/13 22:57:46 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1006
17/01/13 22:57:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[31] at map at utils.scala:56)
17/01/13 22:57:46 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks
17/01/13 22:57:46 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 9, localhost, partition 0,PROCESS_LOCAL, 2646 bytes)
17/01/13 22:57:46 INFO Executor: Running task 0.0 in stage 7.0 (TID 9)
17/01/13 22:57:46 INFO Executor: Finished task 0.0 in stage 7.0 (TID 9). 1067 bytes result sent to driver
17/01/13 22:57:46 INFO DAGScheduler: ResultStage 7 (collect at utils.scala:59) finished in 0.009 s
17/01/13 22:57:46 INFO DAGScheduler: Job 5 finished: collect at utils.scala:59, took 0.015806 s
17/01/13 22:57:46 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 9) in 9 ms on localhost (1/1)
17/01/13 22:57:46 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
17/01/13 22:57:56 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 208.5 KB, free 451.1 KB)
17/01/13 22:57:56 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 19.3 KB, free 470.4 KB)
17/01/13 22:57:56 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:63446 (size: 19.3 KB, free: 511.1 MB)
17/01/13 22:57:56 INFO SparkContext: Created broadcast 11 from textFile at TextFile.scala:30
17/01/13 22:57:56 INFO FileInputFormat: Total input paths to process : 1
17/01/13 22:57:56 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/01/13 22:57:56 INFO DAGScheduler: Got job 6 (take at CsvRelation.scala:249) with 1 output partitions
17/01/13 22:57:56 INFO DAGScheduler: Final stage: ResultStage 8 (take at CsvRelation.scala:249)
17/01/13 22:57:56 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:57:56 INFO DAGScheduler: Missing parents: List()
17/01/13 22:57:56 INFO DAGScheduler: Submitting ResultStage 8 (C:\Users\Baoco\AppData\Local\Temp\RtmpwBbCGg/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv MapPartitionsRDD[33] at textFile at TextFile.scala:30), which has no missing parents
17/01/13 22:57:56 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 3.2 KB, free 473.6 KB)
17/01/13 22:57:56 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 1944.0 B, free 475.5 KB)
17/01/13 22:57:56 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:63446 (size: 1944.0 B, free: 511.1 MB)
17/01/13 22:57:56 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1006
17/01/13 22:57:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (C:\Users\Baoco\AppData\Local\Temp\RtmpwBbCGg/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv MapPartitionsRDD[33] at textFile at TextFile.scala:30)
17/01/13 22:57:56 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks
17/01/13 22:57:56 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 10, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 22:57:56 INFO Executor: Running task 0.0 in stage 8.0 (TID 10)
17/01/13 22:57:56 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpwBbCGg/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv:0+16824941
17/01/13 22:57:56 INFO Executor: Finished task 0.0 in stage 8.0 (TID 10). 3117 bytes result sent to driver
17/01/13 22:57:56 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 10) in 7 ms on localhost (1/1)
17/01/13 22:57:56 INFO DAGScheduler: ResultStage 8 (take at CsvRelation.scala:249) finished in 0.007 s
17/01/13 22:57:56 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
17/01/13 22:57:56 INFO DAGScheduler: Job 6 finished: take at CsvRelation.scala:249, took 0.012627 s
17/01/13 22:57:56 INFO ParseDriver: Parsing command: SELECT * FROM  `flights`
17/01/13 22:57:56 INFO ParseDriver: Parse Completed
17/01/13 22:57:56 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 208.5 KB, free 684.0 KB)
17/01/13 22:57:56 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 19.3 KB, free 703.3 KB)
17/01/13 22:57:56 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on localhost:63446 (size: 19.3 KB, free: 511.1 MB)
17/01/13 22:57:56 INFO SparkContext: Created broadcast 13 from textFile at TextFile.scala:30
17/01/13 22:57:56 INFO FileInputFormat: Total input paths to process : 1
17/01/13 22:57:56 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/01/13 22:57:56 INFO DAGScheduler: Got job 7 (take at CsvRelation.scala:249) with 1 output partitions
17/01/13 22:57:56 INFO DAGScheduler: Final stage: ResultStage 9 (take at CsvRelation.scala:249)
17/01/13 22:57:56 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:57:56 INFO DAGScheduler: Missing parents: List()
17/01/13 22:57:56 INFO DAGScheduler: Submitting ResultStage 9 (C:\Users\Baoco\AppData\Local\Temp\RtmpwBbCGg/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv MapPartitionsRDD[35] at textFile at TextFile.scala:30), which has no missing parents
17/01/13 22:57:56 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 3.2 KB, free 706.5 KB)
17/01/13 22:57:56 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 1944.0 B, free 708.4 KB)
17/01/13 22:57:56 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on localhost:63446 (size: 1944.0 B, free: 511.1 MB)
17/01/13 22:57:56 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1006
17/01/13 22:57:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (C:\Users\Baoco\AppData\Local\Temp\RtmpwBbCGg/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv MapPartitionsRDD[35] at textFile at TextFile.scala:30)
17/01/13 22:57:56 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
17/01/13 22:57:56 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 11, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 22:57:56 INFO Executor: Running task 0.0 in stage 9.0 (TID 11)
17/01/13 22:57:56 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpwBbCGg/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv:0+16824941
17/01/13 22:57:56 INFO Executor: Finished task 0.0 in stage 9.0 (TID 11). 3117 bytes result sent to driver
17/01/13 22:57:56 INFO DAGScheduler: ResultStage 9 (take at CsvRelation.scala:249) finished in 0.012 s
17/01/13 22:57:56 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 11) in 11 ms on localhost (1/1)
17/01/13 22:57:56 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
17/01/13 22:57:56 INFO DAGScheduler: Job 7 finished: take at CsvRelation.scala:249, took 0.017587 s
17/01/13 22:57:56 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 208.5 KB, free 916.9 KB)
17/01/13 22:57:56 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 19.3 KB, free 936.3 KB)
17/01/13 22:57:56 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on localhost:63446 (size: 19.3 KB, free: 511.0 MB)
17/01/13 22:57:56 INFO SparkContext: Created broadcast 15 from textFile at TextFile.scala:30
17/01/13 22:57:56 INFO FileInputFormat: Total input paths to process : 1
17/01/13 22:57:56 INFO SparkContext: Starting job: sql at null:-2
17/01/13 22:57:56 INFO DAGScheduler: Registering RDD 45 (sql at null:-2)
17/01/13 22:57:56 INFO DAGScheduler: Got job 8 (sql at null:-2) with 1 output partitions
17/01/13 22:57:56 INFO DAGScheduler: Final stage: ResultStage 11 (sql at null:-2)
17/01/13 22:57:56 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
17/01/13 22:57:56 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 10)
17/01/13 22:57:56 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[45] at sql at null:-2), which has no missing parents
17/01/13 22:57:56 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 26.8 KB, free 963.1 KB)
17/01/13 22:57:56 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 11.0 KB, free 974.1 KB)
17/01/13 22:57:56 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on localhost:63446 (size: 11.0 KB, free: 511.0 MB)
17/01/13 22:57:56 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1006
17/01/13 22:57:56 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[45] at sql at null:-2)
17/01/13 22:57:56 INFO TaskSchedulerImpl: Adding task set 10.0 with 2 tasks
17/01/13 22:57:56 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 12, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:57:56 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 13, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:57:56 INFO Executor: Running task 0.0 in stage 10.0 (TID 12)
17/01/13 22:57:56 INFO Executor: Running task 1.0 in stage 10.0 (TID 13)
17/01/13 22:57:56 INFO CacheManager: Partition rdd_42_0 not found, computing it
17/01/13 22:57:56 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpwBbCGg/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv:0+16824941
17/01/13 22:57:56 INFO CacheManager: Partition rdd_42_1 not found, computing it
17/01/13 22:57:56 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpwBbCGg/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv:16824941+16824942
17/01/13 22:57:56 INFO GenerateUnsafeProjection: Code generated in 22.250221 ms
17/01/13 22:57:57 INFO BlockManagerInfo: Removed broadcast_14_piece0 on localhost:63446 in memory (size: 1944.0 B, free: 511.0 MB)
17/01/13 22:57:57 INFO ContextCleaner: Cleaned accumulator 30
17/01/13 22:57:57 INFO BlockManagerInfo: Removed broadcast_13_piece0 on localhost:63446 in memory (size: 19.3 KB, free: 511.0 MB)
17/01/13 22:57:57 INFO BlockManagerInfo: Removed broadcast_12_piece0 on localhost:63446 in memory (size: 1944.0 B, free: 511.0 MB)
17/01/13 22:57:57 INFO ContextCleaner: Cleaned accumulator 29
17/01/13 22:57:57 INFO BlockManagerInfo: Removed broadcast_11_piece0 on localhost:63446 in memory (size: 19.3 KB, free: 511.1 MB)
17/01/13 22:57:57 INFO BlockManagerInfo: Removed broadcast_10_piece0 on localhost:63446 in memory (size: 3.0 KB, free: 511.1 MB)
17/01/13 22:57:57 INFO ContextCleaner: Cleaned accumulator 28
17/01/13 22:57:57 INFO ContextCleaner: Cleaned accumulator 27
17/01/13 22:58:01 INFO MemoryStore: Block rdd_42_0 stored as values in memory (estimated size 12.2 MB, free 12.7 MB)
17/01/13 22:58:01 INFO BlockManagerInfo: Added rdd_42_0 in memory on localhost:63446 (size: 12.2 MB, free: 498.9 MB)
17/01/13 22:58:01 INFO Executor: Finished task 0.0 in stage 10.0 (TID 12). 21077 bytes result sent to driver
17/01/13 22:58:01 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 12) in 4350 ms on localhost (1/2)
17/01/13 22:58:01 INFO MemoryStore: Block rdd_42_1 stored as values in memory (estimated size 12.2 MB, free 24.9 MB)
17/01/13 22:58:01 INFO BlockManagerInfo: Added rdd_42_1 in memory on localhost:63446 (size: 12.2 MB, free: 486.7 MB)
17/01/13 22:58:01 INFO Executor: Finished task 1.0 in stage 10.0 (TID 13). 21132 bytes result sent to driver
17/01/13 22:58:01 INFO DAGScheduler: ShuffleMapStage 10 (sql at null:-2) finished in 4.385 s
17/01/13 22:58:01 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 13) in 4385 ms on localhost (2/2)
17/01/13 22:58:01 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:58:01 INFO DAGScheduler: running: Set()
17/01/13 22:58:01 INFO DAGScheduler: waiting: Set(ResultStage 11)
17/01/13 22:58:01 INFO DAGScheduler: failed: Set()
17/01/13 22:58:01 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[48] at sql at null:-2), which has no missing parents
17/01/13 22:58:01 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
17/01/13 22:58:01 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 9.3 KB, free 24.9 MB)
17/01/13 22:58:01 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 4.6 KB, free 24.9 MB)
17/01/13 22:58:01 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on localhost:63446 (size: 4.6 KB, free: 486.7 MB)
17/01/13 22:58:01 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1006
17/01/13 22:58:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[48] at sql at null:-2)
17/01/13 22:58:01 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks
17/01/13 22:58:01 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 14, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 22:58:01 INFO Executor: Running task 0.0 in stage 11.0 (TID 14)
17/01/13 22:58:01 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:58:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:58:01 INFO Executor: Finished task 0.0 in stage 11.0 (TID 14). 1830 bytes result sent to driver
17/01/13 22:58:01 INFO DAGScheduler: ResultStage 11 (sql at null:-2) finished in 0.010 s
17/01/13 22:58:01 INFO DAGScheduler: Job 8 finished: sql at null:-2, took 4.409714 s
17/01/13 22:58:01 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 14) in 10 ms on localhost (1/1)
17/01/13 22:58:01 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
17/01/13 22:58:01 INFO ParseDriver: Parsing command: SELECT count(*) FROM  `flights`
17/01/13 22:58:01 INFO ParseDriver: Parse Completed
17/01/13 22:58:01 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 22:58:01 INFO DAGScheduler: Registering RDD 52 (collect at utils.scala:195)
17/01/13 22:58:01 INFO DAGScheduler: Got job 9 (collect at utils.scala:195) with 1 output partitions
17/01/13 22:58:01 INFO DAGScheduler: Final stage: ResultStage 13 (collect at utils.scala:195)
17/01/13 22:58:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)
17/01/13 22:58:01 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 12)
17/01/13 22:58:01 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[52] at collect at utils.scala:195), which has no missing parents
17/01/13 22:58:01 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 26.9 KB, free 24.9 MB)
17/01/13 22:58:01 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 11.0 KB, free 24.9 MB)
17/01/13 22:58:01 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on localhost:63446 (size: 11.0 KB, free: 486.7 MB)
17/01/13 22:58:01 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1006
17/01/13 22:58:01 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[52] at collect at utils.scala:195)
17/01/13 22:58:01 INFO TaskSchedulerImpl: Adding task set 12.0 with 2 tasks
17/01/13 22:58:01 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 15, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:58:01 INFO TaskSetManager: Starting task 1.0 in stage 12.0 (TID 16, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:58:01 INFO Executor: Running task 0.0 in stage 12.0 (TID 15)
17/01/13 22:58:01 INFO Executor: Running task 1.0 in stage 12.0 (TID 16)
17/01/13 22:58:01 INFO BlockManager: Found block rdd_42_0 locally
17/01/13 22:58:01 INFO BlockManager: Found block rdd_42_1 locally
17/01/13 22:58:01 INFO Executor: Finished task 1.0 in stage 12.0 (TID 16). 2679 bytes result sent to driver
17/01/13 22:58:01 INFO TaskSetManager: Finished task 1.0 in stage 12.0 (TID 16) in 34 ms on localhost (1/2)
17/01/13 22:58:01 INFO Executor: Finished task 0.0 in stage 12.0 (TID 15). 2679 bytes result sent to driver
17/01/13 22:58:01 INFO DAGScheduler: ShuffleMapStage 12 (collect at utils.scala:195) finished in 0.039 s
17/01/13 22:58:01 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:58:01 INFO DAGScheduler: running: Set()
17/01/13 22:58:01 INFO DAGScheduler: waiting: Set(ResultStage 13)
17/01/13 22:58:01 INFO DAGScheduler: failed: Set()
17/01/13 22:58:01 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[55] at collect at utils.scala:195), which has no missing parents
17/01/13 22:58:01 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 9.4 KB, free 24.9 MB)
17/01/13 22:58:01 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 15) in 38 ms on localhost (2/2)
17/01/13 22:58:01 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 4.6 KB, free 24.9 MB)
17/01/13 22:58:01 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
17/01/13 22:58:01 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on localhost:63446 (size: 4.6 KB, free: 486.7 MB)
17/01/13 22:58:01 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1006
17/01/13 22:58:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[55] at collect at utils.scala:195)
17/01/13 22:58:01 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks
17/01/13 22:58:01 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 17, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 22:58:01 INFO Executor: Running task 0.0 in stage 13.0 (TID 17)
17/01/13 22:58:01 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:58:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:58:01 INFO Executor: Finished task 0.0 in stage 13.0 (TID 17). 1830 bytes result sent to driver
17/01/13 22:58:01 INFO DAGScheduler: ResultStage 13 (collect at utils.scala:195) finished in 0.013 s
17/01/13 22:58:01 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 17) in 13 ms on localhost (1/1)
17/01/13 22:58:01 INFO DAGScheduler: Job 9 finished: collect at utils.scala:195, took 0.071266 s
17/01/13 22:58:01 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
17/01/13 22:58:01 INFO ParseDriver: Parsing command: SELECT *
FROM `flights` AS `zzz2`
WHERE (0 = 1)
17/01/13 22:58:01 INFO ParseDriver: Parse Completed
17/01/13 22:58:01 INFO BlockManagerInfo: Removed broadcast_19_piece0 on localhost:63446 in memory (size: 4.6 KB, free: 486.7 MB)
17/01/13 22:58:01 INFO BlockManagerInfo: Removed broadcast_17_piece0 on localhost:63446 in memory (size: 4.6 KB, free: 486.7 MB)
17/01/13 22:58:01 INFO ContextCleaner: Cleaned accumulator 51
17/01/13 22:58:01 INFO BlockManagerInfo: Removed broadcast_18_piece0 on localhost:63446 in memory (size: 11.0 KB, free: 486.7 MB)
17/01/13 22:58:01 INFO ContextCleaner: Cleaned accumulator 50
17/01/13 22:58:01 INFO ContextCleaner: Cleaned shuffle 3
17/01/13 22:58:01 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/01/13 22:58:01 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/01/13 22:58:01 INFO SparkContext: Starting job: collect at utils.scala:59
17/01/13 22:58:01 INFO DAGScheduler: Got job 10 (collect at utils.scala:59) with 1 output partitions
17/01/13 22:58:01 INFO DAGScheduler: Final stage: ResultStage 14 (collect at utils.scala:59)
17/01/13 22:58:01 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:58:01 INFO DAGScheduler: Missing parents: List()
17/01/13 22:58:01 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[59] at map at utils.scala:56), which has no missing parents
17/01/13 22:58:01 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 5.4 KB, free 24.9 MB)
17/01/13 22:58:01 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 3.0 KB, free 24.9 MB)
17/01/13 22:58:01 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on localhost:63446 (size: 3.0 KB, free: 486.7 MB)
17/01/13 22:58:01 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1006
17/01/13 22:58:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[59] at map at utils.scala:56)
17/01/13 22:58:01 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks
17/01/13 22:58:01 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 18, localhost, partition 0,PROCESS_LOCAL, 2687 bytes)
17/01/13 22:58:01 INFO Executor: Running task 0.0 in stage 14.0 (TID 18)
17/01/13 22:58:01 INFO Executor: Finished task 0.0 in stage 14.0 (TID 18). 1077 bytes result sent to driver
17/01/13 22:58:01 INFO DAGScheduler: ResultStage 14 (collect at utils.scala:59) finished in 0.006 s
17/01/13 22:58:01 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 18) in 6 ms on localhost (1/1)
17/01/13 22:58:01 INFO DAGScheduler: Job 10 finished: collect at utils.scala:59, took 0.012367 s
17/01/13 22:58:01 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
17/01/13 22:58:03 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 208.5 KB, free 25.1 MB)
17/01/13 22:58:03 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 19.3 KB, free 25.1 MB)
17/01/13 22:58:03 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on localhost:63446 (size: 19.3 KB, free: 486.7 MB)
17/01/13 22:58:03 INFO SparkContext: Created broadcast 21 from textFile at TextFile.scala:30
17/01/13 22:58:03 INFO FileInputFormat: Total input paths to process : 1
17/01/13 22:58:03 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/01/13 22:58:03 INFO DAGScheduler: Got job 11 (take at CsvRelation.scala:249) with 1 output partitions
17/01/13 22:58:03 INFO DAGScheduler: Final stage: ResultStage 15 (take at CsvRelation.scala:249)
17/01/13 22:58:03 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:58:03 INFO DAGScheduler: Missing parents: List()
17/01/13 22:58:03 INFO DAGScheduler: Submitting ResultStage 15 (C:\Users\Baoco\AppData\Local\Temp\RtmpwBbCGg/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv MapPartitionsRDD[61] at textFile at TextFile.scala:30), which has no missing parents
17/01/13 22:58:03 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 3.2 KB, free 25.1 MB)
17/01/13 22:58:03 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 1944.0 B, free 25.1 MB)
17/01/13 22:58:03 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on localhost:63446 (size: 1944.0 B, free: 486.7 MB)
17/01/13 22:58:03 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1006
17/01/13 22:58:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (C:\Users\Baoco\AppData\Local\Temp\RtmpwBbCGg/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv MapPartitionsRDD[61] at textFile at TextFile.scala:30)
17/01/13 22:58:03 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks
17/01/13 22:58:03 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 19, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 22:58:03 INFO Executor: Running task 0.0 in stage 15.0 (TID 19)
17/01/13 22:58:03 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpwBbCGg/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv:0+3427780
17/01/13 22:58:03 INFO Executor: Finished task 0.0 in stage 15.0 (TID 19). 2768 bytes result sent to driver
17/01/13 22:58:03 INFO DAGScheduler: ResultStage 15 (take at CsvRelation.scala:249) finished in 0.009 s
17/01/13 22:58:03 INFO DAGScheduler: Job 11 finished: take at CsvRelation.scala:249, took 0.013415 s
17/01/13 22:58:03 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 19) in 8 ms on localhost (1/1)
17/01/13 22:58:03 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
17/01/13 22:58:03 INFO ParseDriver: Parsing command: SELECT * FROM  `batting`
17/01/13 22:58:03 INFO ParseDriver: Parse Completed
17/01/13 22:58:03 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 208.5 KB, free 25.3 MB)
17/01/13 22:58:03 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 19.3 KB, free 25.3 MB)
17/01/13 22:58:03 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on localhost:63446 (size: 19.3 KB, free: 486.7 MB)
17/01/13 22:58:03 INFO SparkContext: Created broadcast 23 from textFile at TextFile.scala:30
17/01/13 22:58:03 INFO FileInputFormat: Total input paths to process : 1
17/01/13 22:58:03 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/01/13 22:58:03 INFO DAGScheduler: Got job 12 (take at CsvRelation.scala:249) with 1 output partitions
17/01/13 22:58:03 INFO DAGScheduler: Final stage: ResultStage 16 (take at CsvRelation.scala:249)
17/01/13 22:58:03 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:58:03 INFO DAGScheduler: Missing parents: List()
17/01/13 22:58:03 INFO DAGScheduler: Submitting ResultStage 16 (C:\Users\Baoco\AppData\Local\Temp\RtmpwBbCGg/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv MapPartitionsRDD[63] at textFile at TextFile.scala:30), which has no missing parents
17/01/13 22:58:03 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 3.2 KB, free 25.3 MB)
17/01/13 22:58:03 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 1944.0 B, free 25.3 MB)
17/01/13 22:58:03 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on localhost:63446 (size: 1944.0 B, free: 486.7 MB)
17/01/13 22:58:03 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1006
17/01/13 22:58:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (C:\Users\Baoco\AppData\Local\Temp\RtmpwBbCGg/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv MapPartitionsRDD[63] at textFile at TextFile.scala:30)
17/01/13 22:58:03 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks
17/01/13 22:58:03 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 20, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 22:58:03 INFO Executor: Running task 0.0 in stage 16.0 (TID 20)
17/01/13 22:58:03 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpwBbCGg/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv:0+3427780
17/01/13 22:58:03 INFO Executor: Finished task 0.0 in stage 16.0 (TID 20). 2768 bytes result sent to driver
17/01/13 22:58:03 INFO DAGScheduler: ResultStage 16 (take at CsvRelation.scala:249) finished in 0.007 s
17/01/13 22:58:03 INFO DAGScheduler: Job 12 finished: take at CsvRelation.scala:249, took 0.010896 s
17/01/13 22:58:03 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 20) in 6 ms on localhost (1/1)
17/01/13 22:58:03 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
17/01/13 22:58:03 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 208.5 KB, free 25.5 MB)
17/01/13 22:58:03 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 19.3 KB, free 25.5 MB)
17/01/13 22:58:03 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on localhost:63446 (size: 19.3 KB, free: 486.6 MB)
17/01/13 22:58:03 INFO SparkContext: Created broadcast 25 from textFile at TextFile.scala:30
17/01/13 22:58:03 INFO FileInputFormat: Total input paths to process : 1
17/01/13 22:58:03 INFO SparkContext: Starting job: sql at null:-2
17/01/13 22:58:03 INFO DAGScheduler: Registering RDD 73 (sql at null:-2)
17/01/13 22:58:03 INFO DAGScheduler: Got job 13 (sql at null:-2) with 1 output partitions
17/01/13 22:58:03 INFO DAGScheduler: Final stage: ResultStage 18 (sql at null:-2)
17/01/13 22:58:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)
17/01/13 22:58:03 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 17)
17/01/13 22:58:03 INFO DAGScheduler: Submitting ShuffleMapStage 17 (MapPartitionsRDD[73] at sql at null:-2), which has no missing parents
17/01/13 22:58:03 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 27.9 KB, free 25.6 MB)
17/01/13 22:58:03 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 11.2 KB, free 25.6 MB)
17/01/13 22:58:03 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on localhost:63446 (size: 11.2 KB, free: 486.6 MB)
17/01/13 22:58:03 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1006
17/01/13 22:58:03 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[73] at sql at null:-2)
17/01/13 22:58:03 INFO TaskSchedulerImpl: Adding task set 17.0 with 2 tasks
17/01/13 22:58:03 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 21, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:58:03 INFO TaskSetManager: Starting task 1.0 in stage 17.0 (TID 22, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:58:03 INFO Executor: Running task 0.0 in stage 17.0 (TID 21)
17/01/13 22:58:03 INFO Executor: Running task 1.0 in stage 17.0 (TID 22)
17/01/13 22:58:03 INFO CacheManager: Partition rdd_70_0 not found, computing it
17/01/13 22:58:03 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpwBbCGg/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv:0+3427780
17/01/13 22:58:03 INFO CacheManager: Partition rdd_70_1 not found, computing it
17/01/13 22:58:03 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpwBbCGg/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv:3427780+3427781
17/01/13 22:58:03 INFO GenerateUnsafeProjection: Code generated in 18.72809 ms
17/01/13 22:58:03 INFO ContextCleaner: Cleaned accumulator 55
17/01/13 22:58:03 INFO BlockManagerInfo: Removed broadcast_23_piece0 on localhost:63446 in memory (size: 19.3 KB, free: 486.6 MB)
17/01/13 22:58:03 INFO BlockManagerInfo: Removed broadcast_22_piece0 on localhost:63446 in memory (size: 1944.0 B, free: 486.6 MB)
17/01/13 22:58:03 INFO ContextCleaner: Cleaned accumulator 54
17/01/13 22:58:03 INFO BlockManagerInfo: Removed broadcast_21_piece0 on localhost:63446 in memory (size: 19.3 KB, free: 486.7 MB)
17/01/13 22:58:03 INFO BlockManagerInfo: Removed broadcast_20_piece0 on localhost:63446 in memory (size: 3.0 KB, free: 486.7 MB)
17/01/13 22:58:03 INFO ContextCleaner: Cleaned accumulator 53
17/01/13 22:58:03 INFO ContextCleaner: Cleaned accumulator 52
17/01/13 22:58:03 INFO BlockManagerInfo: Removed broadcast_24_piece0 on localhost:63446 in memory (size: 1944.0 B, free: 486.7 MB)
17/01/13 22:58:04 INFO MemoryStore: Block rdd_70_0 stored as values in memory (estimated size 1897.3 KB, free 27.0 MB)
17/01/13 22:58:04 INFO BlockManagerInfo: Added rdd_70_0 in memory on localhost:63446 (size: 1897.3 KB, free: 484.8 MB)
17/01/13 22:58:04 INFO Executor: Finished task 0.0 in stage 17.0 (TID 21). 9713 bytes result sent to driver
17/01/13 22:58:04 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 21) in 908 ms on localhost (1/2)
17/01/13 22:58:04 INFO MemoryStore: Block rdd_70_1 stored as values in memory (estimated size 1699.8 KB, free 28.6 MB)
17/01/13 22:58:04 INFO BlockManagerInfo: Added rdd_70_1 in memory on localhost:63446 (size: 1699.8 KB, free: 483.2 MB)
17/01/13 22:58:04 INFO Executor: Finished task 1.0 in stage 17.0 (TID 22). 9862 bytes result sent to driver
17/01/13 22:58:04 INFO TaskSetManager: Finished task 1.0 in stage 17.0 (TID 22) in 938 ms on localhost (2/2)
17/01/13 22:58:04 INFO DAGScheduler: ShuffleMapStage 17 (sql at null:-2) finished in 0.940 s
17/01/13 22:58:04 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
17/01/13 22:58:04 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:58:04 INFO DAGScheduler: running: Set()
17/01/13 22:58:04 INFO DAGScheduler: waiting: Set(ResultStage 18)
17/01/13 22:58:04 INFO DAGScheduler: failed: Set()
17/01/13 22:58:04 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[76] at sql at null:-2), which has no missing parents
17/01/13 22:58:04 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 9.3 KB, free 28.6 MB)
17/01/13 22:58:04 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 4.6 KB, free 28.6 MB)
17/01/13 22:58:04 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on localhost:63446 (size: 4.6 KB, free: 483.2 MB)
17/01/13 22:58:04 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1006
17/01/13 22:58:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[76] at sql at null:-2)
17/01/13 22:58:04 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks
17/01/13 22:58:04 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 23, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 22:58:04 INFO Executor: Running task 0.0 in stage 18.0 (TID 23)
17/01/13 22:58:04 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:58:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:58:04 INFO Executor: Finished task 0.0 in stage 18.0 (TID 23). 1830 bytes result sent to driver
17/01/13 22:58:04 INFO DAGScheduler: ResultStage 18 (sql at null:-2) finished in 0.010 s
17/01/13 22:58:04 INFO DAGScheduler: Job 13 finished: sql at null:-2, took 0.960877 s
17/01/13 22:58:04 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 23) in 9 ms on localhost (1/1)
17/01/13 22:58:04 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
17/01/13 22:58:04 INFO ParseDriver: Parsing command: SELECT count(*) FROM  `batting`
17/01/13 22:58:04 INFO ParseDriver: Parse Completed
17/01/13 22:58:04 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 22:58:04 INFO DAGScheduler: Registering RDD 80 (collect at utils.scala:195)
17/01/13 22:58:04 INFO DAGScheduler: Got job 14 (collect at utils.scala:195) with 1 output partitions
17/01/13 22:58:04 INFO DAGScheduler: Final stage: ResultStage 20 (collect at utils.scala:195)
17/01/13 22:58:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)
17/01/13 22:58:04 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 19)
17/01/13 22:58:04 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[80] at collect at utils.scala:195), which has no missing parents
17/01/13 22:58:04 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 28.0 KB, free 28.7 MB)
17/01/13 22:58:04 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 11.2 KB, free 28.7 MB)
17/01/13 22:58:04 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on localhost:63446 (size: 11.2 KB, free: 483.1 MB)
17/01/13 22:58:04 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1006
17/01/13 22:58:04 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[80] at collect at utils.scala:195)
17/01/13 22:58:04 INFO TaskSchedulerImpl: Adding task set 19.0 with 2 tasks
17/01/13 22:58:04 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 24, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:58:04 INFO TaskSetManager: Starting task 1.0 in stage 19.0 (TID 25, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:58:04 INFO Executor: Running task 0.0 in stage 19.0 (TID 24)
17/01/13 22:58:04 INFO Executor: Running task 1.0 in stage 19.0 (TID 25)
17/01/13 22:58:04 INFO BlockManager: Found block rdd_70_0 locally
17/01/13 22:58:04 INFO BlockManager: Found block rdd_70_1 locally
17/01/13 22:58:04 INFO Executor: Finished task 0.0 in stage 19.0 (TID 24). 2679 bytes result sent to driver
17/01/13 22:58:04 INFO Executor: Finished task 1.0 in stage 19.0 (TID 25). 2679 bytes result sent to driver
17/01/13 22:58:04 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 24) in 22 ms on localhost (1/2)
17/01/13 22:58:04 INFO DAGScheduler: ShuffleMapStage 19 (collect at utils.scala:195) finished in 0.025 s
17/01/13 22:58:04 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:58:04 INFO DAGScheduler: running: Set()
17/01/13 22:58:04 INFO DAGScheduler: waiting: Set(ResultStage 20)
17/01/13 22:58:04 INFO DAGScheduler: failed: Set()
17/01/13 22:58:04 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[83] at collect at utils.scala:195), which has no missing parents
17/01/13 22:58:04 INFO TaskSetManager: Finished task 1.0 in stage 19.0 (TID 25) in 24 ms on localhost (2/2)
17/01/13 22:58:04 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
17/01/13 22:58:04 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 9.4 KB, free 28.7 MB)
17/01/13 22:58:04 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 4.7 KB, free 28.7 MB)
17/01/13 22:58:04 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on localhost:63446 (size: 4.7 KB, free: 483.1 MB)
17/01/13 22:58:04 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1006
17/01/13 22:58:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[83] at collect at utils.scala:195)
17/01/13 22:58:04 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks
17/01/13 22:58:04 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 26, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 22:58:04 INFO Executor: Running task 0.0 in stage 20.0 (TID 26)
17/01/13 22:58:04 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:58:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/01/13 22:58:04 INFO Executor: Finished task 0.0 in stage 20.0 (TID 26). 1830 bytes result sent to driver
17/01/13 22:58:04 INFO DAGScheduler: ResultStage 20 (collect at utils.scala:195) finished in 0.009 s
17/01/13 22:58:04 INFO DAGScheduler: Job 14 finished: collect at utils.scala:195, took 0.044763 s
17/01/13 22:58:04 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 26) in 9 ms on localhost (1/1)
17/01/13 22:58:04 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
17/01/13 22:58:04 INFO ParseDriver: Parsing command: SELECT *
FROM `batting` AS `zzz3`
WHERE (0 = 1)
17/01/13 22:58:04 INFO ParseDriver: Parse Completed
17/01/13 22:58:04 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/01/13 22:58:04 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/01/13 22:58:04 INFO SparkContext: Starting job: collect at utils.scala:59
17/01/13 22:58:04 INFO DAGScheduler: Got job 15 (collect at utils.scala:59) with 1 output partitions
17/01/13 22:58:04 INFO DAGScheduler: Final stage: ResultStage 21 (collect at utils.scala:59)
17/01/13 22:58:04 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:58:04 INFO DAGScheduler: Missing parents: List()
17/01/13 22:58:04 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[87] at map at utils.scala:56), which has no missing parents
17/01/13 22:58:04 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 5.4 KB, free 28.7 MB)
17/01/13 22:58:04 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 3.0 KB, free 28.7 MB)
17/01/13 22:58:04 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on localhost:63446 (size: 3.0 KB, free: 483.1 MB)
17/01/13 22:58:04 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1006
17/01/13 22:58:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[87] at map at utils.scala:56)
17/01/13 22:58:04 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks
17/01/13 22:58:04 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 27, localhost, partition 0,PROCESS_LOCAL, 2728 bytes)
17/01/13 22:58:04 INFO Executor: Running task 0.0 in stage 21.0 (TID 27)
17/01/13 22:58:04 INFO Executor: Finished task 0.0 in stage 21.0 (TID 27). 1087 bytes result sent to driver
17/01/13 22:58:04 INFO DAGScheduler: ResultStage 21 (collect at utils.scala:59) finished in 0.005 s
17/01/13 22:58:04 INFO DAGScheduler: Job 15 finished: collect at utils.scala:59, took 0.011976 s
17/01/13 22:58:04 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 27) in 5 ms on localhost (1/1)
17/01/13 22:58:04 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
17/01/13 22:58:04 INFO ParseDriver: Parsing command: SELECT *
FROM `flights`
WHERE (`dep_delay` = 2.0)
17/01/13 22:58:04 INFO ParseDriver: Parse Completed
17/01/13 22:58:04 INFO InMemoryColumnarTableScan: Predicate (dep_delay#146 = 2.0) generates partition filter: ((dep_delay.lowerBound#1071 <= 2.0) && (2.0 <= dep_delay.upperBound#1070))
17/01/13 22:58:04 INFO SparkContext: Starting job: count at null:-2
17/01/13 22:58:04 INFO DAGScheduler: Registering RDD 92 (count at null:-2)
17/01/13 22:58:04 INFO DAGScheduler: Got job 16 (count at null:-2) with 1 output partitions
17/01/13 22:58:04 INFO DAGScheduler: Final stage: ResultStage 23 (count at null:-2)
17/01/13 22:58:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 22)
17/01/13 22:58:04 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 22)
17/01/13 22:58:04 INFO DAGScheduler: Submitting ShuffleMapStage 22 (MapPartitionsRDD[92] at count at null:-2), which has no missing parents
17/01/13 22:58:04 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 28.8 KB, free 28.7 MB)
17/01/13 22:58:04 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 11.7 KB, free 28.7 MB)
17/01/13 22:58:04 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on localhost:63446 (size: 11.7 KB, free: 483.1 MB)
17/01/13 22:58:04 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1006
17/01/13 22:58:04 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[92] at count at null:-2)
17/01/13 22:58:04 INFO TaskSchedulerImpl: Adding task set 22.0 with 2 tasks
17/01/13 22:58:04 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 28, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:58:04 INFO TaskSetManager: Starting task 1.0 in stage 22.0 (TID 29, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:58:04 INFO Executor: Running task 0.0 in stage 22.0 (TID 28)
17/01/13 22:58:04 INFO Executor: Running task 1.0 in stage 22.0 (TID 29)
17/01/13 22:58:04 INFO BlockManager: Found block rdd_42_0 locally
17/01/13 22:58:04 INFO BlockManager: Found block rdd_42_1 locally
17/01/13 22:58:04 INFO GeneratePredicate: Code generated in 17.129799 ms
17/01/13 22:58:04 INFO GenerateColumnAccessor: Code generated in 14.792094 ms
17/01/13 22:58:04 INFO GeneratePredicate: Code generated in 4.48085 ms
17/01/13 22:58:05 INFO Executor: Finished task 1.0 in stage 22.0 (TID 29). 2808 bytes result sent to driver
17/01/13 22:58:05 INFO TaskSetManager: Finished task 1.0 in stage 22.0 (TID 29) in 132 ms on localhost (1/2)
17/01/13 22:58:05 INFO Executor: Finished task 0.0 in stage 22.0 (TID 28). 2808 bytes result sent to driver
17/01/13 22:58:05 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 28) in 140 ms on localhost (2/2)
17/01/13 22:58:05 INFO DAGScheduler: ShuffleMapStage 22 (count at null:-2) finished in 0.141 s
17/01/13 22:58:05 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
17/01/13 22:58:05 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:58:05 INFO DAGScheduler: running: Set()
17/01/13 22:58:05 INFO DAGScheduler: waiting: Set(ResultStage 23)
17/01/13 22:58:05 INFO DAGScheduler: failed: Set()
17/01/13 22:58:05 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[95] at count at null:-2), which has no missing parents
17/01/13 22:58:05 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 10.8 KB, free 28.8 MB)
17/01/13 22:58:05 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 5.3 KB, free 28.8 MB)
17/01/13 22:58:05 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on localhost:63446 (size: 5.3 KB, free: 483.1 MB)
17/01/13 22:58:05 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1006
17/01/13 22:58:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[95] at count at null:-2)
17/01/13 22:58:05 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks
17/01/13 22:58:05 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 30, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 22:58:05 INFO Executor: Running task 0.0 in stage 23.0 (TID 30)
17/01/13 22:58:05 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:58:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:58:05 INFO Executor: Finished task 0.0 in stage 23.0 (TID 30). 1959 bytes result sent to driver
17/01/13 22:58:05 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 30) in 12 ms on localhost (1/1)
17/01/13 22:58:05 INFO DAGScheduler: ResultStage 23 (count at null:-2) finished in 0.012 s
17/01/13 22:58:05 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
17/01/13 22:58:05 INFO DAGScheduler: Job 16 finished: count at null:-2, took 0.164767 s
17/01/13 22:58:05 INFO ParseDriver: Parsing command: SELECT *
FROM `flights`
WHERE (`dep_delay` = 2.0)
17/01/13 22:58:05 INFO ParseDriver: Parse Completed
17/01/13 22:58:05 INFO InMemoryColumnarTableScan: Predicate (dep_delay#146 = 2.0) generates partition filter: ((dep_delay.lowerBound#1170 <= 2.0) && (2.0 <= dep_delay.upperBound#1169))
17/01/13 22:58:05 INFO SparkContext: Starting job: count at null:-2
17/01/13 22:58:05 INFO DAGScheduler: Registering RDD 100 (count at null:-2)
17/01/13 22:58:05 INFO DAGScheduler: Got job 17 (count at null:-2) with 1 output partitions
17/01/13 22:58:05 INFO DAGScheduler: Final stage: ResultStage 25 (count at null:-2)
17/01/13 22:58:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 24)
17/01/13 22:58:05 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 24)
17/01/13 22:58:05 INFO DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[100] at count at null:-2), which has no missing parents
17/01/13 22:58:05 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 28.8 KB, free 28.8 MB)
17/01/13 22:58:05 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 11.8 KB, free 28.8 MB)
17/01/13 22:58:05 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on localhost:63446 (size: 11.8 KB, free: 483.1 MB)
17/01/13 22:58:05 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1006
17/01/13 22:58:05 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[100] at count at null:-2)
17/01/13 22:58:05 INFO TaskSchedulerImpl: Adding task set 24.0 with 2 tasks
17/01/13 22:58:05 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 31, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:58:05 INFO TaskSetManager: Starting task 1.0 in stage 24.0 (TID 32, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:58:05 INFO Executor: Running task 0.0 in stage 24.0 (TID 31)
17/01/13 22:58:05 INFO Executor: Running task 1.0 in stage 24.0 (TID 32)
17/01/13 22:58:05 INFO BlockManager: Found block rdd_42_1 locally
17/01/13 22:58:05 INFO BlockManager: Found block rdd_42_0 locally
17/01/13 22:58:05 INFO Executor: Finished task 1.0 in stage 24.0 (TID 32). 2808 bytes result sent to driver
17/01/13 22:58:05 INFO TaskSetManager: Finished task 1.0 in stage 24.0 (TID 32) in 38 ms on localhost (1/2)
17/01/13 22:58:05 INFO Executor: Finished task 0.0 in stage 24.0 (TID 31). 2808 bytes result sent to driver
17/01/13 22:58:05 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 31) in 41 ms on localhost (2/2)
17/01/13 22:58:05 INFO DAGScheduler: ShuffleMapStage 24 (count at null:-2) finished in 0.041 s
17/01/13 22:58:05 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
17/01/13 22:58:05 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:58:05 INFO DAGScheduler: running: Set()
17/01/13 22:58:05 INFO DAGScheduler: waiting: Set(ResultStage 25)
17/01/13 22:58:05 INFO DAGScheduler: failed: Set()
17/01/13 22:58:05 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[103] at count at null:-2), which has no missing parents
17/01/13 22:58:05 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 10.8 KB, free 28.8 MB)
17/01/13 22:58:05 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 5.3 KB, free 28.8 MB)
17/01/13 22:58:05 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on localhost:63446 (size: 5.3 KB, free: 483.1 MB)
17/01/13 22:58:05 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1006
17/01/13 22:58:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[103] at count at null:-2)
17/01/13 22:58:05 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks
17/01/13 22:58:05 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 33, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 22:58:05 INFO Executor: Running task 0.0 in stage 25.0 (TID 33)
17/01/13 22:58:05 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:58:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:58:05 INFO Executor: Finished task 0.0 in stage 25.0 (TID 33). 1959 bytes result sent to driver
17/01/13 22:58:05 INFO DAGScheduler: ResultStage 25 (count at null:-2) finished in 0.017 s
17/01/13 22:58:05 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 33) in 17 ms on localhost (1/1)
17/01/13 22:58:05 INFO DAGScheduler: Job 17 finished: count at null:-2, took 0.069871 s
17/01/13 22:58:05 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
17/01/13 22:58:05 INFO ParseDriver: Parsing command: SELECT *
FROM (SELECT *
FROM `flights`
WHERE (`dep_delay` = 2.0)) `vkzycdtsmq`
LIMIT 10
17/01/13 22:58:05 INFO ParseDriver: Parse Completed
17/01/13 22:58:05 INFO InMemoryColumnarTableScan: Predicate (dep_delay#146 = 2.0) generates partition filter: ((dep_delay.lowerBound#1268 <= 2.0) && (2.0 <= dep_delay.upperBound#1267))
17/01/13 22:58:05 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 22:58:05 INFO DAGScheduler: Got job 18 (collect at utils.scala:195) with 1 output partitions
17/01/13 22:58:05 INFO DAGScheduler: Final stage: ResultStage 26 (collect at utils.scala:195)
17/01/13 22:58:05 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:58:05 INFO DAGScheduler: Missing parents: List()
17/01/13 22:58:05 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[107] at collect at utils.scala:195), which has no missing parents
17/01/13 22:58:05 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 25.3 KB, free 28.8 MB)
17/01/13 22:58:05 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 10.2 KB, free 28.9 MB)
17/01/13 22:58:05 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on localhost:63446 (size: 10.2 KB, free: 483.1 MB)
17/01/13 22:58:05 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1006
17/01/13 22:58:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[107] at collect at utils.scala:195)
17/01/13 22:58:05 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks
17/01/13 22:58:05 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 34, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 22:58:05 INFO Executor: Running task 0.0 in stage 26.0 (TID 34)
17/01/13 22:58:05 INFO BlockManager: Found block rdd_42_0 locally
17/01/13 22:58:05 INFO GenerateColumnAccessor: Code generated in 22.204568 ms
17/01/13 22:58:05 INFO GeneratePredicate: Code generated in 3.79221 ms
17/01/13 22:58:05 INFO GenerateSafeProjection: Code generated in 16.997532 ms
17/01/13 22:58:05 INFO Executor: Finished task 0.0 in stage 26.0 (TID 34). 5325 bytes result sent to driver
17/01/13 22:58:05 INFO DAGScheduler: ResultStage 26 (collect at utils.scala:195) finished in 0.075 s
17/01/13 22:58:05 INFO DAGScheduler: Job 18 finished: collect at utils.scala:195, took 0.081545 s
17/01/13 22:58:05 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 34) in 74 ms on localhost (1/1)
17/01/13 22:58:05 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
17/01/13 22:58:05 INFO ParseDriver: Parsing command: SELECT *
FROM (SELECT `tailnum`, count(*) AS `count`, AVG(`distance`) AS `dist`, AVG(`arr_delay`) AS `delay`
FROM `flights`
GROUP BY `tailnum`) `clolgtoyjk`
WHERE ((`count` > 20.0) AND (`dist` < 2000.0) AND (NOT((`delay`) IS NULL)))
17/01/13 22:58:05 INFO ParseDriver: Parse Completed
17/01/13 22:58:05 INFO ContextCleaner: Cleaned accumulator 89
17/01/13 22:58:05 INFO BlockManagerInfo: Removed broadcast_27_piece0 on localhost:63446 in memory (size: 4.6 KB, free: 483.1 MB)
17/01/13 22:58:05 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 22:58:05 INFO DAGScheduler: Registering RDD 110 (collect at utils.scala:195)
17/01/13 22:58:05 INFO DAGScheduler: Got job 19 (collect at utils.scala:195) with 4 output partitions
17/01/13 22:58:05 INFO DAGScheduler: Final stage: ResultStage 28 (collect at utils.scala:195)
17/01/13 22:58:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)
17/01/13 22:58:05 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 27)
17/01/13 22:58:05 INFO DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[110] at collect at utils.scala:195), which has no missing parents
17/01/13 22:58:05 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 28.1 KB, free 28.9 MB)
17/01/13 22:58:05 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 11.4 KB, free 28.9 MB)
17/01/13 22:58:05 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on localhost:63446 (size: 11.4 KB, free: 483.1 MB)
17/01/13 22:58:05 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1006
17/01/13 22:58:05 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[110] at collect at utils.scala:195)
17/01/13 22:58:05 INFO TaskSchedulerImpl: Adding task set 27.0 with 2 tasks
17/01/13 22:58:05 INFO BlockManagerInfo: Removed broadcast_35_piece0 on localhost:63446 in memory (size: 10.2 KB, free: 483.1 MB)
17/01/13 22:58:05 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 35, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:58:05 INFO TaskSetManager: Starting task 1.0 in stage 27.0 (TID 36, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:58:05 INFO Executor: Running task 0.0 in stage 27.0 (TID 35)
17/01/13 22:58:05 INFO Executor: Running task 1.0 in stage 27.0 (TID 36)
17/01/13 22:58:05 INFO ContextCleaner: Cleaned accumulator 107
17/01/13 22:58:05 INFO BlockManager: Found block rdd_42_0 locally
17/01/13 22:58:05 INFO BlockManager: Found block rdd_42_1 locally
17/01/13 22:58:05 INFO BlockManagerInfo: Removed broadcast_34_piece0 on localhost:63446 in memory (size: 5.3 KB, free: 483.1 MB)
17/01/13 22:58:05 INFO ContextCleaner: Cleaned accumulator 104
17/01/13 22:58:05 INFO GenerateColumnAccessor: Code generated in 15.394973 ms
17/01/13 22:58:05 INFO GenerateMutableProjection: Code generated in 9.416952 ms
17/01/13 22:58:05 INFO BlockManagerInfo: Removed broadcast_33_piece0 on localhost:63446 in memory (size: 11.8 KB, free: 483.1 MB)
17/01/13 22:58:05 INFO ContextCleaner: Cleaned accumulator 103
17/01/13 22:58:05 INFO ContextCleaner: Cleaned shuffle 7
17/01/13 22:58:05 INFO ContextCleaner: Cleaned accumulator 102
17/01/13 22:58:05 INFO ContextCleaner: Cleaned accumulator 101
17/01/13 22:58:05 INFO ContextCleaner: Cleaned accumulator 100
17/01/13 22:58:05 INFO ContextCleaner: Cleaned accumulator 99
17/01/13 22:58:05 INFO ContextCleaner: Cleaned accumulator 98
17/01/13 22:58:05 INFO ContextCleaner: Cleaned accumulator 97
17/01/13 22:58:05 INFO ContextCleaner: Cleaned accumulator 96
17/01/13 22:58:05 INFO ContextCleaner: Cleaned accumulator 95
17/01/13 22:58:05 INFO ContextCleaner: Cleaned accumulator 94
17/01/13 22:58:05 INFO ContextCleaner: Cleaned accumulator 93
17/01/13 22:58:05 INFO ContextCleaner: Cleaned accumulator 92
17/01/13 22:58:05 INFO GenerateMutableProjection: Code generated in 23.75422 ms
17/01/13 22:58:05 INFO BlockManagerInfo: Removed broadcast_32_piece0 on localhost:63446 in memory (size: 5.3 KB, free: 483.1 MB)
17/01/13 22:58:05 INFO GenerateUnsafeRowJoiner: Code generated in 6.668368 ms
17/01/13 22:58:05 INFO ContextCleaner: Cleaned accumulator 91
17/01/13 22:58:05 INFO BlockManagerInfo: Removed broadcast_31_piece0 on localhost:63446 in memory (size: 11.7 KB, free: 483.1 MB)
17/01/13 22:58:05 INFO ContextCleaner: Cleaned accumulator 90
17/01/13 22:58:05 INFO ContextCleaner: Cleaned shuffle 6
17/01/13 22:58:05 INFO ContextCleaner: Cleaned accumulator 88
17/01/13 22:58:05 INFO ContextCleaner: Cleaned accumulator 87
17/01/13 22:58:05 INFO ContextCleaner: Cleaned accumulator 86
17/01/13 22:58:05 INFO ContextCleaner: Cleaned accumulator 85
17/01/13 22:58:05 INFO ContextCleaner: Cleaned accumulator 84
17/01/13 22:58:05 INFO ContextCleaner: Cleaned accumulator 83
17/01/13 22:58:05 INFO ContextCleaner: Cleaned accumulator 82
17/01/13 22:58:05 INFO ContextCleaner: Cleaned accumulator 81
17/01/13 22:58:05 INFO ContextCleaner: Cleaned accumulator 80
17/01/13 22:58:05 INFO ContextCleaner: Cleaned accumulator 79
17/01/13 22:58:05 INFO BlockManagerInfo: Removed broadcast_30_piece0 on localhost:63446 in memory (size: 3.0 KB, free: 483.1 MB)
17/01/13 22:58:05 INFO ContextCleaner: Cleaned accumulator 78
17/01/13 22:58:05 INFO ContextCleaner: Cleaned accumulator 77
17/01/13 22:58:05 INFO BlockManagerInfo: Removed broadcast_29_piece0 on localhost:63446 in memory (size: 4.7 KB, free: 483.1 MB)
17/01/13 22:58:05 INFO ContextCleaner: Cleaned accumulator 76
17/01/13 22:58:05 INFO BlockManagerInfo: Removed broadcast_28_piece0 on localhost:63446 in memory (size: 11.2 KB, free: 483.1 MB)
17/01/13 22:58:05 INFO ContextCleaner: Cleaned accumulator 75
17/01/13 22:58:05 INFO ContextCleaner: Cleaned shuffle 5
17/01/13 22:58:05 INFO GenerateUnsafeProjection: Code generated in 26.958057 ms
17/01/13 22:58:06 INFO GenerateMutableProjection: Code generated in 5.318396 ms
17/01/13 22:58:06 INFO Executor: Finished task 0.0 in stage 27.0 (TID 35). 2682 bytes result sent to driver
17/01/13 22:58:06 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 35) in 462 ms on localhost (1/2)
17/01/13 22:58:06 INFO Executor: Finished task 1.0 in stage 27.0 (TID 36). 2682 bytes result sent to driver
17/01/13 22:58:06 INFO DAGScheduler: ShuffleMapStage 27 (collect at utils.scala:195) finished in 0.482 s
17/01/13 22:58:06 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:58:06 INFO DAGScheduler: running: Set()
17/01/13 22:58:06 INFO DAGScheduler: waiting: Set(ResultStage 28)
17/01/13 22:58:06 INFO DAGScheduler: failed: Set()
17/01/13 22:58:06 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[115] at collect at utils.scala:195), which has no missing parents
17/01/13 22:58:06 INFO TaskSetManager: Finished task 1.0 in stage 27.0 (TID 36) in 481 ms on localhost (2/2)
17/01/13 22:58:06 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
17/01/13 22:58:06 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 13.5 KB, free 28.7 MB)
17/01/13 22:58:06 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 6.3 KB, free 28.7 MB)
17/01/13 22:58:06 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on localhost:63446 (size: 6.3 KB, free: 483.1 MB)
17/01/13 22:58:06 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1006
17/01/13 22:58:06 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 28 (MapPartitionsRDD[115] at collect at utils.scala:195)
17/01/13 22:58:06 INFO TaskSchedulerImpl: Adding task set 28.0 with 4 tasks
17/01/13 22:58:06 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 37, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 22:58:06 INFO TaskSetManager: Starting task 1.0 in stage 28.0 (TID 38, localhost, partition 1,NODE_LOCAL, 2242 bytes)
17/01/13 22:58:06 INFO TaskSetManager: Starting task 2.0 in stage 28.0 (TID 39, localhost, partition 2,NODE_LOCAL, 2242 bytes)
17/01/13 22:58:06 INFO TaskSetManager: Starting task 3.0 in stage 28.0 (TID 40, localhost, partition 3,NODE_LOCAL, 2242 bytes)
17/01/13 22:58:06 INFO Executor: Running task 0.0 in stage 28.0 (TID 37)
17/01/13 22:58:06 INFO Executor: Running task 1.0 in stage 28.0 (TID 38)
17/01/13 22:58:06 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:58:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:58:06 INFO GenerateMutableProjection: Code generated in 9.752738 ms
17/01/13 22:58:06 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:58:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/01/13 22:58:06 INFO Executor: Running task 3.0 in stage 28.0 (TID 40)
17/01/13 22:58:06 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:58:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/01/13 22:58:06 INFO Executor: Running task 2.0 in stage 28.0 (TID 39)
17/01/13 22:58:06 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:58:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:58:06 INFO GenerateMutableProjection: Code generated in 28.492349 ms
17/01/13 22:58:06 INFO GenerateUnsafeProjection: Code generated in 9.903779 ms
17/01/13 22:58:06 INFO GeneratePredicate: Code generated in 31.90568 ms
17/01/13 22:58:06 INFO Executor: Finished task 0.0 in stage 28.0 (TID 37). 49931 bytes result sent to driver
17/01/13 22:58:06 INFO Executor: Finished task 3.0 in stage 28.0 (TID 40). 48431 bytes result sent to driver
17/01/13 22:58:06 INFO Executor: Finished task 2.0 in stage 28.0 (TID 39). 50586 bytes result sent to driver
17/01/13 22:58:06 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 37) in 135 ms on localhost (1/4)
17/01/13 22:58:06 INFO Executor: Finished task 1.0 in stage 28.0 (TID 38). 52153 bytes result sent to driver
17/01/13 22:58:06 INFO TaskSetManager: Finished task 3.0 in stage 28.0 (TID 40) in 136 ms on localhost (2/4)
17/01/13 22:58:06 INFO TaskSetManager: Finished task 2.0 in stage 28.0 (TID 39) in 139 ms on localhost (3/4)
17/01/13 22:58:06 INFO DAGScheduler: ResultStage 28 (collect at utils.scala:195) finished in 0.141 s
17/01/13 22:58:06 INFO DAGScheduler: Job 19 finished: collect at utils.scala:195, took 0.635488 s
17/01/13 22:58:06 INFO TaskSetManager: Finished task 1.0 in stage 28.0 (TID 38) in 141 ms on localhost (4/4)
17/01/13 22:58:06 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
17/01/13 22:58:08 INFO ParseDriver: Parsing command: SELECT `playerID`, `yearID`, `teamID`, `G`, `AB`, `R`, `H`
FROM (SELECT `playerID`, `yearID`, `teamID`, `G`, `AB`, `R`, `H`, rank() OVER (PARTITION BY `playerID` ORDER BY `H` DESC) AS `zzz4`
FROM (SELECT *
FROM (SELECT `playerID` AS `playerID`, `yearID` AS `yearID`, `teamID` AS `teamID`, `G` AS `G`, `AB` AS `AB`, `R` AS `R`, `H` AS `H`
FROM `batting`) `paugpxbuqz`
ORDER BY `playerID`, `yearID`, `teamID`) `kvfjdmbunv`) `iwkwdwkrjt`
WHERE (`zzz4` <= 2.0 AND `H` > 0.0)
17/01/13 22:58:08 INFO ParseDriver: Parse Completed
17/01/13 22:58:09 INFO SparkContext: Starting job: count at null:-2
17/01/13 22:58:09 INFO DAGScheduler: Got job 20 (count at null:-2) with 2 output partitions
17/01/13 22:58:09 INFO DAGScheduler: Final stage: ResultStage 29 (count at null:-2)
17/01/13 22:58:09 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:58:09 INFO DAGScheduler: Missing parents: List()
17/01/13 22:58:09 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[121] at count at null:-2), which has no missing parents
17/01/13 22:58:09 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 26.4 KB, free 28.7 MB)
17/01/13 22:58:09 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 10.4 KB, free 28.7 MB)
17/01/13 22:58:09 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on localhost:63446 (size: 10.4 KB, free: 483.1 MB)
17/01/13 22:58:09 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1006
17/01/13 22:58:09 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 29 (MapPartitionsRDD[121] at count at null:-2)
17/01/13 22:58:09 INFO TaskSchedulerImpl: Adding task set 29.0 with 2 tasks
17/01/13 22:58:09 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 41, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 22:58:09 INFO TaskSetManager: Starting task 1.0 in stage 29.0 (TID 42, localhost, partition 1,PROCESS_LOCAL, 2481 bytes)
17/01/13 22:58:09 INFO Executor: Running task 0.0 in stage 29.0 (TID 41)
17/01/13 22:58:09 INFO Executor: Running task 1.0 in stage 29.0 (TID 42)
17/01/13 22:58:09 INFO BlockManager: Found block rdd_70_0 locally
17/01/13 22:58:09 INFO BlockManager: Found block rdd_70_1 locally
17/01/13 22:58:09 INFO GenerateColumnAccessor: Code generated in 13.192095 ms
17/01/13 22:58:09 INFO GenerateUnsafeProjection: Code generated in 9.151992 ms
17/01/13 22:58:09 INFO GenerateSafeProjection: Code generated in 9.748045 ms
17/01/13 22:58:09 INFO Executor: Finished task 1.0 in stage 29.0 (TID 42). 13552 bytes result sent to driver
17/01/13 22:58:09 INFO TaskSetManager: Finished task 1.0 in stage 29.0 (TID 42) in 262 ms on localhost (1/2)
17/01/13 22:58:09 INFO Executor: Finished task 0.0 in stage 29.0 (TID 41). 13905 bytes result sent to driver
17/01/13 22:58:09 INFO DAGScheduler: ResultStage 29 (count at null:-2) finished in 0.278 s
17/01/13 22:58:09 INFO DAGScheduler: Job 20 finished: count at null:-2, took 0.283522 s
17/01/13 22:58:09 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 41) in 278 ms on localhost (2/2)
17/01/13 22:58:09 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool 
17/01/13 22:58:09 INFO SparkContext: Starting job: count at null:-2
17/01/13 22:58:09 INFO DAGScheduler: Registering RDD 122 (count at null:-2)
17/01/13 22:58:09 INFO DAGScheduler: Registering RDD 126 (count at null:-2)
17/01/13 22:58:09 INFO DAGScheduler: Registering RDD 133 (count at null:-2)
17/01/13 22:58:09 INFO DAGScheduler: Got job 21 (count at null:-2) with 1 output partitions
17/01/13 22:58:09 INFO DAGScheduler: Final stage: ResultStage 33 (count at null:-2)
17/01/13 22:58:09 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 32)
17/01/13 22:58:09 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 32)
17/01/13 22:58:09 INFO DAGScheduler: Submitting ShuffleMapStage 30 (MapPartitionsRDD[122] at count at null:-2), which has no missing parents
17/01/13 22:58:09 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 28.9 KB, free 28.8 MB)
17/01/13 22:58:09 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 11.7 KB, free 28.8 MB)
17/01/13 22:58:09 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on localhost:63446 (size: 11.7 KB, free: 483.1 MB)
17/01/13 22:58:09 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1006
17/01/13 22:58:09 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 30 (MapPartitionsRDD[122] at count at null:-2)
17/01/13 22:58:09 INFO TaskSchedulerImpl: Adding task set 30.0 with 2 tasks
17/01/13 22:58:09 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 43, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:58:09 INFO TaskSetManager: Starting task 1.0 in stage 30.0 (TID 44, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:58:09 INFO Executor: Running task 0.0 in stage 30.0 (TID 43)
17/01/13 22:58:09 INFO Executor: Running task 1.0 in stage 30.0 (TID 44)
17/01/13 22:58:09 INFO BlockManager: Found block rdd_70_0 locally
17/01/13 22:58:09 INFO BlockManager: Found block rdd_70_1 locally
17/01/13 22:58:10 INFO Executor: Finished task 0.0 in stage 30.0 (TID 43). 2580 bytes result sent to driver
17/01/13 22:58:10 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 43) in 446 ms on localhost (1/2)
17/01/13 22:58:10 INFO Executor: Finished task 1.0 in stage 30.0 (TID 44). 2580 bytes result sent to driver
17/01/13 22:58:10 INFO DAGScheduler: ShuffleMapStage 30 (count at null:-2) finished in 0.463 s
17/01/13 22:58:10 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:58:10 INFO DAGScheduler: running: Set()
17/01/13 22:58:10 INFO DAGScheduler: waiting: Set(ResultStage 33, ShuffleMapStage 31, ShuffleMapStage 32)
17/01/13 22:58:10 INFO DAGScheduler: failed: Set()
17/01/13 22:58:10 INFO DAGScheduler: Submitting ShuffleMapStage 31 (MapPartitionsRDD[126] at count at null:-2), which has no missing parents
17/01/13 22:58:10 INFO TaskSetManager: Finished task 1.0 in stage 30.0 (TID 44) in 462 ms on localhost (2/2)
17/01/13 22:58:10 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool 
17/01/13 22:58:10 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 10.3 KB, free 28.8 MB)
17/01/13 22:58:10 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 5.3 KB, free 28.8 MB)
17/01/13 22:58:10 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on localhost:63446 (size: 5.3 KB, free: 483.1 MB)
17/01/13 22:58:10 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1006
17/01/13 22:58:10 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 31 (MapPartitionsRDD[126] at count at null:-2)
17/01/13 22:58:10 INFO TaskSchedulerImpl: Adding task set 31.0 with 4 tasks
17/01/13 22:58:10 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 45, localhost, partition 0,NODE_LOCAL, 2231 bytes)
17/01/13 22:58:10 INFO TaskSetManager: Starting task 1.0 in stage 31.0 (TID 46, localhost, partition 1,NODE_LOCAL, 2231 bytes)
17/01/13 22:58:10 INFO TaskSetManager: Starting task 2.0 in stage 31.0 (TID 47, localhost, partition 2,NODE_LOCAL, 2231 bytes)
17/01/13 22:58:10 INFO TaskSetManager: Starting task 3.0 in stage 31.0 (TID 48, localhost, partition 3,NODE_LOCAL, 2231 bytes)
17/01/13 22:58:10 INFO Executor: Running task 0.0 in stage 31.0 (TID 45)
17/01/13 22:58:10 INFO Executor: Running task 2.0 in stage 31.0 (TID 47)
17/01/13 22:58:10 INFO Executor: Running task 1.0 in stage 31.0 (TID 46)
17/01/13 22:58:10 INFO Executor: Running task 3.0 in stage 31.0 (TID 48)
17/01/13 22:58:10 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:58:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:58:10 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:58:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:58:10 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:58:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:58:10 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:58:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:58:10 INFO GenerateUnsafeProjection: Code generated in 10.525004 ms
17/01/13 22:58:10 INFO GenerateOrdering: Code generated in 18.493851 ms
17/01/13 22:58:10 INFO GenerateUnsafeProjection: Code generated in 5.412688 ms
17/01/13 22:58:10 INFO Executor: Finished task 2.0 in stage 31.0 (TID 47). 1660 bytes result sent to driver
17/01/13 22:58:10 INFO TaskSetManager: Finished task 2.0 in stage 31.0 (TID 47) in 641 ms on localhost (1/4)
17/01/13 22:58:10 INFO Executor: Finished task 1.0 in stage 31.0 (TID 46). 1660 bytes result sent to driver
17/01/13 22:58:10 INFO TaskSetManager: Finished task 1.0 in stage 31.0 (TID 46) in 691 ms on localhost (2/4)
17/01/13 22:58:10 INFO Executor: Finished task 3.0 in stage 31.0 (TID 48). 1660 bytes result sent to driver
17/01/13 22:58:10 INFO TaskSetManager: Finished task 3.0 in stage 31.0 (TID 48) in 724 ms on localhost (3/4)
17/01/13 22:58:10 INFO Executor: Finished task 0.0 in stage 31.0 (TID 45). 1660 bytes result sent to driver
17/01/13 22:58:10 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 45) in 739 ms on localhost (4/4)
17/01/13 22:58:10 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool 
17/01/13 22:58:10 INFO DAGScheduler: ShuffleMapStage 31 (count at null:-2) finished in 0.739 s
17/01/13 22:58:10 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:58:10 INFO DAGScheduler: running: Set()
17/01/13 22:58:10 INFO DAGScheduler: waiting: Set(ResultStage 33, ShuffleMapStage 32)
17/01/13 22:58:10 INFO DAGScheduler: failed: Set()
17/01/13 22:58:10 INFO DAGScheduler: Submitting ShuffleMapStage 32 (MapPartitionsRDD[133] at count at null:-2), which has no missing parents
17/01/13 22:58:10 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 17.9 KB, free 28.8 MB)
17/01/13 22:58:10 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 8.1 KB, free 28.8 MB)
17/01/13 22:58:10 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on localhost:63446 (size: 8.1 KB, free: 483.1 MB)
17/01/13 22:58:10 INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1006
17/01/13 22:58:10 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 32 (MapPartitionsRDD[133] at count at null:-2)
17/01/13 22:58:10 INFO TaskSchedulerImpl: Adding task set 32.0 with 4 tasks
17/01/13 22:58:10 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 49, localhost, partition 0,NODE_LOCAL, 2231 bytes)
17/01/13 22:58:10 INFO TaskSetManager: Starting task 1.0 in stage 32.0 (TID 50, localhost, partition 1,NODE_LOCAL, 2231 bytes)
17/01/13 22:58:10 INFO TaskSetManager: Starting task 2.0 in stage 32.0 (TID 51, localhost, partition 2,NODE_LOCAL, 2231 bytes)
17/01/13 22:58:10 INFO TaskSetManager: Starting task 3.0 in stage 32.0 (TID 52, localhost, partition 3,NODE_LOCAL, 2231 bytes)
17/01/13 22:58:10 INFO Executor: Running task 0.0 in stage 32.0 (TID 49)
17/01/13 22:58:10 INFO Executor: Running task 1.0 in stage 32.0 (TID 50)
17/01/13 22:58:10 INFO Executor: Running task 2.0 in stage 32.0 (TID 51)
17/01/13 22:58:10 INFO Executor: Running task 3.0 in stage 32.0 (TID 52)
17/01/13 22:58:10 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/01/13 22:58:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:58:10 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/01/13 22:58:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:58:10 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/01/13 22:58:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:58:10 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/01/13 22:58:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:58:10 INFO GenerateOrdering: Code generated in 8.084046 ms
17/01/13 22:58:11 INFO BlockManagerInfo: Removed broadcast_40_piece0 on localhost:63446 in memory (size: 5.3 KB, free: 483.1 MB)
17/01/13 22:58:11 INFO BlockManagerInfo: Removed broadcast_39_piece0 on localhost:63446 in memory (size: 11.7 KB, free: 483.1 MB)
17/01/13 22:58:11 INFO BlockManagerInfo: Removed broadcast_38_piece0 on localhost:63446 in memory (size: 10.4 KB, free: 483.1 MB)
17/01/13 22:58:11 INFO ContextCleaner: Cleaned accumulator 137
17/01/13 22:58:11 INFO BlockManagerInfo: Removed broadcast_37_piece0 on localhost:63446 in memory (size: 6.3 KB, free: 483.1 MB)
17/01/13 22:58:11 INFO ContextCleaner: Cleaned accumulator 120
17/01/13 22:58:11 INFO BlockManagerInfo: Removed broadcast_36_piece0 on localhost:63446 in memory (size: 11.4 KB, free: 483.1 MB)
17/01/13 22:58:11 INFO ContextCleaner: Cleaned accumulator 119
17/01/13 22:58:11 INFO GenerateMutableProjection: Code generated in 13.947295 ms
17/01/13 22:58:11 INFO GeneratePredicate: Code generated in 7.280634 ms
17/01/13 22:58:11 INFO Executor: Finished task 1.0 in stage 32.0 (TID 50). 1965 bytes result sent to driver
17/01/13 22:58:11 INFO TaskSetManager: Finished task 1.0 in stage 32.0 (TID 50) in 445 ms on localhost (1/4)
17/01/13 22:58:11 INFO Executor: Finished task 2.0 in stage 32.0 (TID 51). 1965 bytes result sent to driver
17/01/13 22:58:11 INFO TaskSetManager: Finished task 2.0 in stage 32.0 (TID 51) in 460 ms on localhost (2/4)
17/01/13 22:58:11 INFO Executor: Finished task 0.0 in stage 32.0 (TID 49). 1965 bytes result sent to driver
17/01/13 22:58:11 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 49) in 467 ms on localhost (3/4)
17/01/13 22:58:11 INFO Executor: Finished task 3.0 in stage 32.0 (TID 52). 1965 bytes result sent to driver
17/01/13 22:58:11 INFO TaskSetManager: Finished task 3.0 in stage 32.0 (TID 52) in 496 ms on localhost (4/4)
17/01/13 22:58:11 INFO DAGScheduler: ShuffleMapStage 32 (count at null:-2) finished in 0.497 s
17/01/13 22:58:11 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
17/01/13 22:58:11 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:58:11 INFO DAGScheduler: running: Set()
17/01/13 22:58:11 INFO DAGScheduler: waiting: Set(ResultStage 33)
17/01/13 22:58:11 INFO DAGScheduler: failed: Set()
17/01/13 22:58:11 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[136] at count at null:-2), which has no missing parents
17/01/13 22:58:11 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 15.8 KB, free 28.7 MB)
17/01/13 22:58:11 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 7.2 KB, free 28.7 MB)
17/01/13 22:58:11 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on localhost:63446 (size: 7.2 KB, free: 483.1 MB)
17/01/13 22:58:11 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1006
17/01/13 22:58:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[136] at count at null:-2)
17/01/13 22:58:11 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks
17/01/13 22:58:11 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 53, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 22:58:11 INFO Executor: Running task 0.0 in stage 33.0 (TID 53)
17/01/13 22:58:11 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/01/13 22:58:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:58:11 INFO Executor: Finished task 0.0 in stage 33.0 (TID 53). 2174 bytes result sent to driver
17/01/13 22:58:11 INFO DAGScheduler: ResultStage 33 (count at null:-2) finished in 0.011 s
17/01/13 22:58:11 INFO DAGScheduler: Job 21 finished: count at null:-2, took 1.733177 s
17/01/13 22:58:11 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 53) in 10 ms on localhost (1/1)
17/01/13 22:58:11 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool 
17/01/13 22:58:11 INFO ParseDriver: Parsing command: SELECT `playerID`, `yearID`, `teamID`, `G`, `AB`, `R`, `H`
FROM (SELECT `playerID`, `yearID`, `teamID`, `G`, `AB`, `R`, `H`, rank() OVER (PARTITION BY `playerID` ORDER BY `H` DESC) AS `zzz5`
FROM (SELECT *
FROM (SELECT `playerID` AS `playerID`, `yearID` AS `yearID`, `teamID` AS `teamID`, `G` AS `G`, `AB` AS `AB`, `R` AS `R`, `H` AS `H`
FROM `batting`) `gmnhksdlnr`
ORDER BY `playerID`, `yearID`, `teamID`) `ybhopmjxhh`) `xwpoayxgmf`
WHERE (`zzz5` <= 2.0 AND `H` > 0.0)
17/01/13 22:58:11 INFO ParseDriver: Parse Completed
17/01/13 22:58:11 INFO SparkContext: Starting job: count at null:-2
17/01/13 22:58:11 INFO DAGScheduler: Got job 22 (count at null:-2) with 2 output partitions
17/01/13 22:58:11 INFO DAGScheduler: Final stage: ResultStage 34 (count at null:-2)
17/01/13 22:58:11 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:58:11 INFO DAGScheduler: Missing parents: List()
17/01/13 22:58:11 INFO DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[142] at count at null:-2), which has no missing parents
17/01/13 22:58:11 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 26.4 KB, free 28.7 MB)
17/01/13 22:58:11 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 10.4 KB, free 28.7 MB)
17/01/13 22:58:11 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on localhost:63446 (size: 10.4 KB, free: 483.1 MB)
17/01/13 22:58:11 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1006
17/01/13 22:58:11 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 34 (MapPartitionsRDD[142] at count at null:-2)
17/01/13 22:58:11 INFO TaskSchedulerImpl: Adding task set 34.0 with 2 tasks
17/01/13 22:58:11 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 54, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 22:58:11 INFO TaskSetManager: Starting task 1.0 in stage 34.0 (TID 55, localhost, partition 1,PROCESS_LOCAL, 2481 bytes)
17/01/13 22:58:11 INFO Executor: Running task 0.0 in stage 34.0 (TID 54)
17/01/13 22:58:11 INFO Executor: Running task 1.0 in stage 34.0 (TID 55)
17/01/13 22:58:11 INFO BlockManager: Found block rdd_70_1 locally
17/01/13 22:58:11 INFO BlockManager: Found block rdd_70_0 locally
17/01/13 22:58:11 INFO Executor: Finished task 0.0 in stage 34.0 (TID 54). 13877 bytes result sent to driver
17/01/13 22:58:11 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 54) in 66 ms on localhost (1/2)
17/01/13 22:58:11 INFO Executor: Finished task 1.0 in stage 34.0 (TID 55). 13661 bytes result sent to driver
17/01/13 22:58:11 INFO DAGScheduler: ResultStage 34 (count at null:-2) finished in 0.073 s
17/01/13 22:58:11 INFO DAGScheduler: Job 22 finished: count at null:-2, took 0.079913 s
17/01/13 22:58:11 INFO TaskSetManager: Finished task 1.0 in stage 34.0 (TID 55) in 73 ms on localhost (2/2)
17/01/13 22:58:11 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool 
17/01/13 22:58:11 INFO SparkContext: Starting job: count at null:-2
17/01/13 22:58:11 INFO DAGScheduler: Registering RDD 143 (count at null:-2)
17/01/13 22:58:11 INFO DAGScheduler: Registering RDD 147 (count at null:-2)
17/01/13 22:58:11 INFO DAGScheduler: Registering RDD 154 (count at null:-2)
17/01/13 22:58:11 INFO DAGScheduler: Got job 23 (count at null:-2) with 1 output partitions
17/01/13 22:58:11 INFO DAGScheduler: Final stage: ResultStage 38 (count at null:-2)
17/01/13 22:58:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 37)
17/01/13 22:58:11 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 37)
17/01/13 22:58:11 INFO DAGScheduler: Submitting ShuffleMapStage 35 (MapPartitionsRDD[143] at count at null:-2), which has no missing parents
17/01/13 22:58:11 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 28.9 KB, free 28.7 MB)
17/01/13 22:58:11 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 11.7 KB, free 28.8 MB)
17/01/13 22:58:11 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on localhost:63446 (size: 11.7 KB, free: 483.1 MB)
17/01/13 22:58:11 INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1006
17/01/13 22:58:11 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 35 (MapPartitionsRDD[143] at count at null:-2)
17/01/13 22:58:11 INFO TaskSchedulerImpl: Adding task set 35.0 with 2 tasks
17/01/13 22:58:11 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 56, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:58:11 INFO TaskSetManager: Starting task 1.0 in stage 35.0 (TID 57, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:58:11 INFO Executor: Running task 0.0 in stage 35.0 (TID 56)
17/01/13 22:58:11 INFO Executor: Running task 1.0 in stage 35.0 (TID 57)
17/01/13 22:58:11 INFO BlockManager: Found block rdd_70_0 locally
17/01/13 22:58:11 INFO BlockManager: Found block rdd_70_1 locally
17/01/13 22:58:11 INFO Executor: Finished task 0.0 in stage 35.0 (TID 56). 2580 bytes result sent to driver
17/01/13 22:58:11 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 56) in 205 ms on localhost (1/2)
17/01/13 22:58:11 INFO Executor: Finished task 1.0 in stage 35.0 (TID 57). 2580 bytes result sent to driver
17/01/13 22:58:11 INFO DAGScheduler: ShuffleMapStage 35 (count at null:-2) finished in 0.236 s
17/01/13 22:58:11 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:58:11 INFO TaskSetManager: Finished task 1.0 in stage 35.0 (TID 57) in 234 ms on localhost (2/2)
17/01/13 22:58:11 INFO DAGScheduler: running: Set()
17/01/13 22:58:11 INFO DAGScheduler: waiting: Set(ShuffleMapStage 37, ResultStage 38, ShuffleMapStage 36)
17/01/13 22:58:11 INFO DAGScheduler: failed: Set()
17/01/13 22:58:11 INFO DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[147] at count at null:-2), which has no missing parents
17/01/13 22:58:11 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool 
17/01/13 22:58:11 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 10.3 KB, free 28.8 MB)
17/01/13 22:58:11 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 5.3 KB, free 28.8 MB)
17/01/13 22:58:11 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on localhost:63446 (size: 5.3 KB, free: 483.1 MB)
17/01/13 22:58:11 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1006
17/01/13 22:58:11 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[147] at count at null:-2)
17/01/13 22:58:11 INFO TaskSchedulerImpl: Adding task set 36.0 with 4 tasks
17/01/13 22:58:11 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 58, localhost, partition 0,NODE_LOCAL, 2231 bytes)
17/01/13 22:58:11 INFO TaskSetManager: Starting task 1.0 in stage 36.0 (TID 59, localhost, partition 1,NODE_LOCAL, 2231 bytes)
17/01/13 22:58:11 INFO TaskSetManager: Starting task 2.0 in stage 36.0 (TID 60, localhost, partition 2,NODE_LOCAL, 2231 bytes)
17/01/13 22:58:11 INFO TaskSetManager: Starting task 3.0 in stage 36.0 (TID 61, localhost, partition 3,NODE_LOCAL, 2231 bytes)
17/01/13 22:58:11 INFO Executor: Running task 0.0 in stage 36.0 (TID 58)
17/01/13 22:58:11 INFO Executor: Running task 2.0 in stage 36.0 (TID 60)
17/01/13 22:58:11 INFO Executor: Running task 3.0 in stage 36.0 (TID 61)
17/01/13 22:58:11 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:58:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/01/13 22:58:11 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:58:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:58:11 INFO Executor: Running task 1.0 in stage 36.0 (TID 59)
17/01/13 22:58:11 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:58:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/01/13 22:58:11 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:58:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:58:12 INFO Executor: Finished task 0.0 in stage 36.0 (TID 58). 1660 bytes result sent to driver
17/01/13 22:58:12 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 58) in 259 ms on localhost (1/4)
17/01/13 22:58:12 INFO Executor: Finished task 3.0 in stage 36.0 (TID 61). 1660 bytes result sent to driver
17/01/13 22:58:12 INFO TaskSetManager: Finished task 3.0 in stage 36.0 (TID 61) in 291 ms on localhost (2/4)
17/01/13 22:58:12 INFO Executor: Finished task 2.0 in stage 36.0 (TID 60). 1660 bytes result sent to driver
17/01/13 22:58:12 INFO TaskSetManager: Finished task 2.0 in stage 36.0 (TID 60) in 294 ms on localhost (3/4)
17/01/13 22:58:12 INFO Executor: Finished task 1.0 in stage 36.0 (TID 59). 1660 bytes result sent to driver
17/01/13 22:58:12 INFO TaskSetManager: Finished task 1.0 in stage 36.0 (TID 59) in 340 ms on localhost (4/4)
17/01/13 22:58:12 INFO DAGScheduler: ShuffleMapStage 36 (count at null:-2) finished in 0.340 s
17/01/13 22:58:12 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool 
17/01/13 22:58:12 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:58:12 INFO DAGScheduler: running: Set()
17/01/13 22:58:12 INFO DAGScheduler: waiting: Set(ShuffleMapStage 37, ResultStage 38)
17/01/13 22:58:12 INFO DAGScheduler: failed: Set()
17/01/13 22:58:12 INFO DAGScheduler: Submitting ShuffleMapStage 37 (MapPartitionsRDD[154] at count at null:-2), which has no missing parents
17/01/13 22:58:12 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 17.9 KB, free 28.8 MB)
17/01/13 22:58:12 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 8.1 KB, free 28.8 MB)
17/01/13 22:58:12 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on localhost:63446 (size: 8.1 KB, free: 483.1 MB)
17/01/13 22:58:12 INFO SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1006
17/01/13 22:58:12 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 37 (MapPartitionsRDD[154] at count at null:-2)
17/01/13 22:58:12 INFO TaskSchedulerImpl: Adding task set 37.0 with 4 tasks
17/01/13 22:58:12 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 62, localhost, partition 0,NODE_LOCAL, 2231 bytes)
17/01/13 22:58:12 INFO TaskSetManager: Starting task 1.0 in stage 37.0 (TID 63, localhost, partition 1,NODE_LOCAL, 2231 bytes)
17/01/13 22:58:12 INFO TaskSetManager: Starting task 2.0 in stage 37.0 (TID 64, localhost, partition 2,NODE_LOCAL, 2231 bytes)
17/01/13 22:58:12 INFO TaskSetManager: Starting task 3.0 in stage 37.0 (TID 65, localhost, partition 3,NODE_LOCAL, 2231 bytes)
17/01/13 22:58:12 INFO Executor: Running task 0.0 in stage 37.0 (TID 62)
17/01/13 22:58:12 INFO Executor: Running task 1.0 in stage 37.0 (TID 63)
17/01/13 22:58:12 INFO Executor: Running task 2.0 in stage 37.0 (TID 64)
17/01/13 22:58:12 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/01/13 22:58:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:58:12 INFO Executor: Running task 3.0 in stage 37.0 (TID 65)
17/01/13 22:58:12 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/01/13 22:58:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:58:12 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/01/13 22:58:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:58:12 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/01/13 22:58:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/01/13 22:58:12 INFO BlockManagerInfo: Removed broadcast_45_piece0 on localhost:63446 in memory (size: 5.3 KB, free: 483.1 MB)
17/01/13 22:58:12 INFO BlockManagerInfo: Removed broadcast_42_piece0 on localhost:63446 in memory (size: 7.2 KB, free: 483.1 MB)
17/01/13 22:58:12 INFO Executor: Finished task 3.0 in stage 37.0 (TID 65). 1965 bytes result sent to driver
17/01/13 22:58:12 INFO TaskSetManager: Finished task 3.0 in stage 37.0 (TID 65) in 144 ms on localhost (1/4)
17/01/13 22:58:12 INFO ContextCleaner: Cleaned accumulator 141
17/01/13 22:58:12 INFO BlockManagerInfo: Removed broadcast_44_piece0 on localhost:63446 in memory (size: 11.7 KB, free: 483.1 MB)
17/01/13 22:58:12 INFO BlockManagerInfo: Removed broadcast_43_piece0 on localhost:63446 in memory (size: 10.4 KB, free: 483.1 MB)
17/01/13 22:58:12 INFO ContextCleaner: Cleaned accumulator 158
17/01/13 22:58:12 INFO Executor: Finished task 2.0 in stage 37.0 (TID 64). 1965 bytes result sent to driver
17/01/13 22:58:12 INFO Executor: Finished task 1.0 in stage 37.0 (TID 63). 1965 bytes result sent to driver
17/01/13 22:58:12 INFO TaskSetManager: Finished task 2.0 in stage 37.0 (TID 64) in 166 ms on localhost (2/4)
17/01/13 22:58:12 INFO TaskSetManager: Finished task 1.0 in stage 37.0 (TID 63) in 169 ms on localhost (3/4)
17/01/13 22:58:12 INFO Executor: Finished task 0.0 in stage 37.0 (TID 62). 1965 bytes result sent to driver
17/01/13 22:58:12 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 62) in 170 ms on localhost (4/4)
17/01/13 22:58:12 INFO DAGScheduler: ShuffleMapStage 37 (count at null:-2) finished in 0.171 s
17/01/13 22:58:12 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:58:12 INFO DAGScheduler: running: Set()
17/01/13 22:58:12 INFO DAGScheduler: waiting: Set(ResultStage 38)
17/01/13 22:58:12 INFO DAGScheduler: failed: Set()
17/01/13 22:58:12 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool 
17/01/13 22:58:12 INFO DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[157] at count at null:-2), which has no missing parents
17/01/13 22:58:12 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 15.8 KB, free 28.7 MB)
17/01/13 22:58:12 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 7.2 KB, free 28.7 MB)
17/01/13 22:58:12 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on localhost:63446 (size: 7.2 KB, free: 483.1 MB)
17/01/13 22:58:12 INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1006
17/01/13 22:58:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 38 (MapPartitionsRDD[157] at count at null:-2)
17/01/13 22:58:12 INFO TaskSchedulerImpl: Adding task set 38.0 with 1 tasks
17/01/13 22:58:12 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 66, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 22:58:12 INFO Executor: Running task 0.0 in stage 38.0 (TID 66)
17/01/13 22:58:12 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/01/13 22:58:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/01/13 22:58:12 INFO Executor: Finished task 0.0 in stage 38.0 (TID 66). 2174 bytes result sent to driver
17/01/13 22:58:12 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 66) in 9 ms on localhost (1/1)
17/01/13 22:58:12 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool 
17/01/13 22:58:12 INFO DAGScheduler: ResultStage 38 (count at null:-2) finished in 0.009 s
17/01/13 22:58:12 INFO DAGScheduler: Job 23 finished: count at null:-2, took 0.774841 s
17/01/13 22:58:12 INFO ParseDriver: Parsing command: SELECT *
FROM (SELECT `playerID`, `yearID`, `teamID`, `G`, `AB`, `R`, `H`
FROM (SELECT `playerID`, `yearID`, `teamID`, `G`, `AB`, `R`, `H`, rank() OVER (PARTITION BY `playerID` ORDER BY `H` DESC) AS `zzz6`
FROM (SELECT *
FROM (SELECT `playerID` AS `playerID`, `yearID` AS `yearID`, `teamID` AS `teamID`, `G` AS `G`, `AB` AS `AB`, `R` AS `R`, `H` AS `H`
FROM `batting`) `ydqwopcmxt`
ORDER BY `playerID`, `yearID`, `teamID`) `pzvzamcqog`) `dobejsgoxg`
WHERE (`zzz6` <= 2.0 AND `H` > 0.0)) `whuawbqebu`
LIMIT 10
17/01/13 22:58:12 INFO ParseDriver: Parse Completed
17/01/13 22:58:12 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 22:58:12 INFO DAGScheduler: Got job 24 (collect at utils.scala:195) with 2 output partitions
17/01/13 22:58:12 INFO DAGScheduler: Final stage: ResultStage 39 (collect at utils.scala:195)
17/01/13 22:58:12 INFO DAGScheduler: Parents of final stage: List()
17/01/13 22:58:12 INFO DAGScheduler: Missing parents: List()
17/01/13 22:58:12 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[163] at collect at utils.scala:195), which has no missing parents
17/01/13 22:58:12 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 26.4 KB, free 28.7 MB)
17/01/13 22:58:12 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 10.4 KB, free 28.7 MB)
17/01/13 22:58:12 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on localhost:63446 (size: 10.4 KB, free: 483.1 MB)
17/01/13 22:58:12 INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1006
17/01/13 22:58:12 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 39 (MapPartitionsRDD[163] at collect at utils.scala:195)
17/01/13 22:58:12 INFO TaskSchedulerImpl: Adding task set 39.0 with 2 tasks
17/01/13 22:58:12 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 67, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 22:58:12 INFO TaskSetManager: Starting task 1.0 in stage 39.0 (TID 68, localhost, partition 1,PROCESS_LOCAL, 2481 bytes)
17/01/13 22:58:12 INFO Executor: Running task 1.0 in stage 39.0 (TID 68)
17/01/13 22:58:12 INFO Executor: Running task 0.0 in stage 39.0 (TID 67)
17/01/13 22:58:12 INFO BlockManager: Found block rdd_70_0 locally
17/01/13 22:58:12 INFO BlockManager: Found block rdd_70_1 locally
17/01/13 22:58:12 INFO Executor: Finished task 1.0 in stage 39.0 (TID 68). 13625 bytes result sent to driver
17/01/13 22:58:12 INFO TaskSetManager: Finished task 1.0 in stage 39.0 (TID 68) in 43 ms on localhost (1/2)
17/01/13 22:58:12 INFO Executor: Finished task 0.0 in stage 39.0 (TID 67). 13904 bytes result sent to driver
17/01/13 22:58:12 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 67) in 49 ms on localhost (2/2)
17/01/13 22:58:12 INFO DAGScheduler: ResultStage 39 (collect at utils.scala:195) finished in 0.050 s
17/01/13 22:58:12 INFO DAGScheduler: Job 24 finished: collect at utils.scala:195, took 0.054851 s
17/01/13 22:58:12 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool 
17/01/13 22:58:12 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 22:58:12 INFO DAGScheduler: Registering RDD 164 (collect at utils.scala:195)
17/01/13 22:58:12 INFO DAGScheduler: Registering RDD 168 (collect at utils.scala:195)
17/01/13 22:58:12 INFO DAGScheduler: Got job 25 (collect at utils.scala:195) with 1 output partitions
17/01/13 22:58:12 INFO DAGScheduler: Final stage: ResultStage 42 (collect at utils.scala:195)
17/01/13 22:58:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 41)
17/01/13 22:58:12 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 41)
17/01/13 22:58:12 INFO DAGScheduler: Submitting ShuffleMapStage 40 (MapPartitionsRDD[164] at collect at utils.scala:195), which has no missing parents
17/01/13 22:58:12 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 28.9 KB, free 28.8 MB)
17/01/13 22:58:12 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 11.7 KB, free 28.8 MB)
17/01/13 22:58:12 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on localhost:63446 (size: 11.7 KB, free: 483.1 MB)
17/01/13 22:58:12 INFO SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1006
17/01/13 22:58:12 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 40 (MapPartitionsRDD[164] at collect at utils.scala:195)
17/01/13 22:58:12 INFO TaskSchedulerImpl: Adding task set 40.0 with 2 tasks
17/01/13 22:58:12 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 69, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:58:12 INFO TaskSetManager: Starting task 1.0 in stage 40.0 (TID 70, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 22:58:12 INFO Executor: Running task 0.0 in stage 40.0 (TID 69)
17/01/13 22:58:12 INFO Executor: Running task 1.0 in stage 40.0 (TID 70)
17/01/13 22:58:12 INFO BlockManager: Found block rdd_70_0 locally
17/01/13 22:58:12 INFO BlockManager: Found block rdd_70_1 locally
17/01/13 22:58:12 INFO Executor: Finished task 0.0 in stage 40.0 (TID 69). 2580 bytes result sent to driver
17/01/13 22:58:12 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 69) in 179 ms on localhost (1/2)
17/01/13 22:58:12 INFO Executor: Finished task 1.0 in stage 40.0 (TID 70). 2580 bytes result sent to driver
17/01/13 22:58:12 INFO TaskSetManager: Finished task 1.0 in stage 40.0 (TID 70) in 206 ms on localhost (2/2)
17/01/13 22:58:12 INFO DAGScheduler: ShuffleMapStage 40 (collect at utils.scala:195) finished in 0.207 s
17/01/13 22:58:12 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool 
17/01/13 22:58:12 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:58:12 INFO DAGScheduler: running: Set()
17/01/13 22:58:12 INFO DAGScheduler: waiting: Set(ResultStage 42, ShuffleMapStage 41)
17/01/13 22:58:12 INFO DAGScheduler: failed: Set()
17/01/13 22:58:12 INFO DAGScheduler: Submitting ShuffleMapStage 41 (MapPartitionsRDD[168] at collect at utils.scala:195), which has no missing parents
17/01/13 22:58:12 INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 10.3 KB, free 28.8 MB)
17/01/13 22:58:12 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 5.3 KB, free 28.8 MB)
17/01/13 22:58:12 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on localhost:63446 (size: 5.3 KB, free: 483.1 MB)
17/01/13 22:58:12 INFO SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:1006
17/01/13 22:58:12 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 41 (MapPartitionsRDD[168] at collect at utils.scala:195)
17/01/13 22:58:12 INFO TaskSchedulerImpl: Adding task set 41.0 with 4 tasks
17/01/13 22:58:12 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 71, localhost, partition 0,NODE_LOCAL, 2231 bytes)
17/01/13 22:58:12 INFO TaskSetManager: Starting task 1.0 in stage 41.0 (TID 72, localhost, partition 1,NODE_LOCAL, 2231 bytes)
17/01/13 22:58:12 INFO TaskSetManager: Starting task 2.0 in stage 41.0 (TID 73, localhost, partition 2,NODE_LOCAL, 2231 bytes)
17/01/13 22:58:12 INFO TaskSetManager: Starting task 3.0 in stage 41.0 (TID 74, localhost, partition 3,NODE_LOCAL, 2231 bytes)
17/01/13 22:58:12 INFO Executor: Running task 0.0 in stage 41.0 (TID 71)
17/01/13 22:58:12 INFO Executor: Running task 3.0 in stage 41.0 (TID 74)
17/01/13 22:58:12 INFO Executor: Running task 2.0 in stage 41.0 (TID 73)
17/01/13 22:58:12 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:58:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:58:12 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:58:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/01/13 22:58:12 INFO Executor: Running task 1.0 in stage 41.0 (TID 72)
17/01/13 22:58:12 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:58:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:58:12 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 22:58:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
17/01/13 22:58:13 INFO Executor: Finished task 2.0 in stage 41.0 (TID 73). 1660 bytes result sent to driver
17/01/13 22:58:13 INFO TaskSetManager: Finished task 2.0 in stage 41.0 (TID 73) in 165 ms on localhost (1/4)
17/01/13 22:58:13 INFO Executor: Finished task 0.0 in stage 41.0 (TID 71). 1660 bytes result sent to driver
17/01/13 22:58:13 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 71) in 181 ms on localhost (2/4)
17/01/13 22:58:13 INFO Executor: Finished task 3.0 in stage 41.0 (TID 74). 1660 bytes result sent to driver
17/01/13 22:58:13 INFO TaskSetManager: Finished task 3.0 in stage 41.0 (TID 74) in 194 ms on localhost (3/4)
17/01/13 22:58:13 INFO Executor: Finished task 1.0 in stage 41.0 (TID 72). 1660 bytes result sent to driver
17/01/13 22:58:13 INFO DAGScheduler: ShuffleMapStage 41 (collect at utils.scala:195) finished in 0.209 s
17/01/13 22:58:13 INFO TaskSetManager: Finished task 1.0 in stage 41.0 (TID 72) in 208 ms on localhost (4/4)
17/01/13 22:58:13 INFO DAGScheduler: looking for newly runnable stages
17/01/13 22:58:13 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool 
17/01/13 22:58:13 INFO DAGScheduler: running: Set()
17/01/13 22:58:13 INFO DAGScheduler: waiting: Set(ResultStage 42)
17/01/13 22:58:13 INFO DAGScheduler: failed: Set()
17/01/13 22:58:13 INFO DAGScheduler: Submitting ResultStage 42 (MapPartitionsRDD[175] at collect at utils.scala:195), which has no missing parents
17/01/13 22:58:13 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 15.6 KB, free 28.8 MB)
17/01/13 22:58:13 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 7.2 KB, free 28.8 MB)
17/01/13 22:58:13 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on localhost:63446 (size: 7.2 KB, free: 483.1 MB)
17/01/13 22:58:13 INFO SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1006
17/01/13 22:58:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (MapPartitionsRDD[175] at collect at utils.scala:195)
17/01/13 22:58:13 INFO TaskSchedulerImpl: Adding task set 42.0 with 1 tasks
17/01/13 22:58:13 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 75, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 22:58:13 INFO Executor: Running task 0.0 in stage 42.0 (TID 75)
17/01/13 22:58:13 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/01/13 22:58:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 22:58:13 INFO Executor: Finished task 0.0 in stage 42.0 (TID 75). 2884 bytes result sent to driver
17/01/13 22:58:13 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 75) in 29 ms on localhost (1/1)
17/01/13 22:58:13 INFO DAGScheduler: ResultStage 42 (collect at utils.scala:195) finished in 0.029 s
17/01/13 22:58:13 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool 
17/01/13 22:58:13 INFO DAGScheduler: Job 25 finished: collect at utils.scala:195, took 0.460181 s
17/01/13 22:58:13 INFO ParseDriver: Parsing command: SELECT * FROM iris WHERE Species = 'virginica' ORDER BY Sepal.Length LIMIT 10
17/01/13 22:58:13 INFO ParseDriver: Parse Completed
17/01/13 22:58:14 INFO SparkContext: Invoking stop() from shutdown hook
17/01/13 22:58:14 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
17/01/13 22:58:14 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/01/13 22:58:14 INFO MemoryStore: MemoryStore cleared
17/01/13 22:58:14 INFO BlockManager: BlockManager stopped
17/01/13 22:58:14 INFO BlockManagerMaster: BlockManagerMaster stopped
17/01/13 22:58:14 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/01/13 22:58:14 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\Baoco\AppData\Local\Temp\spark-124892fd-52e1-4df7-9712-e85eb2df212d\userFiles-241599d2-58eb-4122-9c12-6b033c2d3a34
java.io.IOException: Failed to delete: C:\Users\Baoco\AppData\Local\Temp\spark-124892fd-52e1-4df7-9712-e85eb2df212d\userFiles-241599d2-58eb-4122-9c12-6b033c2d3a34
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:119)
	at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1219)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1755)
	at org.apache.spark.SparkContext$$anonfun$3.apply$mcV$sp(SparkContext.scala:596)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/01/13 22:58:14 INFO SparkContext: Successfully stopped SparkContext
17/01/13 22:58:14 INFO ShutdownHookManager: Shutdown hook called
17/01/13 22:58:14 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
17/01/13 22:58:14 INFO ShutdownHookManager: Deleting directory C:\Users\Baoco\AppData\Local\Temp\spark-124892fd-52e1-4df7-9712-e85eb2df212d\userFiles-241599d2-58eb-4122-9c12-6b033c2d3a34
17/01/13 22:58:14 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
17/01/13 22:58:14 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\Baoco\AppData\Local\Temp\spark-124892fd-52e1-4df7-9712-e85eb2df212d\userFiles-241599d2-58eb-4122-9c12-6b033c2d3a34
java.io.IOException: Failed to delete: C:\Users\Baoco\AppData\Local\Temp\spark-124892fd-52e1-4df7-9712-e85eb2df212d\userFiles-241599d2-58eb-4122-9c12-6b033c2d3a34
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/01/13 22:58:14 INFO ShutdownHookManager: Deleting directory C:\Users\Baoco\AppData\Local\Temp\spark-124892fd-52e1-4df7-9712-e85eb2df212d\httpd-1aa40b97-6cf8-4abf-9d8c-97ec1cf43c00
17/01/13 22:58:14 INFO ShutdownHookManager: Deleting directory C:\Users\Baoco\AppData\Local\Temp\spark-111a646c-79ef-46c2-b68f-e34bcce7f3f9
17/01/13 22:58:14 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
17/01/13 22:58:14 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\Baoco\AppData\Local\Temp\spark-111a646c-79ef-46c2-b68f-e34bcce7f3f9
java.io.IOException: Failed to delete: C:\Users\Baoco\AppData\Local\Temp\spark-111a646c-79ef-46c2-b68f-e34bcce7f3f9
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/01/13 22:58:14 INFO ShutdownHookManager: Deleting directory C:\Users\Baoco\AppData\Local\Temp\spark-124892fd-52e1-4df7-9712-e85eb2df212d
17/01/13 22:58:14 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\Baoco\AppData\Local\Temp\spark-124892fd-52e1-4df7-9712-e85eb2df212d
java.io.IOException: Failed to delete: C:\Users\Baoco\AppData\Local\Temp\spark-124892fd-52e1-4df7-9712-e85eb2df212d
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/01/13 23:02:19 INFO SparkContext: Running Spark version 1.6.2
17/01/13 23:02:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/01/13 23:02:19 INFO SecurityManager: Changing view acls to: Baoco
17/01/13 23:02:19 INFO SecurityManager: Changing modify acls to: Baoco
17/01/13 23:02:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(Baoco); users with modify permissions: Set(Baoco)
17/01/13 23:02:20 INFO Utils: Successfully started service 'sparkDriver' on port 63553.
17/01/13 23:02:20 INFO Slf4jLogger: Slf4jLogger started
17/01/13 23:02:20 INFO Remoting: Starting remoting
17/01/13 23:02:20 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@127.0.0.1:63566]
17/01/13 23:02:20 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 63566.
17/01/13 23:02:20 INFO SparkEnv: Registering MapOutputTracker
17/01/13 23:02:20 INFO SparkEnv: Registering BlockManagerMaster
17/01/13 23:02:20 INFO DiskBlockManager: Created local directory at C:\Users\Baoco\AppData\Local\Temp\blockmgr-ccf9773f-e192-460e-8039-be4b3dd2b96d
17/01/13 23:02:20 INFO MemoryStore: MemoryStore started with capacity 511.1 MB
17/01/13 23:02:20 INFO SparkEnv: Registering OutputCommitCoordinator
17/01/13 23:02:21 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/01/13 23:02:21 INFO SparkUI: Started SparkUI at http://127.0.0.1:4040
17/01/13 23:02:21 INFO HttpFileServer: HTTP File server directory is C:\Users\Baoco\AppData\Local\Temp\spark-2832be6c-3ac9-44da-bbe7-34543b51c4dd\httpd-136dac86-a29b-4589-80a1-a344001c5f98
17/01/13 23:02:21 INFO HttpServer: Starting HTTP Server
17/01/13 23:02:21 INFO Utils: Successfully started service 'HTTP file server' on port 63569.
17/01/13 23:02:21 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/spark-csv_2.11-1.3.0.jar at http://127.0.0.1:63569/jars/spark-csv_2.11-1.3.0.jar with timestamp 1484377341251
17/01/13 23:02:21 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/commons-csv-1.1.jar at http://127.0.0.1:63569/jars/commons-csv-1.1.jar with timestamp 1484377341254
17/01/13 23:02:21 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/univocity-parsers-1.5.1.jar at http://127.0.0.1:63569/jars/univocity-parsers-1.5.1.jar with timestamp 1484377341256
17/01/13 23:02:21 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.2/library/sparklyr/java/sparklyr-1.6-2.10.jar at http://127.0.0.1:63569/jars/sparklyr-1.6-2.10.jar with timestamp 1484377341258
17/01/13 23:02:21 INFO Executor: Starting executor ID driver on host localhost
17/01/13 23:02:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63586.
17/01/13 23:02:21 INFO NettyBlockTransferService: Server created on 63586
17/01/13 23:02:21 INFO BlockManagerMaster: Trying to register BlockManager
17/01/13 23:02:21 INFO BlockManagerMasterEndpoint: Registering block manager localhost:63586 with 511.1 MB RAM, BlockManagerId(driver, localhost, 63586)
17/01/13 23:02:21 INFO BlockManagerMaster: Registered BlockManager
17/01/13 23:02:22 INFO HiveContext: Initializing execution hive, version 1.2.1
17/01/13 23:02:22 INFO ClientWrapper: Inspected Hadoop version: 2.6.0
17/01/13 23:02:22 INFO ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
17/01/13 23:02:22 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/01/13 23:02:22 INFO ObjectStore: ObjectStore, initialize called
17/01/13 23:02:22 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/01/13 23:02:22 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/01/13 23:02:23 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/01/13 23:02:23 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/01/13 23:02:30 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/01/13 23:02:31 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 23:02:31 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 23:02:37 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 23:02:37 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 23:02:39 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/01/13 23:02:39 INFO ObjectStore: Initialized ObjectStore
17/01/13 23:02:39 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/01/13 23:02:40 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/01/13 23:02:45 INFO HiveMetaStore: Added admin role in metastore
17/01/13 23:02:45 INFO HiveMetaStore: Added public role in metastore
17/01/13 23:02:45 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/01/13 23:02:45 INFO HiveMetaStore: 0: get_all_databases
17/01/13 23:02:45 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_all_databases	
17/01/13 23:02:45 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/01/13 23:02:45 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/01/13 23:02:45 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 23:02:46 INFO SessionState: Created local directory: C:/Users/Baoco/AppData/Local/Temp/ad5af00f-5fcf-4666-8558-f0d794344d1e_resources
17/01/13 23:02:47 INFO SessionState: Created HDFS directory: C:/Users/Baoco/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/Baoco/ad5af00f-5fcf-4666-8558-f0d794344d1e
17/01/13 23:02:47 INFO SessionState: Created local directory: C:/Users/Baoco/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/ad5af00f-5fcf-4666-8558-f0d794344d1e
17/01/13 23:02:47 INFO SessionState: Created HDFS directory: C:/Users/Baoco/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/Baoco/ad5af00f-5fcf-4666-8558-f0d794344d1e/_tmp_space.db
17/01/13 23:02:47 INFO HiveContext: default warehouse location is C:\Users\Baoco\AppData\Local\rstudio\spark\Cache\spark-1.6.2-bin-hadoop2.6\tmp\hive
17/01/13 23:02:47 INFO HiveContext: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/01/13 23:02:47 INFO ClientWrapper: Inspected Hadoop version: 2.6.0
17/01/13 23:02:47 INFO ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
17/01/13 23:02:47 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/01/13 23:02:47 INFO ObjectStore: ObjectStore, initialize called
17/01/13 23:02:47 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/01/13 23:02:47 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/01/13 23:02:47 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/01/13 23:02:48 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/01/13 23:02:49 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/01/13 23:02:49 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 23:02:49 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 23:02:50 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 23:02:50 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 23:02:50 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/01/13 23:02:50 INFO ObjectStore: Initialized ObjectStore
17/01/13 23:02:50 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/01/13 23:02:50 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/01/13 23:02:50 INFO HiveMetaStore: Added admin role in metastore
17/01/13 23:02:50 INFO HiveMetaStore: Added public role in metastore
17/01/13 23:02:50 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/01/13 23:02:51 INFO HiveMetaStore: 0: get_all_databases
17/01/13 23:02:51 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_all_databases	
17/01/13 23:02:51 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/01/13 23:02:51 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/01/13 23:02:51 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/01/13 23:02:51 INFO SessionState: Created local directory: C:/Users/Baoco/AppData/Local/Temp/2f159a99-40b5-40ad-877a-f42a1dd3cfe3_resources
17/01/13 23:02:51 INFO SessionState: Created HDFS directory: C:/Users/Baoco/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/Baoco/2f159a99-40b5-40ad-877a-f42a1dd3cfe3
17/01/13 23:02:51 INFO SessionState: Created local directory: C:/Users/Baoco/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/2f159a99-40b5-40ad-877a-f42a1dd3cfe3
17/01/13 23:02:51 INFO SessionState: Created HDFS directory: C:/Users/Baoco/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/Baoco/2f159a99-40b5-40ad-877a-f42a1dd3cfe3/_tmp_space.db
17/01/13 23:03:01 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/01/13 23:03:01 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/01/13 23:03:01 INFO SparkContext: Starting job: collect at utils.scala:59
17/01/13 23:03:01 INFO DAGScheduler: Got job 0 (collect at utils.scala:59) with 1 output partitions
17/01/13 23:03:01 INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:59)
17/01/13 23:03:01 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:01 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:02 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at map at utils.scala:56), which has no missing parents
17/01/13 23:03:02 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 5.4 KB, free 5.4 KB)
17/01/13 23:03:02 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.0 KB, free 8.4 KB)
17/01/13 23:03:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:63586 (size: 3.0 KB, free: 511.1 MB)
17/01/13 23:03:02 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at map at utils.scala:56)
17/01/13 23:03:02 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
17/01/13 23:03:02 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2348 bytes)
17/01/13 23:03:02 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/01/13 23:03:02 INFO Executor: Fetching http://127.0.0.1:63569/jars/spark-csv_2.11-1.3.0.jar with timestamp 1484377341251
17/01/13 23:03:02 INFO Utils: Fetching http://127.0.0.1:63569/jars/spark-csv_2.11-1.3.0.jar to C:\Users\Baoco\AppData\Local\Temp\spark-2832be6c-3ac9-44da-bbe7-34543b51c4dd\userFiles-f0f5ea2d-5cc0-415b-9d63-6baf82062343\fetchFileTemp8986063875276247463.tmp
17/01/13 23:03:02 INFO Executor: Adding file:/C:/Users/Baoco/AppData/Local/Temp/spark-2832be6c-3ac9-44da-bbe7-34543b51c4dd/userFiles-f0f5ea2d-5cc0-415b-9d63-6baf82062343/spark-csv_2.11-1.3.0.jar to class loader
17/01/13 23:03:02 INFO Executor: Fetching http://127.0.0.1:63569/jars/sparklyr-1.6-2.10.jar with timestamp 1484377341258
17/01/13 23:03:02 INFO Utils: Fetching http://127.0.0.1:63569/jars/sparklyr-1.6-2.10.jar to C:\Users\Baoco\AppData\Local\Temp\spark-2832be6c-3ac9-44da-bbe7-34543b51c4dd\userFiles-f0f5ea2d-5cc0-415b-9d63-6baf82062343\fetchFileTemp615091554496120591.tmp
17/01/13 23:03:02 INFO Executor: Adding file:/C:/Users/Baoco/AppData/Local/Temp/spark-2832be6c-3ac9-44da-bbe7-34543b51c4dd/userFiles-f0f5ea2d-5cc0-415b-9d63-6baf82062343/sparklyr-1.6-2.10.jar to class loader
17/01/13 23:03:02 INFO Executor: Fetching http://127.0.0.1:63569/jars/univocity-parsers-1.5.1.jar with timestamp 1484377341256
17/01/13 23:03:02 INFO Utils: Fetching http://127.0.0.1:63569/jars/univocity-parsers-1.5.1.jar to C:\Users\Baoco\AppData\Local\Temp\spark-2832be6c-3ac9-44da-bbe7-34543b51c4dd\userFiles-f0f5ea2d-5cc0-415b-9d63-6baf82062343\fetchFileTemp370487574115802124.tmp
17/01/13 23:03:02 INFO Executor: Adding file:/C:/Users/Baoco/AppData/Local/Temp/spark-2832be6c-3ac9-44da-bbe7-34543b51c4dd/userFiles-f0f5ea2d-5cc0-415b-9d63-6baf82062343/univocity-parsers-1.5.1.jar to class loader
17/01/13 23:03:02 INFO Executor: Fetching http://127.0.0.1:63569/jars/commons-csv-1.1.jar with timestamp 1484377341254
17/01/13 23:03:02 INFO Utils: Fetching http://127.0.0.1:63569/jars/commons-csv-1.1.jar to C:\Users\Baoco\AppData\Local\Temp\spark-2832be6c-3ac9-44da-bbe7-34543b51c4dd\userFiles-f0f5ea2d-5cc0-415b-9d63-6baf82062343\fetchFileTemp2938366682325789015.tmp
17/01/13 23:03:02 INFO Executor: Adding file:/C:/Users/Baoco/AppData/Local/Temp/spark-2832be6c-3ac9-44da-bbe7-34543b51c4dd/userFiles-f0f5ea2d-5cc0-415b-9d63-6baf82062343/commons-csv-1.1.jar to class loader
17/01/13 23:03:02 INFO GenerateUnsafeProjection: Code generated in 168.29895 ms
17/01/13 23:03:02 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1060 bytes result sent to driver
17/01/13 23:03:02 INFO DAGScheduler: ResultStage 0 (collect at utils.scala:59) finished in 0.677 s
17/01/13 23:03:02 INFO DAGScheduler: Job 0 finished: collect at utils.scala:59, took 0.965554 s
17/01/13 23:03:02 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 664 ms on localhost (1/1)
17/01/13 23:03:02 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/01/13 23:03:03 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 61.8 KB, free 70.2 KB)
17/01/13 23:03:03 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.3 KB, free 89.5 KB)
17/01/13 23:03:03 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:63586 (size: 19.3 KB, free: 511.1 MB)
17/01/13 23:03:03 INFO SparkContext: Created broadcast 1 from textFile at TextFile.scala:30
17/01/13 23:03:03 INFO FileInputFormat: Total input paths to process : 1
17/01/13 23:03:03 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/01/13 23:03:03 INFO DAGScheduler: Got job 1 (take at CsvRelation.scala:249) with 1 output partitions
17/01/13 23:03:03 INFO DAGScheduler: Final stage: ResultStage 1 (take at CsvRelation.scala:249)
17/01/13 23:03:03 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:03 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:03 INFO DAGScheduler: Submitting ResultStage 1 (C:\Users\Baoco\AppData\Local\Temp\RtmpOgwhNO/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv MapPartitionsRDD[5] at textFile at TextFile.scala:30), which has no missing parents
17/01/13 23:03:03 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 3.2 KB, free 92.7 KB)
17/01/13 23:03:03 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 1950.0 B, free 94.6 KB)
17/01/13 23:03:03 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:63586 (size: 1950.0 B, free: 511.1 MB)
17/01/13 23:03:03 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (C:\Users\Baoco\AppData\Local\Temp\RtmpOgwhNO/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv MapPartitionsRDD[5] at textFile at TextFile.scala:30)
17/01/13 23:03:03 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
17/01/13 23:03:03 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 23:03:03 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
17/01/13 23:03:03 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv:0+2088
17/01/13 23:03:03 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
17/01/13 23:03:03 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
17/01/13 23:03:03 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
17/01/13 23:03:03 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
17/01/13 23:03:03 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
17/01/13 23:03:03 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2354 bytes result sent to driver
17/01/13 23:03:03 INFO DAGScheduler: ResultStage 1 (take at CsvRelation.scala:249) finished in 0.058 s
17/01/13 23:03:03 INFO DAGScheduler: Job 1 finished: take at CsvRelation.scala:249, took 0.067360 s
17/01/13 23:03:03 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 57 ms on localhost (1/1)
17/01/13 23:03:03 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/01/13 23:03:03 INFO ParseDriver: Parsing command: SELECT * FROM  `iris`
17/01/13 23:03:04 INFO ParseDriver: Parse Completed
17/01/13 23:03:04 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 208.5 KB, free 303.1 KB)
17/01/13 23:03:04 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 19.3 KB, free 322.5 KB)
17/01/13 23:03:04 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:63586 (size: 19.3 KB, free: 511.1 MB)
17/01/13 23:03:04 INFO SparkContext: Created broadcast 3 from textFile at TextFile.scala:30
17/01/13 23:03:04 INFO FileInputFormat: Total input paths to process : 1
17/01/13 23:03:04 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/01/13 23:03:04 INFO DAGScheduler: Got job 2 (take at CsvRelation.scala:249) with 1 output partitions
17/01/13 23:03:04 INFO DAGScheduler: Final stage: ResultStage 2 (take at CsvRelation.scala:249)
17/01/13 23:03:04 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:04 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:04 INFO DAGScheduler: Submitting ResultStage 2 (C:\Users\Baoco\AppData\Local\Temp\RtmpOgwhNO/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv MapPartitionsRDD[7] at textFile at TextFile.scala:30), which has no missing parents
17/01/13 23:03:04 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 3.2 KB, free 325.7 KB)
17/01/13 23:03:04 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 1951.0 B, free 327.6 KB)
17/01/13 23:03:04 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:63586 (size: 1951.0 B, free: 511.1 MB)
17/01/13 23:03:04 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (C:\Users\Baoco\AppData\Local\Temp\RtmpOgwhNO/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv MapPartitionsRDD[7] at textFile at TextFile.scala:30)
17/01/13 23:03:04 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
17/01/13 23:03:04 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 23:03:04 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
17/01/13 23:03:04 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv:0+2088
17/01/13 23:03:04 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2354 bytes result sent to driver
17/01/13 23:03:04 INFO DAGScheduler: ResultStage 2 (take at CsvRelation.scala:249) finished in 0.015 s
17/01/13 23:03:04 INFO DAGScheduler: Job 2 finished: take at CsvRelation.scala:249, took 0.023309 s
17/01/13 23:03:04 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 14 ms on localhost (1/1)
17/01/13 23:03:04 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
17/01/13 23:03:04 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 208.5 KB, free 536.1 KB)
17/01/13 23:03:04 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 19.3 KB, free 555.4 KB)
17/01/13 23:03:04 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:63586 (size: 19.3 KB, free: 511.1 MB)
17/01/13 23:03:04 INFO SparkContext: Created broadcast 5 from textFile at TextFile.scala:30
17/01/13 23:03:04 INFO FileInputFormat: Total input paths to process : 1
17/01/13 23:03:04 INFO SparkContext: Starting job: sql at null:-2
17/01/13 23:03:04 INFO DAGScheduler: Registering RDD 17 (sql at null:-2)
17/01/13 23:03:04 INFO DAGScheduler: Got job 3 (sql at null:-2) with 1 output partitions
17/01/13 23:03:04 INFO DAGScheduler: Final stage: ResultStage 4 (sql at null:-2)
17/01/13 23:03:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
17/01/13 23:03:04 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
17/01/13 23:03:04 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[17] at sql at null:-2), which has no missing parents
17/01/13 23:03:04 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 18.3 KB, free 573.7 KB)
17/01/13 23:03:04 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 8.7 KB, free 582.3 KB)
17/01/13 23:03:04 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:63586 (size: 8.7 KB, free: 511.1 MB)
17/01/13 23:03:04 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:04 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[17] at sql at null:-2)
17/01/13 23:03:04 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks
17/01/13 23:03:04 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:04 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 4, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:04 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
17/01/13 23:03:04 INFO Executor: Running task 1.0 in stage 3.0 (TID 4)
17/01/13 23:03:04 INFO CacheManager: Partition rdd_14_0 not found, computing it
17/01/13 23:03:04 INFO CacheManager: Partition rdd_14_1 not found, computing it
17/01/13 23:03:04 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv:0+2088
17/01/13 23:03:04 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv:2088+2089
17/01/13 23:03:04 INFO GenerateUnsafeProjection: Code generated in 17.069639 ms
17/01/13 23:03:04 INFO MemoryStore: Block rdd_14_0 stored as values in memory (estimated size 3.2 KB, free 585.5 KB)
17/01/13 23:03:04 INFO MemoryStore: Block rdd_14_1 stored as values in memory (estimated size 3.1 KB, free 588.7 KB)
17/01/13 23:03:04 INFO BlockManagerInfo: Added rdd_14_0 in memory on localhost:63586 (size: 3.2 KB, free: 511.1 MB)
17/01/13 23:03:04 INFO BlockManagerInfo: Added rdd_14_1 in memory on localhost:63586 (size: 3.1 KB, free: 511.0 MB)
17/01/13 23:03:04 INFO GeneratePredicate: Code generated in 5.217702 ms
17/01/13 23:03:04 INFO GenerateColumnAccessor: Code generated in 13.648202 ms
17/01/13 23:03:04 INFO GenerateMutableProjection: Code generated in 5.234769 ms
17/01/13 23:03:04 INFO GenerateUnsafeProjection: Code generated in 6.041595 ms
17/01/13 23:03:04 INFO GenerateMutableProjection: Code generated in 9.995512 ms
17/01/13 23:03:04 INFO GenerateUnsafeRowJoiner: Code generated in 6.752848 ms
17/01/13 23:03:04 INFO GenerateUnsafeProjection: Code generated in 6.804474 ms
17/01/13 23:03:04 INFO Executor: Finished task 1.0 in stage 3.0 (TID 4). 3787 bytes result sent to driver
17/01/13 23:03:04 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3784 bytes result sent to driver
17/01/13 23:03:04 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 297 ms on localhost (1/2)
17/01/13 23:03:04 INFO DAGScheduler: ShuffleMapStage 3 (sql at null:-2) finished in 0.301 s
17/01/13 23:03:04 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 4) in 300 ms on localhost (2/2)
17/01/13 23:03:04 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
17/01/13 23:03:04 INFO DAGScheduler: looking for newly runnable stages
17/01/13 23:03:04 INFO DAGScheduler: running: Set()
17/01/13 23:03:04 INFO DAGScheduler: waiting: Set(ResultStage 4)
17/01/13 23:03:04 INFO DAGScheduler: failed: Set()
17/01/13 23:03:04 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[20] at sql at null:-2), which has no missing parents
17/01/13 23:03:04 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 9.3 KB, free 598.0 KB)
17/01/13 23:03:04 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 4.6 KB, free 602.6 KB)
17/01/13 23:03:04 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:63586 (size: 4.6 KB, free: 511.0 MB)
17/01/13 23:03:04 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[20] at sql at null:-2)
17/01/13 23:03:04 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
17/01/13 23:03:04 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 5, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 23:03:04 INFO Executor: Running task 0.0 in stage 4.0 (TID 5)
17/01/13 23:03:04 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
17/01/13 23:03:04 INFO GenerateMutableProjection: Code generated in 9.152845 ms
17/01/13 23:03:04 INFO GenerateMutableProjection: Code generated in 6.078288 ms
17/01/13 23:03:04 INFO Executor: Finished task 0.0 in stage 4.0 (TID 5). 1830 bytes result sent to driver
17/01/13 23:03:04 INFO DAGScheduler: ResultStage 4 (sql at null:-2) finished in 0.125 s
17/01/13 23:03:04 INFO DAGScheduler: Job 3 finished: sql at null:-2, took 0.468073 s
17/01/13 23:03:04 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 5) in 123 ms on localhost (1/1)
17/01/13 23:03:04 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
17/01/13 23:03:04 INFO ParseDriver: Parsing command: SELECT count(*) FROM  `iris`
17/01/13 23:03:04 INFO ParseDriver: Parse Completed
17/01/13 23:03:05 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 23:03:05 INFO DAGScheduler: Registering RDD 24 (collect at utils.scala:195)
17/01/13 23:03:05 INFO DAGScheduler: Got job 4 (collect at utils.scala:195) with 1 output partitions
17/01/13 23:03:05 INFO DAGScheduler: Final stage: ResultStage 6 (collect at utils.scala:195)
17/01/13 23:03:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
17/01/13 23:03:05 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 5)
17/01/13 23:03:05 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[24] at collect at utils.scala:195), which has no missing parents
17/01/13 23:03:05 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 18.4 KB, free 620.9 KB)
17/01/13 23:03:05 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.7 KB, free 629.6 KB)
17/01/13 23:03:05 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:63586 (size: 8.7 KB, free: 511.0 MB)
17/01/13 23:03:05 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:05 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[24] at collect at utils.scala:195)
17/01/13 23:03:05 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks
17/01/13 23:03:05 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:05 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 7, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:05 INFO Executor: Running task 0.0 in stage 5.0 (TID 6)
17/01/13 23:03:05 INFO Executor: Running task 1.0 in stage 5.0 (TID 7)
17/01/13 23:03:05 INFO BlockManager: Found block rdd_14_0 locally
17/01/13 23:03:05 INFO Executor: Finished task 0.0 in stage 5.0 (TID 6). 2679 bytes result sent to driver
17/01/13 23:03:05 INFO BlockManager: Found block rdd_14_1 locally
17/01/13 23:03:05 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 6) in 31 ms on localhost (1/2)
17/01/13 23:03:05 INFO Executor: Finished task 1.0 in stage 5.0 (TID 7). 2679 bytes result sent to driver
17/01/13 23:03:05 INFO DAGScheduler: ShuffleMapStage 5 (collect at utils.scala:195) finished in 0.043 s
17/01/13 23:03:05 INFO DAGScheduler: looking for newly runnable stages
17/01/13 23:03:05 INFO DAGScheduler: running: Set()
17/01/13 23:03:05 INFO DAGScheduler: waiting: Set(ResultStage 6)
17/01/13 23:03:05 INFO DAGScheduler: failed: Set()
17/01/13 23:03:05 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[27] at collect at utils.scala:195), which has no missing parents
17/01/13 23:03:05 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 9.4 KB, free 639.0 KB)
17/01/13 23:03:05 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 7) in 42 ms on localhost (2/2)
17/01/13 23:03:05 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
17/01/13 23:03:05 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.6 KB, free 643.6 KB)
17/01/13 23:03:05 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:63586 (size: 4.6 KB, free: 511.0 MB)
17/01/13 23:03:05 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[27] at collect at utils.scala:195)
17/01/13 23:03:05 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
17/01/13 23:03:05 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 23:03:05 INFO Executor: Running task 0.0 in stage 6.0 (TID 8)
17/01/13 23:03:05 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:05 INFO Executor: Finished task 0.0 in stage 6.0 (TID 8). 1830 bytes result sent to driver
17/01/13 23:03:05 INFO DAGScheduler: ResultStage 6 (collect at utils.scala:195) finished in 0.013 s
17/01/13 23:03:05 INFO DAGScheduler: Job 4 finished: collect at utils.scala:195, took 0.075970 s
17/01/13 23:03:05 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 12 ms on localhost (1/1)
17/01/13 23:03:05 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
17/01/13 23:03:05 INFO ParseDriver: Parsing command: SELECT *
FROM `iris` AS `zzz1`
WHERE (0 = 1)
17/01/13 23:03:05 INFO ParseDriver: Parse Completed
17/01/13 23:03:05 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:63586 in memory (size: 3.0 KB, free: 511.0 MB)
17/01/13 23:03:05 INFO ContextCleaner: Cleaned accumulator 10
17/01/13 23:03:05 INFO ContextCleaner: Cleaned accumulator 9
17/01/13 23:03:05 INFO ContextCleaner: Cleaned accumulator 8
17/01/13 23:03:05 INFO ContextCleaner: Cleaned accumulator 7
17/01/13 23:03:05 INFO BlockManagerInfo: Removed broadcast_4_piece0 on localhost:63586 in memory (size: 1951.0 B, free: 511.0 MB)
17/01/13 23:03:05 INFO ContextCleaner: Cleaned accumulator 5
17/01/13 23:03:05 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:63586 in memory (size: 19.3 KB, free: 511.1 MB)
17/01/13 23:03:05 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:63586 in memory (size: 1950.0 B, free: 511.1 MB)
17/01/13 23:03:05 INFO ContextCleaner: Cleaned accumulator 4
17/01/13 23:03:05 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:63586 in memory (size: 19.3 KB, free: 511.1 MB)
17/01/13 23:03:05 INFO ContextCleaner: Cleaned accumulator 3
17/01/13 23:03:05 INFO ContextCleaner: Cleaned accumulator 2
17/01/13 23:03:05 INFO BlockManagerInfo: Removed broadcast_9_piece0 on localhost:63586 in memory (size: 4.6 KB, free: 511.1 MB)
17/01/13 23:03:05 INFO ContextCleaner: Cleaned accumulator 26
17/01/13 23:03:05 INFO BlockManagerInfo: Removed broadcast_8_piece0 on localhost:63586 in memory (size: 8.7 KB, free: 511.1 MB)
17/01/13 23:03:05 INFO ContextCleaner: Cleaned accumulator 25
17/01/13 23:03:05 INFO ContextCleaner: Cleaned shuffle 1
17/01/13 23:03:05 INFO BlockManagerInfo: Removed broadcast_7_piece0 on localhost:63586 in memory (size: 4.6 KB, free: 511.1 MB)
17/01/13 23:03:05 INFO ContextCleaner: Cleaned accumulator 16
17/01/13 23:03:05 INFO BlockManagerInfo: Removed broadcast_6_piece0 on localhost:63586 in memory (size: 8.7 KB, free: 511.1 MB)
17/01/13 23:03:05 INFO ContextCleaner: Cleaned accumulator 15
17/01/13 23:03:05 INFO ContextCleaner: Cleaned shuffle 0
17/01/13 23:03:05 INFO ContextCleaner: Cleaned accumulator 14
17/01/13 23:03:05 INFO ContextCleaner: Cleaned accumulator 13
17/01/13 23:03:05 INFO ContextCleaner: Cleaned accumulator 12
17/01/13 23:03:05 INFO ContextCleaner: Cleaned accumulator 11
17/01/13 23:03:05 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/01/13 23:03:05 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/01/13 23:03:05 INFO SparkContext: Starting job: collect at utils.scala:59
17/01/13 23:03:05 INFO DAGScheduler: Got job 5 (collect at utils.scala:59) with 1 output partitions
17/01/13 23:03:05 INFO DAGScheduler: Final stage: ResultStage 7 (collect at utils.scala:59)
17/01/13 23:03:05 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:05 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:05 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[31] at map at utils.scala:56), which has no missing parents
17/01/13 23:03:05 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.4 KB, free 239.6 KB)
17/01/13 23:03:05 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.0 KB, free 242.6 KB)
17/01/13 23:03:05 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:63586 (size: 3.0 KB, free: 511.1 MB)
17/01/13 23:03:05 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[31] at map at utils.scala:56)
17/01/13 23:03:05 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks
17/01/13 23:03:05 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 9, localhost, partition 0,PROCESS_LOCAL, 2646 bytes)
17/01/13 23:03:05 INFO Executor: Running task 0.0 in stage 7.0 (TID 9)
17/01/13 23:03:05 INFO Executor: Finished task 0.0 in stage 7.0 (TID 9). 1067 bytes result sent to driver
17/01/13 23:03:05 INFO DAGScheduler: ResultStage 7 (collect at utils.scala:59) finished in 0.006 s
17/01/13 23:03:05 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 9) in 5 ms on localhost (1/1)
17/01/13 23:03:05 INFO DAGScheduler: Job 5 finished: collect at utils.scala:59, took 0.011750 s
17/01/13 23:03:05 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
17/01/13 23:03:15 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 208.5 KB, free 451.1 KB)
17/01/13 23:03:15 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 19.3 KB, free 470.4 KB)
17/01/13 23:03:15 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:63586 (size: 19.3 KB, free: 511.1 MB)
17/01/13 23:03:15 INFO SparkContext: Created broadcast 11 from textFile at TextFile.scala:30
17/01/13 23:03:15 INFO FileInputFormat: Total input paths to process : 1
17/01/13 23:03:15 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/01/13 23:03:15 INFO DAGScheduler: Got job 6 (take at CsvRelation.scala:249) with 1 output partitions
17/01/13 23:03:15 INFO DAGScheduler: Final stage: ResultStage 8 (take at CsvRelation.scala:249)
17/01/13 23:03:15 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:15 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:15 INFO DAGScheduler: Submitting ResultStage 8 (C:\Users\Baoco\AppData\Local\Temp\RtmpOgwhNO/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv MapPartitionsRDD[33] at textFile at TextFile.scala:30), which has no missing parents
17/01/13 23:03:15 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 3.2 KB, free 473.6 KB)
17/01/13 23:03:15 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 1949.0 B, free 475.5 KB)
17/01/13 23:03:15 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:63586 (size: 1949.0 B, free: 511.1 MB)
17/01/13 23:03:15 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (C:\Users\Baoco\AppData\Local\Temp\RtmpOgwhNO/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv MapPartitionsRDD[33] at textFile at TextFile.scala:30)
17/01/13 23:03:15 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks
17/01/13 23:03:15 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 10, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 23:03:15 INFO Executor: Running task 0.0 in stage 8.0 (TID 10)
17/01/13 23:03:15 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv:0+16824941
17/01/13 23:03:15 INFO Executor: Finished task 0.0 in stage 8.0 (TID 10). 3117 bytes result sent to driver
17/01/13 23:03:15 INFO DAGScheduler: ResultStage 8 (take at CsvRelation.scala:249) finished in 0.008 s
17/01/13 23:03:15 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 10) in 8 ms on localhost (1/1)
17/01/13 23:03:15 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
17/01/13 23:03:15 INFO DAGScheduler: Job 6 finished: take at CsvRelation.scala:249, took 0.013902 s
17/01/13 23:03:15 INFO ParseDriver: Parsing command: SELECT * FROM  `flights`
17/01/13 23:03:15 INFO ParseDriver: Parse Completed
17/01/13 23:03:15 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 208.5 KB, free 684.0 KB)
17/01/13 23:03:15 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 19.3 KB, free 703.3 KB)
17/01/13 23:03:15 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on localhost:63586 (size: 19.3 KB, free: 511.1 MB)
17/01/13 23:03:15 INFO SparkContext: Created broadcast 13 from textFile at TextFile.scala:30
17/01/13 23:03:15 INFO FileInputFormat: Total input paths to process : 1
17/01/13 23:03:15 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/01/13 23:03:15 INFO DAGScheduler: Got job 7 (take at CsvRelation.scala:249) with 1 output partitions
17/01/13 23:03:15 INFO DAGScheduler: Final stage: ResultStage 9 (take at CsvRelation.scala:249)
17/01/13 23:03:15 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:15 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:15 INFO DAGScheduler: Submitting ResultStage 9 (C:\Users\Baoco\AppData\Local\Temp\RtmpOgwhNO/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv MapPartitionsRDD[35] at textFile at TextFile.scala:30), which has no missing parents
17/01/13 23:03:15 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 3.2 KB, free 706.5 KB)
17/01/13 23:03:15 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 1949.0 B, free 708.4 KB)
17/01/13 23:03:15 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on localhost:63586 (size: 1949.0 B, free: 511.1 MB)
17/01/13 23:03:15 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (C:\Users\Baoco\AppData\Local\Temp\RtmpOgwhNO/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv MapPartitionsRDD[35] at textFile at TextFile.scala:30)
17/01/13 23:03:15 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
17/01/13 23:03:15 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 11, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 23:03:15 INFO Executor: Running task 0.0 in stage 9.0 (TID 11)
17/01/13 23:03:15 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv:0+16824941
17/01/13 23:03:15 INFO Executor: Finished task 0.0 in stage 9.0 (TID 11). 3117 bytes result sent to driver
17/01/13 23:03:15 INFO DAGScheduler: ResultStage 9 (take at CsvRelation.scala:249) finished in 0.014 s
17/01/13 23:03:15 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 11) in 14 ms on localhost (1/1)
17/01/13 23:03:15 INFO DAGScheduler: Job 7 finished: take at CsvRelation.scala:249, took 0.020416 s
17/01/13 23:03:15 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
17/01/13 23:03:15 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 208.5 KB, free 917.0 KB)
17/01/13 23:03:15 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 19.3 KB, free 936.3 KB)
17/01/13 23:03:15 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on localhost:63586 (size: 19.3 KB, free: 511.0 MB)
17/01/13 23:03:15 INFO SparkContext: Created broadcast 15 from textFile at TextFile.scala:30
17/01/13 23:03:15 INFO FileInputFormat: Total input paths to process : 1
17/01/13 23:03:15 INFO SparkContext: Starting job: sql at null:-2
17/01/13 23:03:15 INFO DAGScheduler: Registering RDD 45 (sql at null:-2)
17/01/13 23:03:15 INFO DAGScheduler: Got job 8 (sql at null:-2) with 1 output partitions
17/01/13 23:03:15 INFO DAGScheduler: Final stage: ResultStage 11 (sql at null:-2)
17/01/13 23:03:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
17/01/13 23:03:15 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 10)
17/01/13 23:03:15 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[45] at sql at null:-2), which has no missing parents
17/01/13 23:03:15 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 26.8 KB, free 963.1 KB)
17/01/13 23:03:15 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 11.0 KB, free 974.1 KB)
17/01/13 23:03:15 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on localhost:63586 (size: 11.0 KB, free: 511.0 MB)
17/01/13 23:03:15 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:15 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[45] at sql at null:-2)
17/01/13 23:03:15 INFO TaskSchedulerImpl: Adding task set 10.0 with 2 tasks
17/01/13 23:03:15 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 12, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:15 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 13, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:15 INFO Executor: Running task 0.0 in stage 10.0 (TID 12)
17/01/13 23:03:15 INFO Executor: Running task 1.0 in stage 10.0 (TID 13)
17/01/13 23:03:15 INFO CacheManager: Partition rdd_42_0 not found, computing it
17/01/13 23:03:15 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv:0+16824941
17/01/13 23:03:15 INFO CacheManager: Partition rdd_42_1 not found, computing it
17/01/13 23:03:15 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv:16824941+16824942
17/01/13 23:03:15 INFO GenerateUnsafeProjection: Code generated in 24.410432 ms
17/01/13 23:03:16 INFO BlockManagerInfo: Removed broadcast_14_piece0 on localhost:63586 in memory (size: 1949.0 B, free: 511.0 MB)
17/01/13 23:03:16 INFO ContextCleaner: Cleaned accumulator 30
17/01/13 23:03:16 INFO BlockManagerInfo: Removed broadcast_13_piece0 on localhost:63586 in memory (size: 19.3 KB, free: 511.0 MB)
17/01/13 23:03:16 INFO BlockManagerInfo: Removed broadcast_12_piece0 on localhost:63586 in memory (size: 1949.0 B, free: 511.0 MB)
17/01/13 23:03:16 INFO ContextCleaner: Cleaned accumulator 29
17/01/13 23:03:16 INFO BlockManagerInfo: Removed broadcast_11_piece0 on localhost:63586 in memory (size: 19.3 KB, free: 511.1 MB)
17/01/13 23:03:16 INFO BlockManagerInfo: Removed broadcast_10_piece0 on localhost:63586 in memory (size: 3.0 KB, free: 511.1 MB)
17/01/13 23:03:16 INFO ContextCleaner: Cleaned accumulator 28
17/01/13 23:03:16 INFO ContextCleaner: Cleaned accumulator 27
17/01/13 23:03:20 INFO MemoryStore: Block rdd_42_1 stored as values in memory (estimated size 12.2 MB, free 12.7 MB)
17/01/13 23:03:20 INFO BlockManagerInfo: Added rdd_42_1 in memory on localhost:63586 (size: 12.2 MB, free: 498.9 MB)
17/01/13 23:03:20 INFO MemoryStore: Block rdd_42_0 stored as values in memory (estimated size 12.2 MB, free 24.9 MB)
17/01/13 23:03:20 INFO BlockManagerInfo: Added rdd_42_0 in memory on localhost:63586 (size: 12.2 MB, free: 486.7 MB)
17/01/13 23:03:20 INFO Executor: Finished task 1.0 in stage 10.0 (TID 13). 21132 bytes result sent to driver
17/01/13 23:03:20 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 13) in 4766 ms on localhost (1/2)
17/01/13 23:03:20 INFO Executor: Finished task 0.0 in stage 10.0 (TID 12). 21077 bytes result sent to driver
17/01/13 23:03:20 INFO DAGScheduler: ShuffleMapStage 10 (sql at null:-2) finished in 4.779 s
17/01/13 23:03:20 INFO DAGScheduler: looking for newly runnable stages
17/01/13 23:03:20 INFO DAGScheduler: running: Set()
17/01/13 23:03:20 INFO DAGScheduler: waiting: Set(ResultStage 11)
17/01/13 23:03:20 INFO DAGScheduler: failed: Set()
17/01/13 23:03:20 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[48] at sql at null:-2), which has no missing parents
17/01/13 23:03:20 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 9.3 KB, free 24.9 MB)
17/01/13 23:03:20 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 12) in 4778 ms on localhost (2/2)
17/01/13 23:03:20 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 4.6 KB, free 24.9 MB)
17/01/13 23:03:20 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
17/01/13 23:03:20 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on localhost:63586 (size: 4.6 KB, free: 486.7 MB)
17/01/13 23:03:20 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[48] at sql at null:-2)
17/01/13 23:03:20 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks
17/01/13 23:03:20 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 14, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 23:03:20 INFO Executor: Running task 0.0 in stage 11.0 (TID 14)
17/01/13 23:03:20 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:20 INFO Executor: Finished task 0.0 in stage 11.0 (TID 14). 1830 bytes result sent to driver
17/01/13 23:03:20 INFO DAGScheduler: ResultStage 11 (sql at null:-2) finished in 0.012 s
17/01/13 23:03:20 INFO DAGScheduler: Job 8 finished: sql at null:-2, took 4.805165 s
17/01/13 23:03:20 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 14) in 11 ms on localhost (1/1)
17/01/13 23:03:20 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
17/01/13 23:03:20 INFO ParseDriver: Parsing command: SELECT count(*) FROM  `flights`
17/01/13 23:03:20 INFO ParseDriver: Parse Completed
17/01/13 23:03:20 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 23:03:20 INFO DAGScheduler: Registering RDD 52 (collect at utils.scala:195)
17/01/13 23:03:20 INFO DAGScheduler: Got job 9 (collect at utils.scala:195) with 1 output partitions
17/01/13 23:03:20 INFO DAGScheduler: Final stage: ResultStage 13 (collect at utils.scala:195)
17/01/13 23:03:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)
17/01/13 23:03:20 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 12)
17/01/13 23:03:20 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[52] at collect at utils.scala:195), which has no missing parents
17/01/13 23:03:20 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 26.9 KB, free 24.9 MB)
17/01/13 23:03:20 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 11.0 KB, free 24.9 MB)
17/01/13 23:03:20 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on localhost:63586 (size: 11.0 KB, free: 486.7 MB)
17/01/13 23:03:20 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:20 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[52] at collect at utils.scala:195)
17/01/13 23:03:20 INFO TaskSchedulerImpl: Adding task set 12.0 with 2 tasks
17/01/13 23:03:20 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 15, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:20 INFO TaskSetManager: Starting task 1.0 in stage 12.0 (TID 16, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:20 INFO Executor: Running task 0.0 in stage 12.0 (TID 15)
17/01/13 23:03:20 INFO Executor: Running task 1.0 in stage 12.0 (TID 16)
17/01/13 23:03:20 INFO BlockManager: Found block rdd_42_0 locally
17/01/13 23:03:20 INFO BlockManager: Found block rdd_42_1 locally
17/01/13 23:03:20 INFO Executor: Finished task 1.0 in stage 12.0 (TID 16). 2679 bytes result sent to driver
17/01/13 23:03:20 INFO TaskSetManager: Finished task 1.0 in stage 12.0 (TID 16) in 30 ms on localhost (1/2)
17/01/13 23:03:20 INFO Executor: Finished task 0.0 in stage 12.0 (TID 15). 2679 bytes result sent to driver
17/01/13 23:03:20 INFO DAGScheduler: ShuffleMapStage 12 (collect at utils.scala:195) finished in 0.042 s
17/01/13 23:03:20 INFO DAGScheduler: looking for newly runnable stages
17/01/13 23:03:20 INFO DAGScheduler: running: Set()
17/01/13 23:03:20 INFO DAGScheduler: waiting: Set(ResultStage 13)
17/01/13 23:03:20 INFO DAGScheduler: failed: Set()
17/01/13 23:03:20 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[55] at collect at utils.scala:195), which has no missing parents
17/01/13 23:03:20 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 9.4 KB, free 24.9 MB)
17/01/13 23:03:20 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 15) in 41 ms on localhost (2/2)
17/01/13 23:03:20 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
17/01/13 23:03:20 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 4.6 KB, free 24.9 MB)
17/01/13 23:03:20 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on localhost:63586 (size: 4.6 KB, free: 486.7 MB)
17/01/13 23:03:20 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[55] at collect at utils.scala:195)
17/01/13 23:03:20 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks
17/01/13 23:03:20 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 17, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 23:03:20 INFO Executor: Running task 0.0 in stage 13.0 (TID 17)
17/01/13 23:03:20 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:20 INFO Executor: Finished task 0.0 in stage 13.0 (TID 17). 1830 bytes result sent to driver
17/01/13 23:03:20 INFO DAGScheduler: ResultStage 13 (collect at utils.scala:195) finished in 0.011 s
17/01/13 23:03:20 INFO DAGScheduler: Job 9 finished: collect at utils.scala:195, took 0.070004 s
17/01/13 23:03:20 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 17) in 11 ms on localhost (1/1)
17/01/13 23:03:20 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
17/01/13 23:03:20 INFO ParseDriver: Parsing command: SELECT *
FROM `flights` AS `zzz2`
WHERE (0 = 1)
17/01/13 23:03:20 INFO ParseDriver: Parse Completed
17/01/13 23:03:20 INFO BlockManagerInfo: Removed broadcast_17_piece0 on localhost:63586 in memory (size: 4.6 KB, free: 486.7 MB)
17/01/13 23:03:20 INFO BlockManagerInfo: Removed broadcast_18_piece0 on localhost:63586 in memory (size: 11.0 KB, free: 486.7 MB)
17/01/13 23:03:20 INFO ContextCleaner: Cleaned accumulator 50
17/01/13 23:03:20 INFO ContextCleaner: Cleaned shuffle 3
17/01/13 23:03:20 INFO BlockManagerInfo: Removed broadcast_19_piece0 on localhost:63586 in memory (size: 4.6 KB, free: 486.7 MB)
17/01/13 23:03:20 INFO ContextCleaner: Cleaned accumulator 51
17/01/13 23:03:20 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/01/13 23:03:20 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/01/13 23:03:20 INFO SparkContext: Starting job: collect at utils.scala:59
17/01/13 23:03:20 INFO DAGScheduler: Got job 10 (collect at utils.scala:59) with 1 output partitions
17/01/13 23:03:20 INFO DAGScheduler: Final stage: ResultStage 14 (collect at utils.scala:59)
17/01/13 23:03:20 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:20 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:20 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[59] at map at utils.scala:56), which has no missing parents
17/01/13 23:03:20 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 5.4 KB, free 24.9 MB)
17/01/13 23:03:20 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 3.0 KB, free 24.9 MB)
17/01/13 23:03:20 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on localhost:63586 (size: 3.0 KB, free: 486.7 MB)
17/01/13 23:03:20 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[59] at map at utils.scala:56)
17/01/13 23:03:20 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks
17/01/13 23:03:20 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 18, localhost, partition 0,PROCESS_LOCAL, 2687 bytes)
17/01/13 23:03:20 INFO Executor: Running task 0.0 in stage 14.0 (TID 18)
17/01/13 23:03:20 INFO Executor: Finished task 0.0 in stage 14.0 (TID 18). 1077 bytes result sent to driver
17/01/13 23:03:20 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 18) in 5 ms on localhost (1/1)
17/01/13 23:03:20 INFO DAGScheduler: ResultStage 14 (collect at utils.scala:59) finished in 0.005 s
17/01/13 23:03:20 INFO DAGScheduler: Job 10 finished: collect at utils.scala:59, took 0.010211 s
17/01/13 23:03:20 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
17/01/13 23:03:22 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 208.5 KB, free 25.1 MB)
17/01/13 23:03:22 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 19.3 KB, free 25.1 MB)
17/01/13 23:03:22 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on localhost:63586 (size: 19.3 KB, free: 486.7 MB)
17/01/13 23:03:22 INFO SparkContext: Created broadcast 21 from textFile at TextFile.scala:30
17/01/13 23:03:22 INFO FileInputFormat: Total input paths to process : 1
17/01/13 23:03:22 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/01/13 23:03:22 INFO DAGScheduler: Got job 11 (take at CsvRelation.scala:249) with 1 output partitions
17/01/13 23:03:22 INFO DAGScheduler: Final stage: ResultStage 15 (take at CsvRelation.scala:249)
17/01/13 23:03:22 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:22 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:22 INFO DAGScheduler: Submitting ResultStage 15 (C:\Users\Baoco\AppData\Local\Temp\RtmpOgwhNO/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv MapPartitionsRDD[61] at textFile at TextFile.scala:30), which has no missing parents
17/01/13 23:03:22 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 3.2 KB, free 25.1 MB)
17/01/13 23:03:22 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 1949.0 B, free 25.1 MB)
17/01/13 23:03:22 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on localhost:63586 (size: 1949.0 B, free: 486.7 MB)
17/01/13 23:03:22 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (C:\Users\Baoco\AppData\Local\Temp\RtmpOgwhNO/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv MapPartitionsRDD[61] at textFile at TextFile.scala:30)
17/01/13 23:03:22 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks
17/01/13 23:03:22 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 19, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 23:03:22 INFO Executor: Running task 0.0 in stage 15.0 (TID 19)
17/01/13 23:03:22 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv:0+3427780
17/01/13 23:03:22 INFO Executor: Finished task 0.0 in stage 15.0 (TID 19). 2768 bytes result sent to driver
17/01/13 23:03:22 INFO DAGScheduler: ResultStage 15 (take at CsvRelation.scala:249) finished in 0.010 s
17/01/13 23:03:22 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 19) in 10 ms on localhost (1/1)
17/01/13 23:03:22 INFO DAGScheduler: Job 11 finished: take at CsvRelation.scala:249, took 0.014033 s
17/01/13 23:03:22 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
17/01/13 23:03:22 INFO ParseDriver: Parsing command: SELECT * FROM  `batting`
17/01/13 23:03:22 INFO ParseDriver: Parse Completed
17/01/13 23:03:22 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 208.5 KB, free 25.3 MB)
17/01/13 23:03:22 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 19.3 KB, free 25.3 MB)
17/01/13 23:03:22 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on localhost:63586 (size: 19.3 KB, free: 486.7 MB)
17/01/13 23:03:22 INFO SparkContext: Created broadcast 23 from textFile at TextFile.scala:30
17/01/13 23:03:22 INFO FileInputFormat: Total input paths to process : 1
17/01/13 23:03:22 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/01/13 23:03:22 INFO DAGScheduler: Got job 12 (take at CsvRelation.scala:249) with 1 output partitions
17/01/13 23:03:22 INFO DAGScheduler: Final stage: ResultStage 16 (take at CsvRelation.scala:249)
17/01/13 23:03:22 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:22 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:22 INFO DAGScheduler: Submitting ResultStage 16 (C:\Users\Baoco\AppData\Local\Temp\RtmpOgwhNO/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv MapPartitionsRDD[63] at textFile at TextFile.scala:30), which has no missing parents
17/01/13 23:03:22 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 3.2 KB, free 25.3 MB)
17/01/13 23:03:22 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 1949.0 B, free 25.3 MB)
17/01/13 23:03:22 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on localhost:63586 (size: 1949.0 B, free: 486.7 MB)
17/01/13 23:03:22 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (C:\Users\Baoco\AppData\Local\Temp\RtmpOgwhNO/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv MapPartitionsRDD[63] at textFile at TextFile.scala:30)
17/01/13 23:03:22 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks
17/01/13 23:03:22 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 20, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 23:03:22 INFO Executor: Running task 0.0 in stage 16.0 (TID 20)
17/01/13 23:03:22 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv:0+3427780
17/01/13 23:03:22 INFO Executor: Finished task 0.0 in stage 16.0 (TID 20). 2768 bytes result sent to driver
17/01/13 23:03:22 INFO DAGScheduler: ResultStage 16 (take at CsvRelation.scala:249) finished in 0.006 s
17/01/13 23:03:22 INFO DAGScheduler: Job 12 finished: take at CsvRelation.scala:249, took 0.010816 s
17/01/13 23:03:22 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 20) in 6 ms on localhost (1/1)
17/01/13 23:03:22 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
17/01/13 23:03:22 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 208.5 KB, free 25.5 MB)
17/01/13 23:03:22 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 19.3 KB, free 25.5 MB)
17/01/13 23:03:22 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on localhost:63586 (size: 19.3 KB, free: 486.6 MB)
17/01/13 23:03:22 INFO SparkContext: Created broadcast 25 from textFile at TextFile.scala:30
17/01/13 23:03:22 INFO FileInputFormat: Total input paths to process : 1
17/01/13 23:03:22 INFO SparkContext: Starting job: sql at null:-2
17/01/13 23:03:22 INFO DAGScheduler: Registering RDD 73 (sql at null:-2)
17/01/13 23:03:22 INFO DAGScheduler: Got job 13 (sql at null:-2) with 1 output partitions
17/01/13 23:03:22 INFO DAGScheduler: Final stage: ResultStage 18 (sql at null:-2)
17/01/13 23:03:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)
17/01/13 23:03:22 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 17)
17/01/13 23:03:22 INFO DAGScheduler: Submitting ShuffleMapStage 17 (MapPartitionsRDD[73] at sql at null:-2), which has no missing parents
17/01/13 23:03:22 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 27.9 KB, free 25.6 MB)
17/01/13 23:03:22 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 11.2 KB, free 25.6 MB)
17/01/13 23:03:22 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on localhost:63586 (size: 11.2 KB, free: 486.6 MB)
17/01/13 23:03:22 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:22 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[73] at sql at null:-2)
17/01/13 23:03:22 INFO TaskSchedulerImpl: Adding task set 17.0 with 2 tasks
17/01/13 23:03:22 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 21, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:22 INFO TaskSetManager: Starting task 1.0 in stage 17.0 (TID 22, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:22 INFO Executor: Running task 0.0 in stage 17.0 (TID 21)
17/01/13 23:03:22 INFO Executor: Running task 1.0 in stage 17.0 (TID 22)
17/01/13 23:03:22 INFO CacheManager: Partition rdd_70_1 not found, computing it
17/01/13 23:03:22 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv:3427780+3427781
17/01/13 23:03:22 INFO CacheManager: Partition rdd_70_0 not found, computing it
17/01/13 23:03:22 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv:0+3427780
17/01/13 23:03:22 INFO GenerateUnsafeProjection: Code generated in 23.465367 ms
17/01/13 23:03:22 INFO BlockManagerInfo: Removed broadcast_24_piece0 on localhost:63586 in memory (size: 1949.0 B, free: 486.6 MB)
17/01/13 23:03:22 INFO ContextCleaner: Cleaned accumulator 55
17/01/13 23:03:22 INFO BlockManagerInfo: Removed broadcast_23_piece0 on localhost:63586 in memory (size: 19.3 KB, free: 486.6 MB)
17/01/13 23:03:22 INFO BlockManagerInfo: Removed broadcast_22_piece0 on localhost:63586 in memory (size: 1949.0 B, free: 486.6 MB)
17/01/13 23:03:22 INFO ContextCleaner: Cleaned accumulator 54
17/01/13 23:03:22 INFO BlockManagerInfo: Removed broadcast_21_piece0 on localhost:63586 in memory (size: 19.3 KB, free: 486.7 MB)
17/01/13 23:03:22 INFO BlockManagerInfo: Removed broadcast_20_piece0 on localhost:63586 in memory (size: 3.0 KB, free: 486.7 MB)
17/01/13 23:03:22 INFO ContextCleaner: Cleaned accumulator 53
17/01/13 23:03:22 INFO ContextCleaner: Cleaned accumulator 52
17/01/13 23:03:23 INFO MemoryStore: Block rdd_70_0 stored as values in memory (estimated size 1897.3 KB, free 27.0 MB)
17/01/13 23:03:23 INFO BlockManagerInfo: Added rdd_70_0 in memory on localhost:63586 (size: 1897.3 KB, free: 484.8 MB)
17/01/13 23:03:23 INFO MemoryStore: Block rdd_70_1 stored as values in memory (estimated size 1699.8 KB, free 28.6 MB)
17/01/13 23:03:23 INFO BlockManagerInfo: Added rdd_70_1 in memory on localhost:63586 (size: 1699.8 KB, free: 483.2 MB)
17/01/13 23:03:23 INFO Executor: Finished task 0.0 in stage 17.0 (TID 21). 9713 bytes result sent to driver
17/01/13 23:03:23 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 21) in 896 ms on localhost (1/2)
17/01/13 23:03:23 INFO Executor: Finished task 1.0 in stage 17.0 (TID 22). 9862 bytes result sent to driver
17/01/13 23:03:23 INFO DAGScheduler: ShuffleMapStage 17 (sql at null:-2) finished in 0.907 s
17/01/13 23:03:23 INFO DAGScheduler: looking for newly runnable stages
17/01/13 23:03:23 INFO DAGScheduler: running: Set()
17/01/13 23:03:23 INFO DAGScheduler: waiting: Set(ResultStage 18)
17/01/13 23:03:23 INFO DAGScheduler: failed: Set()
17/01/13 23:03:23 INFO TaskSetManager: Finished task 1.0 in stage 17.0 (TID 22) in 904 ms on localhost (2/2)
17/01/13 23:03:23 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[76] at sql at null:-2), which has no missing parents
17/01/13 23:03:23 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 9.3 KB, free 28.6 MB)
17/01/13 23:03:23 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
17/01/13 23:03:23 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 4.6 KB, free 28.6 MB)
17/01/13 23:03:23 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on localhost:63586 (size: 4.6 KB, free: 483.2 MB)
17/01/13 23:03:23 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[76] at sql at null:-2)
17/01/13 23:03:23 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks
17/01/13 23:03:23 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 23, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 23:03:23 INFO Executor: Running task 0.0 in stage 18.0 (TID 23)
17/01/13 23:03:23 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:23 INFO Executor: Finished task 0.0 in stage 18.0 (TID 23). 1830 bytes result sent to driver
17/01/13 23:03:23 INFO DAGScheduler: ResultStage 18 (sql at null:-2) finished in 0.011 s
17/01/13 23:03:23 INFO DAGScheduler: Job 13 finished: sql at null:-2, took 0.929936 s
17/01/13 23:03:23 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 23) in 11 ms on localhost (1/1)
17/01/13 23:03:23 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
17/01/13 23:03:23 INFO ParseDriver: Parsing command: SELECT count(*) FROM  `batting`
17/01/13 23:03:23 INFO ParseDriver: Parse Completed
17/01/13 23:03:23 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 23:03:23 INFO DAGScheduler: Registering RDD 80 (collect at utils.scala:195)
17/01/13 23:03:23 INFO DAGScheduler: Got job 14 (collect at utils.scala:195) with 1 output partitions
17/01/13 23:03:23 INFO DAGScheduler: Final stage: ResultStage 20 (collect at utils.scala:195)
17/01/13 23:03:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)
17/01/13 23:03:23 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 19)
17/01/13 23:03:23 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[80] at collect at utils.scala:195), which has no missing parents
17/01/13 23:03:23 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 28.0 KB, free 28.7 MB)
17/01/13 23:03:23 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 11.2 KB, free 28.7 MB)
17/01/13 23:03:23 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on localhost:63586 (size: 11.2 KB, free: 483.1 MB)
17/01/13 23:03:23 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:23 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[80] at collect at utils.scala:195)
17/01/13 23:03:23 INFO TaskSchedulerImpl: Adding task set 19.0 with 2 tasks
17/01/13 23:03:23 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 24, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:23 INFO TaskSetManager: Starting task 1.0 in stage 19.0 (TID 25, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:23 INFO Executor: Running task 0.0 in stage 19.0 (TID 24)
17/01/13 23:03:23 INFO Executor: Running task 1.0 in stage 19.0 (TID 25)
17/01/13 23:03:23 INFO BlockManager: Found block rdd_70_0 locally
17/01/13 23:03:23 INFO BlockManager: Found block rdd_70_1 locally
17/01/13 23:03:23 INFO Executor: Finished task 0.0 in stage 19.0 (TID 24). 2679 bytes result sent to driver
17/01/13 23:03:23 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 24) in 24 ms on localhost (1/2)
17/01/13 23:03:23 INFO Executor: Finished task 1.0 in stage 19.0 (TID 25). 2679 bytes result sent to driver
17/01/13 23:03:23 INFO DAGScheduler: ShuffleMapStage 19 (collect at utils.scala:195) finished in 0.033 s
17/01/13 23:03:23 INFO DAGScheduler: looking for newly runnable stages
17/01/13 23:03:23 INFO DAGScheduler: running: Set()
17/01/13 23:03:23 INFO DAGScheduler: waiting: Set(ResultStage 20)
17/01/13 23:03:23 INFO DAGScheduler: failed: Set()
17/01/13 23:03:23 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[83] at collect at utils.scala:195), which has no missing parents
17/01/13 23:03:23 INFO TaskSetManager: Finished task 1.0 in stage 19.0 (TID 25) in 32 ms on localhost (2/2)
17/01/13 23:03:23 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
17/01/13 23:03:23 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 9.4 KB, free 28.7 MB)
17/01/13 23:03:23 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 4.7 KB, free 28.7 MB)
17/01/13 23:03:23 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on localhost:63586 (size: 4.7 KB, free: 483.1 MB)
17/01/13 23:03:23 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[83] at collect at utils.scala:195)
17/01/13 23:03:23 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks
17/01/13 23:03:23 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 26, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 23:03:23 INFO Executor: Running task 0.0 in stage 20.0 (TID 26)
17/01/13 23:03:23 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:23 INFO Executor: Finished task 0.0 in stage 20.0 (TID 26). 1830 bytes result sent to driver
17/01/13 23:03:23 INFO DAGScheduler: ResultStage 20 (collect at utils.scala:195) finished in 0.013 s
17/01/13 23:03:23 INFO DAGScheduler: Job 14 finished: collect at utils.scala:195, took 0.056555 s
17/01/13 23:03:23 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 26) in 13 ms on localhost (1/1)
17/01/13 23:03:23 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
17/01/13 23:03:23 INFO ParseDriver: Parsing command: SELECT *
FROM `batting` AS `zzz3`
WHERE (0 = 1)
17/01/13 23:03:23 INFO ParseDriver: Parse Completed
17/01/13 23:03:23 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/01/13 23:03:23 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/01/13 23:03:23 INFO SparkContext: Starting job: collect at utils.scala:59
17/01/13 23:03:23 INFO DAGScheduler: Got job 15 (collect at utils.scala:59) with 1 output partitions
17/01/13 23:03:23 INFO DAGScheduler: Final stage: ResultStage 21 (collect at utils.scala:59)
17/01/13 23:03:23 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:23 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:23 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[87] at map at utils.scala:56), which has no missing parents
17/01/13 23:03:23 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 5.4 KB, free 28.7 MB)
17/01/13 23:03:23 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 3.0 KB, free 28.7 MB)
17/01/13 23:03:23 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on localhost:63586 (size: 3.0 KB, free: 483.1 MB)
17/01/13 23:03:23 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[87] at map at utils.scala:56)
17/01/13 23:03:23 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks
17/01/13 23:03:23 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 27, localhost, partition 0,PROCESS_LOCAL, 2728 bytes)
17/01/13 23:03:23 INFO Executor: Running task 0.0 in stage 21.0 (TID 27)
17/01/13 23:03:23 INFO Executor: Finished task 0.0 in stage 21.0 (TID 27). 1087 bytes result sent to driver
17/01/13 23:03:23 INFO DAGScheduler: ResultStage 21 (collect at utils.scala:59) finished in 0.003 s
17/01/13 23:03:23 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 27) in 3 ms on localhost (1/1)
17/01/13 23:03:23 INFO DAGScheduler: Job 15 finished: collect at utils.scala:59, took 0.008757 s
17/01/13 23:03:23 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
17/01/13 23:03:23 INFO ParseDriver: Parsing command: SELECT *
FROM `flights`
WHERE (`dep_delay` = 2.0)
17/01/13 23:03:23 INFO ParseDriver: Parse Completed
17/01/13 23:03:23 INFO InMemoryColumnarTableScan: Predicate (dep_delay#146 = 2.0) generates partition filter: ((dep_delay.lowerBound#1071 <= 2.0) && (2.0 <= dep_delay.upperBound#1070))
17/01/13 23:03:23 INFO SparkContext: Starting job: count at null:-2
17/01/13 23:03:23 INFO DAGScheduler: Registering RDD 92 (count at null:-2)
17/01/13 23:03:23 INFO DAGScheduler: Got job 16 (count at null:-2) with 1 output partitions
17/01/13 23:03:23 INFO DAGScheduler: Final stage: ResultStage 23 (count at null:-2)
17/01/13 23:03:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 22)
17/01/13 23:03:23 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 22)
17/01/13 23:03:23 INFO DAGScheduler: Submitting ShuffleMapStage 22 (MapPartitionsRDD[92] at count at null:-2), which has no missing parents
17/01/13 23:03:23 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 28.8 KB, free 28.7 MB)
17/01/13 23:03:23 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 11.7 KB, free 28.7 MB)
17/01/13 23:03:23 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on localhost:63586 (size: 11.7 KB, free: 483.1 MB)
17/01/13 23:03:23 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:23 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[92] at count at null:-2)
17/01/13 23:03:23 INFO TaskSchedulerImpl: Adding task set 22.0 with 2 tasks
17/01/13 23:03:23 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 28, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:23 INFO TaskSetManager: Starting task 1.0 in stage 22.0 (TID 29, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:23 INFO Executor: Running task 0.0 in stage 22.0 (TID 28)
17/01/13 23:03:24 INFO BlockManager: Found block rdd_42_0 locally
17/01/13 23:03:24 INFO Executor: Running task 1.0 in stage 22.0 (TID 29)
17/01/13 23:03:24 INFO BlockManager: Found block rdd_42_1 locally
17/01/13 23:03:24 INFO GeneratePredicate: Code generated in 22.245527 ms
17/01/13 23:03:24 INFO GenerateColumnAccessor: Code generated in 13.969908 ms
17/01/13 23:03:24 INFO GeneratePredicate: Code generated in 5.355515 ms
17/01/13 23:03:24 INFO Executor: Finished task 1.0 in stage 22.0 (TID 29). 2808 bytes result sent to driver
17/01/13 23:03:24 INFO TaskSetManager: Finished task 1.0 in stage 22.0 (TID 29) in 159 ms on localhost (1/2)
17/01/13 23:03:24 INFO Executor: Finished task 0.0 in stage 22.0 (TID 28). 2808 bytes result sent to driver
17/01/13 23:03:24 INFO DAGScheduler: ShuffleMapStage 22 (count at null:-2) finished in 0.179 s
17/01/13 23:03:24 INFO DAGScheduler: looking for newly runnable stages
17/01/13 23:03:24 INFO DAGScheduler: running: Set()
17/01/13 23:03:24 INFO DAGScheduler: waiting: Set(ResultStage 23)
17/01/13 23:03:24 INFO DAGScheduler: failed: Set()
17/01/13 23:03:24 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[95] at count at null:-2), which has no missing parents
17/01/13 23:03:24 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 28) in 179 ms on localhost (2/2)
17/01/13 23:03:24 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
17/01/13 23:03:24 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 10.8 KB, free 28.8 MB)
17/01/13 23:03:24 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 5.3 KB, free 28.8 MB)
17/01/13 23:03:24 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on localhost:63586 (size: 5.3 KB, free: 483.1 MB)
17/01/13 23:03:24 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[95] at count at null:-2)
17/01/13 23:03:24 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks
17/01/13 23:03:24 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 30, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 23:03:24 INFO Executor: Running task 0.0 in stage 23.0 (TID 30)
17/01/13 23:03:24 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:24 INFO Executor: Finished task 0.0 in stage 23.0 (TID 30). 1959 bytes result sent to driver
17/01/13 23:03:24 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 30) in 8 ms on localhost (1/1)
17/01/13 23:03:24 INFO DAGScheduler: ResultStage 23 (count at null:-2) finished in 0.008 s
17/01/13 23:03:24 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
17/01/13 23:03:24 INFO DAGScheduler: Job 16 finished: count at null:-2, took 0.201930 s
17/01/13 23:03:24 INFO ParseDriver: Parsing command: SELECT *
FROM `flights`
WHERE (`dep_delay` = 2.0)
17/01/13 23:03:24 INFO ParseDriver: Parse Completed
17/01/13 23:03:24 INFO InMemoryColumnarTableScan: Predicate (dep_delay#146 = 2.0) generates partition filter: ((dep_delay.lowerBound#1170 <= 2.0) && (2.0 <= dep_delay.upperBound#1169))
17/01/13 23:03:24 INFO SparkContext: Starting job: count at null:-2
17/01/13 23:03:24 INFO DAGScheduler: Registering RDD 100 (count at null:-2)
17/01/13 23:03:24 INFO DAGScheduler: Got job 17 (count at null:-2) with 1 output partitions
17/01/13 23:03:24 INFO DAGScheduler: Final stage: ResultStage 25 (count at null:-2)
17/01/13 23:03:24 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 24)
17/01/13 23:03:24 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 24)
17/01/13 23:03:24 INFO DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[100] at count at null:-2), which has no missing parents
17/01/13 23:03:24 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 28.8 KB, free 28.8 MB)
17/01/13 23:03:24 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 11.8 KB, free 28.8 MB)
17/01/13 23:03:24 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on localhost:63586 (size: 11.8 KB, free: 483.1 MB)
17/01/13 23:03:24 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:24 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[100] at count at null:-2)
17/01/13 23:03:24 INFO TaskSchedulerImpl: Adding task set 24.0 with 2 tasks
17/01/13 23:03:24 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 31, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:24 INFO TaskSetManager: Starting task 1.0 in stage 24.0 (TID 32, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:24 INFO Executor: Running task 0.0 in stage 24.0 (TID 31)
17/01/13 23:03:24 INFO Executor: Running task 1.0 in stage 24.0 (TID 32)
17/01/13 23:03:24 INFO BlockManager: Found block rdd_42_0 locally
17/01/13 23:03:24 INFO BlockManager: Found block rdd_42_1 locally
17/01/13 23:03:24 INFO Executor: Finished task 1.0 in stage 24.0 (TID 32). 2808 bytes result sent to driver
17/01/13 23:03:24 INFO TaskSetManager: Finished task 1.0 in stage 24.0 (TID 32) in 36 ms on localhost (1/2)
17/01/13 23:03:24 INFO Executor: Finished task 0.0 in stage 24.0 (TID 31). 2808 bytes result sent to driver
17/01/13 23:03:24 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 31) in 49 ms on localhost (2/2)
17/01/13 23:03:24 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
17/01/13 23:03:24 INFO DAGScheduler: ShuffleMapStage 24 (count at null:-2) finished in 0.049 s
17/01/13 23:03:24 INFO DAGScheduler: looking for newly runnable stages
17/01/13 23:03:24 INFO DAGScheduler: running: Set()
17/01/13 23:03:24 INFO DAGScheduler: waiting: Set(ResultStage 25)
17/01/13 23:03:24 INFO DAGScheduler: failed: Set()
17/01/13 23:03:24 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[103] at count at null:-2), which has no missing parents
17/01/13 23:03:24 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 10.8 KB, free 28.8 MB)
17/01/13 23:03:24 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 5.3 KB, free 28.8 MB)
17/01/13 23:03:24 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on localhost:63586 (size: 5.3 KB, free: 483.1 MB)
17/01/13 23:03:24 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[103] at count at null:-2)
17/01/13 23:03:24 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks
17/01/13 23:03:24 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 33, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 23:03:24 INFO Executor: Running task 0.0 in stage 25.0 (TID 33)
17/01/13 23:03:24 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/01/13 23:03:24 INFO Executor: Finished task 0.0 in stage 25.0 (TID 33). 1959 bytes result sent to driver
17/01/13 23:03:24 INFO DAGScheduler: ResultStage 25 (count at null:-2) finished in 0.011 s
17/01/13 23:03:24 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 33) in 11 ms on localhost (1/1)
17/01/13 23:03:24 INFO DAGScheduler: Job 17 finished: count at null:-2, took 0.070817 s
17/01/13 23:03:24 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
17/01/13 23:03:24 INFO ParseDriver: Parsing command: SELECT *
FROM (SELECT *
FROM `flights`
WHERE (`dep_delay` = 2.0)) `ldrpkgafsk`
LIMIT 10
17/01/13 23:03:24 INFO ParseDriver: Parse Completed
17/01/13 23:03:24 INFO InMemoryColumnarTableScan: Predicate (dep_delay#146 = 2.0) generates partition filter: ((dep_delay.lowerBound#1268 <= 2.0) && (2.0 <= dep_delay.upperBound#1267))
17/01/13 23:03:24 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 23:03:24 INFO DAGScheduler: Got job 18 (collect at utils.scala:195) with 1 output partitions
17/01/13 23:03:24 INFO DAGScheduler: Final stage: ResultStage 26 (collect at utils.scala:195)
17/01/13 23:03:24 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:24 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:24 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[107] at collect at utils.scala:195), which has no missing parents
17/01/13 23:03:24 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 25.3 KB, free 28.8 MB)
17/01/13 23:03:24 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 10.2 KB, free 28.9 MB)
17/01/13 23:03:24 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on localhost:63586 (size: 10.2 KB, free: 483.1 MB)
17/01/13 23:03:24 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[107] at collect at utils.scala:195)
17/01/13 23:03:24 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks
17/01/13 23:03:24 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 34, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 23:03:24 INFO Executor: Running task 0.0 in stage 26.0 (TID 34)
17/01/13 23:03:24 INFO BlockManager: Found block rdd_42_0 locally
17/01/13 23:03:24 INFO GenerateColumnAccessor: Code generated in 36.277302 ms
17/01/13 23:03:24 INFO GeneratePredicate: Code generated in 4.566609 ms
17/01/13 23:03:24 INFO GenerateSafeProjection: Code generated in 14.222068 ms
17/01/13 23:03:24 INFO Executor: Finished task 0.0 in stage 26.0 (TID 34). 5325 bytes result sent to driver
17/01/13 23:03:24 INFO DAGScheduler: ResultStage 26 (collect at utils.scala:195) finished in 0.102 s
17/01/13 23:03:24 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 34) in 101 ms on localhost (1/1)
17/01/13 23:03:24 INFO DAGScheduler: Job 18 finished: collect at utils.scala:195, took 0.108210 s
17/01/13 23:03:24 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
17/01/13 23:03:24 INFO ParseDriver: Parsing command: SELECT *
FROM (SELECT `tailnum`, count(*) AS `count`, AVG(`distance`) AS `dist`, AVG(`arr_delay`) AS `delay`
FROM `flights`
GROUP BY `tailnum`) `pmkqzxzzuf`
WHERE ((`count` > 20.0) AND (`dist` < 2000.0) AND (NOT((`delay`) IS NULL)))
17/01/13 23:03:24 INFO ParseDriver: Parse Completed
17/01/13 23:03:24 INFO BlockManagerInfo: Removed broadcast_32_piece0 on localhost:63586 in memory (size: 5.3 KB, free: 483.1 MB)
17/01/13 23:03:24 INFO ContextCleaner: Cleaned accumulator 91
17/01/13 23:03:24 INFO BlockManagerInfo: Removed broadcast_31_piece0 on localhost:63586 in memory (size: 11.7 KB, free: 483.1 MB)
17/01/13 23:03:24 INFO ContextCleaner: Cleaned accumulator 90
17/01/13 23:03:24 INFO ContextCleaner: Cleaned shuffle 6
17/01/13 23:03:24 INFO ContextCleaner: Cleaned accumulator 89
17/01/13 23:03:24 INFO ContextCleaner: Cleaned accumulator 88
17/01/13 23:03:24 INFO ContextCleaner: Cleaned accumulator 87
17/01/13 23:03:24 INFO ContextCleaner: Cleaned accumulator 86
17/01/13 23:03:24 INFO ContextCleaner: Cleaned accumulator 85
17/01/13 23:03:24 INFO ContextCleaner: Cleaned accumulator 84
17/01/13 23:03:24 INFO ContextCleaner: Cleaned accumulator 83
17/01/13 23:03:24 INFO ContextCleaner: Cleaned accumulator 82
17/01/13 23:03:24 INFO ContextCleaner: Cleaned accumulator 81
17/01/13 23:03:24 INFO ContextCleaner: Cleaned accumulator 80
17/01/13 23:03:24 INFO ContextCleaner: Cleaned accumulator 79
17/01/13 23:03:24 INFO BlockManagerInfo: Removed broadcast_30_piece0 on localhost:63586 in memory (size: 3.0 KB, free: 483.1 MB)
17/01/13 23:03:24 INFO ContextCleaner: Cleaned accumulator 78
17/01/13 23:03:24 INFO BlockManagerInfo: Removed broadcast_27_piece0 on localhost:63586 in memory (size: 4.6 KB, free: 483.1 MB)
17/01/13 23:03:24 INFO BlockManagerInfo: Removed broadcast_35_piece0 on localhost:63586 in memory (size: 10.2 KB, free: 483.1 MB)
17/01/13 23:03:24 INFO ContextCleaner: Cleaned accumulator 107
17/01/13 23:03:24 INFO BlockManagerInfo: Removed broadcast_34_piece0 on localhost:63586 in memory (size: 5.3 KB, free: 483.1 MB)
17/01/13 23:03:24 INFO ContextCleaner: Cleaned accumulator 104
17/01/13 23:03:24 INFO BlockManagerInfo: Removed broadcast_33_piece0 on localhost:63586 in memory (size: 11.8 KB, free: 483.1 MB)
17/01/13 23:03:24 INFO ContextCleaner: Cleaned accumulator 103
17/01/13 23:03:24 INFO ContextCleaner: Cleaned shuffle 7
17/01/13 23:03:24 INFO ContextCleaner: Cleaned accumulator 102
17/01/13 23:03:24 INFO ContextCleaner: Cleaned accumulator 101
17/01/13 23:03:24 INFO ContextCleaner: Cleaned accumulator 100
17/01/13 23:03:24 INFO ContextCleaner: Cleaned accumulator 99
17/01/13 23:03:24 INFO ContextCleaner: Cleaned accumulator 98
17/01/13 23:03:24 INFO ContextCleaner: Cleaned accumulator 97
17/01/13 23:03:24 INFO ContextCleaner: Cleaned accumulator 96
17/01/13 23:03:24 INFO ContextCleaner: Cleaned accumulator 95
17/01/13 23:03:24 INFO ContextCleaner: Cleaned accumulator 94
17/01/13 23:03:24 INFO ContextCleaner: Cleaned accumulator 93
17/01/13 23:03:24 INFO ContextCleaner: Cleaned accumulator 92
17/01/13 23:03:24 INFO ContextCleaner: Cleaned accumulator 77
17/01/13 23:03:24 INFO BlockManagerInfo: Removed broadcast_29_piece0 on localhost:63586 in memory (size: 4.7 KB, free: 483.1 MB)
17/01/13 23:03:24 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 23:03:24 INFO DAGScheduler: Registering RDD 110 (collect at utils.scala:195)
17/01/13 23:03:24 INFO DAGScheduler: Got job 19 (collect at utils.scala:195) with 4 output partitions
17/01/13 23:03:24 INFO DAGScheduler: Final stage: ResultStage 28 (collect at utils.scala:195)
17/01/13 23:03:24 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)
17/01/13 23:03:24 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 27)
17/01/13 23:03:24 INFO DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[110] at collect at utils.scala:195), which has no missing parents
17/01/13 23:03:24 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 28.1 KB, free 28.7 MB)
17/01/13 23:03:24 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 11.4 KB, free 28.7 MB)
17/01/13 23:03:24 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on localhost:63586 (size: 11.4 KB, free: 483.1 MB)
17/01/13 23:03:24 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:24 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[110] at collect at utils.scala:195)
17/01/13 23:03:24 INFO TaskSchedulerImpl: Adding task set 27.0 with 2 tasks
17/01/13 23:03:24 INFO ContextCleaner: Cleaned accumulator 76
17/01/13 23:03:24 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 35, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:24 INFO TaskSetManager: Starting task 1.0 in stage 27.0 (TID 36, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:24 INFO Executor: Running task 0.0 in stage 27.0 (TID 35)
17/01/13 23:03:24 INFO Executor: Running task 1.0 in stage 27.0 (TID 36)
17/01/13 23:03:24 INFO BlockManagerInfo: Removed broadcast_28_piece0 on localhost:63586 in memory (size: 11.2 KB, free: 483.1 MB)
17/01/13 23:03:24 INFO ContextCleaner: Cleaned accumulator 75
17/01/13 23:03:24 INFO ContextCleaner: Cleaned shuffle 5
17/01/13 23:03:24 INFO BlockManager: Found block rdd_42_0 locally
17/01/13 23:03:24 INFO BlockManager: Found block rdd_42_1 locally
17/01/13 23:03:24 INFO GenerateColumnAccessor: Code generated in 17.163932 ms
17/01/13 23:03:24 INFO GenerateMutableProjection: Code generated in 9.131085 ms
17/01/13 23:03:24 INFO GenerateMutableProjection: Code generated in 13.635829 ms
17/01/13 23:03:24 INFO GenerateUnsafeRowJoiner: Code generated in 6.667088 ms
17/01/13 23:03:24 INFO GenerateUnsafeProjection: Code generated in 8.526073 ms
17/01/13 23:03:25 INFO GenerateMutableProjection: Code generated in 6.228475 ms
17/01/13 23:03:25 INFO Executor: Finished task 1.0 in stage 27.0 (TID 36). 2682 bytes result sent to driver
17/01/13 23:03:25 INFO TaskSetManager: Finished task 1.0 in stage 27.0 (TID 36) in 398 ms on localhost (1/2)
17/01/13 23:03:25 INFO Executor: Finished task 0.0 in stage 27.0 (TID 35). 2682 bytes result sent to driver
17/01/13 23:03:25 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 35) in 405 ms on localhost (2/2)
17/01/13 23:03:25 INFO DAGScheduler: ShuffleMapStage 27 (collect at utils.scala:195) finished in 0.405 s
17/01/13 23:03:25 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
17/01/13 23:03:25 INFO DAGScheduler: looking for newly runnable stages
17/01/13 23:03:25 INFO DAGScheduler: running: Set()
17/01/13 23:03:25 INFO DAGScheduler: waiting: Set(ResultStage 28)
17/01/13 23:03:25 INFO DAGScheduler: failed: Set()
17/01/13 23:03:25 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[115] at collect at utils.scala:195), which has no missing parents
17/01/13 23:03:25 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 13.5 KB, free 28.7 MB)
17/01/13 23:03:25 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 6.2 KB, free 28.7 MB)
17/01/13 23:03:25 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on localhost:63586 (size: 6.2 KB, free: 483.1 MB)
17/01/13 23:03:25 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:25 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 28 (MapPartitionsRDD[115] at collect at utils.scala:195)
17/01/13 23:03:25 INFO TaskSchedulerImpl: Adding task set 28.0 with 4 tasks
17/01/13 23:03:25 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 37, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 23:03:25 INFO TaskSetManager: Starting task 1.0 in stage 28.0 (TID 38, localhost, partition 1,NODE_LOCAL, 2242 bytes)
17/01/13 23:03:25 INFO TaskSetManager: Starting task 2.0 in stage 28.0 (TID 39, localhost, partition 2,NODE_LOCAL, 2242 bytes)
17/01/13 23:03:25 INFO TaskSetManager: Starting task 3.0 in stage 28.0 (TID 40, localhost, partition 3,NODE_LOCAL, 2242 bytes)
17/01/13 23:03:25 INFO Executor: Running task 1.0 in stage 28.0 (TID 38)
17/01/13 23:03:25 INFO Executor: Running task 0.0 in stage 28.0 (TID 37)
17/01/13 23:03:25 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:25 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:25 INFO Executor: Running task 2.0 in stage 28.0 (TID 39)
17/01/13 23:03:25 INFO Executor: Running task 3.0 in stage 28.0 (TID 40)
17/01/13 23:03:25 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:25 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:25 INFO GenerateMutableProjection: Code generated in 15.397107 ms
17/01/13 23:03:25 INFO GenerateMutableProjection: Code generated in 9.545806 ms
17/01/13 23:03:25 INFO GenerateUnsafeProjection: Code generated in 7.521273 ms
17/01/13 23:03:25 INFO GeneratePredicate: Code generated in 20.42153 ms
17/01/13 23:03:25 INFO Executor: Finished task 1.0 in stage 28.0 (TID 38). 52153 bytes result sent to driver
17/01/13 23:03:25 INFO Executor: Finished task 0.0 in stage 28.0 (TID 37). 49931 bytes result sent to driver
17/01/13 23:03:25 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 37) in 108 ms on localhost (1/4)
17/01/13 23:03:25 INFO TaskSetManager: Finished task 1.0 in stage 28.0 (TID 38) in 111 ms on localhost (2/4)
17/01/13 23:03:25 INFO Executor: Finished task 2.0 in stage 28.0 (TID 39). 50586 bytes result sent to driver
17/01/13 23:03:25 INFO Executor: Finished task 3.0 in stage 28.0 (TID 40). 48431 bytes result sent to driver
17/01/13 23:03:25 INFO TaskSetManager: Finished task 2.0 in stage 28.0 (TID 39) in 114 ms on localhost (3/4)
17/01/13 23:03:25 INFO DAGScheduler: ResultStage 28 (collect at utils.scala:195) finished in 0.116 s
17/01/13 23:03:25 INFO DAGScheduler: Job 19 finished: collect at utils.scala:195, took 0.533568 s
17/01/13 23:03:25 INFO TaskSetManager: Finished task 3.0 in stage 28.0 (TID 40) in 115 ms on localhost (4/4)
17/01/13 23:03:25 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
17/01/13 23:03:27 INFO ParseDriver: Parsing command: SELECT `playerID`, `yearID`, `teamID`, `G`, `AB`, `R`, `H`
FROM (SELECT `playerID`, `yearID`, `teamID`, `G`, `AB`, `R`, `H`, rank() OVER (PARTITION BY `playerID` ORDER BY `H` DESC) AS `zzz4`
FROM (SELECT *
FROM (SELECT `playerID` AS `playerID`, `yearID` AS `yearID`, `teamID` AS `teamID`, `G` AS `G`, `AB` AS `AB`, `R` AS `R`, `H` AS `H`
FROM `batting`) `ddltnlwatm`
ORDER BY `playerID`, `yearID`, `teamID`) `tqlvxcustm`) `ppbcvnurrj`
WHERE (`zzz4` <= 2.0 AND `H` > 0.0)
17/01/13 23:03:27 INFO ParseDriver: Parse Completed
17/01/13 23:03:27 INFO SparkContext: Starting job: count at null:-2
17/01/13 23:03:27 INFO DAGScheduler: Got job 20 (count at null:-2) with 2 output partitions
17/01/13 23:03:27 INFO DAGScheduler: Final stage: ResultStage 29 (count at null:-2)
17/01/13 23:03:27 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:27 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:27 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[121] at count at null:-2), which has no missing parents
17/01/13 23:03:27 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 26.4 KB, free 28.7 MB)
17/01/13 23:03:27 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 10.5 KB, free 28.7 MB)
17/01/13 23:03:27 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on localhost:63586 (size: 10.5 KB, free: 483.1 MB)
17/01/13 23:03:27 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:27 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 29 (MapPartitionsRDD[121] at count at null:-2)
17/01/13 23:03:27 INFO TaskSchedulerImpl: Adding task set 29.0 with 2 tasks
17/01/13 23:03:27 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 41, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 23:03:27 INFO TaskSetManager: Starting task 1.0 in stage 29.0 (TID 42, localhost, partition 1,PROCESS_LOCAL, 2481 bytes)
17/01/13 23:03:27 INFO Executor: Running task 0.0 in stage 29.0 (TID 41)
17/01/13 23:03:27 INFO Executor: Running task 1.0 in stage 29.0 (TID 42)
17/01/13 23:03:27 INFO BlockManager: Found block rdd_70_1 locally
17/01/13 23:03:27 INFO BlockManager: Found block rdd_70_0 locally
17/01/13 23:03:27 INFO GenerateColumnAccessor: Code generated in 12.185163 ms
17/01/13 23:03:27 INFO GenerateUnsafeProjection: Code generated in 13.482228 ms
17/01/13 23:03:27 INFO GenerateSafeProjection: Code generated in 5.744209 ms
17/01/13 23:03:28 INFO Executor: Finished task 0.0 in stage 29.0 (TID 41). 13905 bytes result sent to driver
17/01/13 23:03:28 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 41) in 265 ms on localhost (1/2)
17/01/13 23:03:28 INFO Executor: Finished task 1.0 in stage 29.0 (TID 42). 13552 bytes result sent to driver
17/01/13 23:03:28 INFO DAGScheduler: ResultStage 29 (count at null:-2) finished in 0.272 s
17/01/13 23:03:28 INFO DAGScheduler: Job 20 finished: count at null:-2, took 0.278423 s
17/01/13 23:03:28 INFO TaskSetManager: Finished task 1.0 in stage 29.0 (TID 42) in 271 ms on localhost (2/2)
17/01/13 23:03:28 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool 
17/01/13 23:03:28 INFO SparkContext: Starting job: count at null:-2
17/01/13 23:03:28 INFO DAGScheduler: Registering RDD 122 (count at null:-2)
17/01/13 23:03:28 INFO DAGScheduler: Registering RDD 126 (count at null:-2)
17/01/13 23:03:28 INFO DAGScheduler: Registering RDD 133 (count at null:-2)
17/01/13 23:03:28 INFO DAGScheduler: Got job 21 (count at null:-2) with 1 output partitions
17/01/13 23:03:28 INFO DAGScheduler: Final stage: ResultStage 33 (count at null:-2)
17/01/13 23:03:28 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 32)
17/01/13 23:03:28 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 32)
17/01/13 23:03:28 INFO DAGScheduler: Submitting ShuffleMapStage 30 (MapPartitionsRDD[122] at count at null:-2), which has no missing parents
17/01/13 23:03:28 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 28.9 KB, free 28.8 MB)
17/01/13 23:03:28 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 11.8 KB, free 28.8 MB)
17/01/13 23:03:28 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on localhost:63586 (size: 11.8 KB, free: 483.1 MB)
17/01/13 23:03:28 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:28 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 30 (MapPartitionsRDD[122] at count at null:-2)
17/01/13 23:03:28 INFO TaskSchedulerImpl: Adding task set 30.0 with 2 tasks
17/01/13 23:03:28 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 43, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:28 INFO TaskSetManager: Starting task 1.0 in stage 30.0 (TID 44, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:28 INFO Executor: Running task 0.0 in stage 30.0 (TID 43)
17/01/13 23:03:28 INFO Executor: Running task 1.0 in stage 30.0 (TID 44)
17/01/13 23:03:28 INFO BlockManager: Found block rdd_70_1 locally
17/01/13 23:03:28 INFO BlockManager: Found block rdd_70_0 locally
17/01/13 23:03:28 INFO Executor: Finished task 1.0 in stage 30.0 (TID 44). 2580 bytes result sent to driver
17/01/13 23:03:28 INFO TaskSetManager: Finished task 1.0 in stage 30.0 (TID 44) in 486 ms on localhost (1/2)
17/01/13 23:03:28 INFO Executor: Finished task 0.0 in stage 30.0 (TID 43). 2580 bytes result sent to driver
17/01/13 23:03:28 INFO DAGScheduler: ShuffleMapStage 30 (count at null:-2) finished in 0.524 s
17/01/13 23:03:28 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 43) in 523 ms on localhost (2/2)
17/01/13 23:03:28 INFO DAGScheduler: looking for newly runnable stages
17/01/13 23:03:28 INFO DAGScheduler: running: Set()
17/01/13 23:03:28 INFO DAGScheduler: waiting: Set(ResultStage 33, ShuffleMapStage 31, ShuffleMapStage 32)
17/01/13 23:03:28 INFO DAGScheduler: failed: Set()
17/01/13 23:03:28 INFO DAGScheduler: Submitting ShuffleMapStage 31 (MapPartitionsRDD[126] at count at null:-2), which has no missing parents
17/01/13 23:03:28 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool 
17/01/13 23:03:28 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 10.3 KB, free 28.8 MB)
17/01/13 23:03:28 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 5.2 KB, free 28.8 MB)
17/01/13 23:03:28 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on localhost:63586 (size: 5.2 KB, free: 483.1 MB)
17/01/13 23:03:28 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:28 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 31 (MapPartitionsRDD[126] at count at null:-2)
17/01/13 23:03:28 INFO TaskSchedulerImpl: Adding task set 31.0 with 4 tasks
17/01/13 23:03:28 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 45, localhost, partition 0,NODE_LOCAL, 2231 bytes)
17/01/13 23:03:28 INFO TaskSetManager: Starting task 1.0 in stage 31.0 (TID 46, localhost, partition 1,NODE_LOCAL, 2231 bytes)
17/01/13 23:03:28 INFO TaskSetManager: Starting task 2.0 in stage 31.0 (TID 47, localhost, partition 2,NODE_LOCAL, 2231 bytes)
17/01/13 23:03:28 INFO TaskSetManager: Starting task 3.0 in stage 31.0 (TID 48, localhost, partition 3,NODE_LOCAL, 2231 bytes)
17/01/13 23:03:28 INFO Executor: Running task 0.0 in stage 31.0 (TID 45)
17/01/13 23:03:28 INFO Executor: Running task 2.0 in stage 31.0 (TID 47)
17/01/13 23:03:28 INFO Executor: Running task 3.0 in stage 31.0 (TID 48)
17/01/13 23:03:28 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:28 INFO Executor: Running task 1.0 in stage 31.0 (TID 46)
17/01/13 23:03:28 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:28 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:28 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:28 INFO GenerateUnsafeProjection: Code generated in 5.361062 ms
17/01/13 23:03:28 INFO GenerateOrdering: Code generated in 17.018866 ms
17/01/13 23:03:28 INFO GenerateUnsafeProjection: Code generated in 4.833703 ms
17/01/13 23:03:29 INFO Executor: Finished task 2.0 in stage 31.0 (TID 47). 1660 bytes result sent to driver
17/01/13 23:03:29 INFO TaskSetManager: Finished task 2.0 in stage 31.0 (TID 47) in 708 ms on localhost (1/4)
17/01/13 23:03:29 INFO Executor: Finished task 3.0 in stage 31.0 (TID 48). 1660 bytes result sent to driver
17/01/13 23:03:29 INFO TaskSetManager: Finished task 3.0 in stage 31.0 (TID 48) in 772 ms on localhost (2/4)
17/01/13 23:03:29 INFO Executor: Finished task 0.0 in stage 31.0 (TID 45). 1660 bytes result sent to driver
17/01/13 23:03:29 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 45) in 859 ms on localhost (3/4)
17/01/13 23:03:29 INFO Executor: Finished task 1.0 in stage 31.0 (TID 46). 1660 bytes result sent to driver
17/01/13 23:03:29 INFO TaskSetManager: Finished task 1.0 in stage 31.0 (TID 46) in 888 ms on localhost (4/4)
17/01/13 23:03:29 INFO DAGScheduler: ShuffleMapStage 31 (count at null:-2) finished in 0.889 s
17/01/13 23:03:29 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool 
17/01/13 23:03:29 INFO DAGScheduler: looking for newly runnable stages
17/01/13 23:03:29 INFO DAGScheduler: running: Set()
17/01/13 23:03:29 INFO DAGScheduler: waiting: Set(ResultStage 33, ShuffleMapStage 32)
17/01/13 23:03:29 INFO DAGScheduler: failed: Set()
17/01/13 23:03:29 INFO DAGScheduler: Submitting ShuffleMapStage 32 (MapPartitionsRDD[133] at count at null:-2), which has no missing parents
17/01/13 23:03:29 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 17.9 KB, free 28.8 MB)
17/01/13 23:03:29 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 8.1 KB, free 28.8 MB)
17/01/13 23:03:29 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on localhost:63586 (size: 8.1 KB, free: 483.1 MB)
17/01/13 23:03:29 INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:29 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 32 (MapPartitionsRDD[133] at count at null:-2)
17/01/13 23:03:29 INFO TaskSchedulerImpl: Adding task set 32.0 with 4 tasks
17/01/13 23:03:29 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 49, localhost, partition 0,NODE_LOCAL, 2231 bytes)
17/01/13 23:03:29 INFO TaskSetManager: Starting task 1.0 in stage 32.0 (TID 50, localhost, partition 1,NODE_LOCAL, 2231 bytes)
17/01/13 23:03:29 INFO TaskSetManager: Starting task 2.0 in stage 32.0 (TID 51, localhost, partition 2,NODE_LOCAL, 2231 bytes)
17/01/13 23:03:29 INFO TaskSetManager: Starting task 3.0 in stage 32.0 (TID 52, localhost, partition 3,NODE_LOCAL, 2231 bytes)
17/01/13 23:03:29 INFO Executor: Running task 0.0 in stage 32.0 (TID 49)
17/01/13 23:03:29 INFO Executor: Running task 1.0 in stage 32.0 (TID 50)
17/01/13 23:03:29 INFO Executor: Running task 2.0 in stage 32.0 (TID 51)
17/01/13 23:03:29 INFO Executor: Running task 3.0 in stage 32.0 (TID 52)
17/01/13 23:03:29 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/01/13 23:03:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/01/13 23:03:29 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/01/13 23:03:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:29 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/01/13 23:03:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:29 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/01/13 23:03:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/01/13 23:03:29 INFO GenerateOrdering: Code generated in 11.843403 ms
17/01/13 23:03:29 INFO BlockManagerInfo: Removed broadcast_40_piece0 on localhost:63586 in memory (size: 5.2 KB, free: 483.1 MB)
17/01/13 23:03:29 INFO BlockManagerInfo: Removed broadcast_39_piece0 on localhost:63586 in memory (size: 11.8 KB, free: 483.1 MB)
17/01/13 23:03:29 INFO BlockManagerInfo: Removed broadcast_38_piece0 on localhost:63586 in memory (size: 10.5 KB, free: 483.1 MB)
17/01/13 23:03:29 INFO BlockManagerInfo: Removed broadcast_36_piece0 on localhost:63586 in memory (size: 11.4 KB, free: 483.1 MB)
17/01/13 23:03:29 INFO ContextCleaner: Cleaned shuffle 8
17/01/13 23:03:29 INFO BlockManagerInfo: Removed broadcast_37_piece0 on localhost:63586 in memory (size: 6.2 KB, free: 483.1 MB)
17/01/13 23:03:29 INFO GenerateMutableProjection: Code generated in 20.183023 ms
17/01/13 23:03:29 INFO GeneratePredicate: Code generated in 8.869539 ms
17/01/13 23:03:30 INFO Executor: Finished task 0.0 in stage 32.0 (TID 49). 1965 bytes result sent to driver
17/01/13 23:03:30 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 49) in 520 ms on localhost (1/4)
17/01/13 23:03:30 INFO Executor: Finished task 3.0 in stage 32.0 (TID 52). 1965 bytes result sent to driver
17/01/13 23:03:30 INFO TaskSetManager: Finished task 3.0 in stage 32.0 (TID 52) in 534 ms on localhost (2/4)
17/01/13 23:03:30 INFO Executor: Finished task 2.0 in stage 32.0 (TID 51). 1965 bytes result sent to driver
17/01/13 23:03:30 INFO TaskSetManager: Finished task 2.0 in stage 32.0 (TID 51) in 539 ms on localhost (3/4)
17/01/13 23:03:30 INFO Executor: Finished task 1.0 in stage 32.0 (TID 50). 1965 bytes result sent to driver
17/01/13 23:03:30 INFO TaskSetManager: Finished task 1.0 in stage 32.0 (TID 50) in 567 ms on localhost (4/4)
17/01/13 23:03:30 INFO DAGScheduler: ShuffleMapStage 32 (count at null:-2) finished in 0.569 s
17/01/13 23:03:30 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
17/01/13 23:03:30 INFO DAGScheduler: looking for newly runnable stages
17/01/13 23:03:30 INFO DAGScheduler: running: Set()
17/01/13 23:03:30 INFO DAGScheduler: waiting: Set(ResultStage 33)
17/01/13 23:03:30 INFO DAGScheduler: failed: Set()
17/01/13 23:03:30 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[136] at count at null:-2), which has no missing parents
17/01/13 23:03:30 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 15.8 KB, free 28.7 MB)
17/01/13 23:03:30 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 7.2 KB, free 28.7 MB)
17/01/13 23:03:30 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on localhost:63586 (size: 7.2 KB, free: 483.1 MB)
17/01/13 23:03:30 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[136] at count at null:-2)
17/01/13 23:03:30 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks
17/01/13 23:03:30 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 53, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 23:03:30 INFO Executor: Running task 0.0 in stage 33.0 (TID 53)
17/01/13 23:03:30 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/01/13 23:03:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:30 INFO Executor: Finished task 0.0 in stage 33.0 (TID 53). 2174 bytes result sent to driver
17/01/13 23:03:30 INFO DAGScheduler: ResultStage 33 (count at null:-2) finished in 0.014 s
17/01/13 23:03:30 INFO DAGScheduler: Job 21 finished: count at null:-2, took 2.022214 s
17/01/13 23:03:30 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 53) in 13 ms on localhost (1/1)
17/01/13 23:03:30 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool 
17/01/13 23:03:30 INFO ParseDriver: Parsing command: SELECT `playerID`, `yearID`, `teamID`, `G`, `AB`, `R`, `H`
FROM (SELECT `playerID`, `yearID`, `teamID`, `G`, `AB`, `R`, `H`, rank() OVER (PARTITION BY `playerID` ORDER BY `H` DESC) AS `zzz5`
FROM (SELECT *
FROM (SELECT `playerID` AS `playerID`, `yearID` AS `yearID`, `teamID` AS `teamID`, `G` AS `G`, `AB` AS `AB`, `R` AS `R`, `H` AS `H`
FROM `batting`) `enugctctbg`
ORDER BY `playerID`, `yearID`, `teamID`) `qvppybzeuc`) `yeshvbqqnh`
WHERE (`zzz5` <= 2.0 AND `H` > 0.0)
17/01/13 23:03:30 INFO ParseDriver: Parse Completed
17/01/13 23:03:30 INFO SparkContext: Starting job: count at null:-2
17/01/13 23:03:30 INFO DAGScheduler: Got job 22 (count at null:-2) with 2 output partitions
17/01/13 23:03:30 INFO DAGScheduler: Final stage: ResultStage 34 (count at null:-2)
17/01/13 23:03:30 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:30 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:30 INFO DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[142] at count at null:-2), which has no missing parents
17/01/13 23:03:30 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 26.4 KB, free 28.7 MB)
17/01/13 23:03:30 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 10.4 KB, free 28.7 MB)
17/01/13 23:03:30 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on localhost:63586 (size: 10.4 KB, free: 483.1 MB)
17/01/13 23:03:30 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:30 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 34 (MapPartitionsRDD[142] at count at null:-2)
17/01/13 23:03:30 INFO TaskSchedulerImpl: Adding task set 34.0 with 2 tasks
17/01/13 23:03:30 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 54, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 23:03:30 INFO TaskSetManager: Starting task 1.0 in stage 34.0 (TID 55, localhost, partition 1,PROCESS_LOCAL, 2481 bytes)
17/01/13 23:03:30 INFO Executor: Running task 0.0 in stage 34.0 (TID 54)
17/01/13 23:03:30 INFO Executor: Running task 1.0 in stage 34.0 (TID 55)
17/01/13 23:03:30 INFO BlockManager: Found block rdd_70_1 locally
17/01/13 23:03:30 INFO BlockManager: Found block rdd_70_0 locally
17/01/13 23:03:30 INFO Executor: Finished task 0.0 in stage 34.0 (TID 54). 13877 bytes result sent to driver
17/01/13 23:03:30 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 54) in 61 ms on localhost (1/2)
17/01/13 23:03:30 INFO Executor: Finished task 1.0 in stage 34.0 (TID 55). 13661 bytes result sent to driver
17/01/13 23:03:30 INFO DAGScheduler: ResultStage 34 (count at null:-2) finished in 0.081 s
17/01/13 23:03:30 INFO DAGScheduler: Job 22 finished: count at null:-2, took 0.085944 s
17/01/13 23:03:30 INFO TaskSetManager: Finished task 1.0 in stage 34.0 (TID 55) in 80 ms on localhost (2/2)
17/01/13 23:03:30 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool 
17/01/13 23:03:30 INFO SparkContext: Starting job: count at null:-2
17/01/13 23:03:30 INFO DAGScheduler: Registering RDD 143 (count at null:-2)
17/01/13 23:03:30 INFO DAGScheduler: Registering RDD 147 (count at null:-2)
17/01/13 23:03:30 INFO DAGScheduler: Registering RDD 154 (count at null:-2)
17/01/13 23:03:30 INFO DAGScheduler: Got job 23 (count at null:-2) with 1 output partitions
17/01/13 23:03:30 INFO DAGScheduler: Final stage: ResultStage 38 (count at null:-2)
17/01/13 23:03:30 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 37)
17/01/13 23:03:30 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 37)
17/01/13 23:03:30 INFO DAGScheduler: Submitting ShuffleMapStage 35 (MapPartitionsRDD[143] at count at null:-2), which has no missing parents
17/01/13 23:03:30 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 28.9 KB, free 28.7 MB)
17/01/13 23:03:30 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 11.7 KB, free 28.8 MB)
17/01/13 23:03:30 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on localhost:63586 (size: 11.7 KB, free: 483.1 MB)
17/01/13 23:03:30 INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:30 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 35 (MapPartitionsRDD[143] at count at null:-2)
17/01/13 23:03:30 INFO TaskSchedulerImpl: Adding task set 35.0 with 2 tasks
17/01/13 23:03:30 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 56, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:30 INFO TaskSetManager: Starting task 1.0 in stage 35.0 (TID 57, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:30 INFO Executor: Running task 0.0 in stage 35.0 (TID 56)
17/01/13 23:03:30 INFO BlockManager: Found block rdd_70_0 locally
17/01/13 23:03:30 INFO Executor: Running task 1.0 in stage 35.0 (TID 57)
17/01/13 23:03:30 INFO BlockManager: Found block rdd_70_1 locally
17/01/13 23:03:30 INFO Executor: Finished task 0.0 in stage 35.0 (TID 56). 2580 bytes result sent to driver
17/01/13 23:03:30 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 56) in 251 ms on localhost (1/2)
17/01/13 23:03:30 INFO Executor: Finished task 1.0 in stage 35.0 (TID 57). 2580 bytes result sent to driver
17/01/13 23:03:30 INFO TaskSetManager: Finished task 1.0 in stage 35.0 (TID 57) in 274 ms on localhost (2/2)
17/01/13 23:03:30 INFO DAGScheduler: ShuffleMapStage 35 (count at null:-2) finished in 0.275 s
17/01/13 23:03:30 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool 
17/01/13 23:03:30 INFO DAGScheduler: looking for newly runnable stages
17/01/13 23:03:30 INFO DAGScheduler: running: Set()
17/01/13 23:03:30 INFO DAGScheduler: waiting: Set(ShuffleMapStage 37, ResultStage 38, ShuffleMapStage 36)
17/01/13 23:03:30 INFO DAGScheduler: failed: Set()
17/01/13 23:03:30 INFO DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[147] at count at null:-2), which has no missing parents
17/01/13 23:03:30 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 10.3 KB, free 28.8 MB)
17/01/13 23:03:30 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 5.2 KB, free 28.8 MB)
17/01/13 23:03:30 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on localhost:63586 (size: 5.2 KB, free: 483.1 MB)
17/01/13 23:03:30 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:30 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[147] at count at null:-2)
17/01/13 23:03:30 INFO TaskSchedulerImpl: Adding task set 36.0 with 4 tasks
17/01/13 23:03:30 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 58, localhost, partition 0,NODE_LOCAL, 2231 bytes)
17/01/13 23:03:30 INFO TaskSetManager: Starting task 1.0 in stage 36.0 (TID 59, localhost, partition 1,NODE_LOCAL, 2231 bytes)
17/01/13 23:03:30 INFO TaskSetManager: Starting task 2.0 in stage 36.0 (TID 60, localhost, partition 2,NODE_LOCAL, 2231 bytes)
17/01/13 23:03:30 INFO TaskSetManager: Starting task 3.0 in stage 36.0 (TID 61, localhost, partition 3,NODE_LOCAL, 2231 bytes)
17/01/13 23:03:30 INFO Executor: Running task 0.0 in stage 36.0 (TID 58)
17/01/13 23:03:30 INFO Executor: Running task 3.0 in stage 36.0 (TID 61)
17/01/13 23:03:30 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:30 INFO Executor: Running task 2.0 in stage 36.0 (TID 60)
17/01/13 23:03:30 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/01/13 23:03:30 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:30 INFO Executor: Running task 1.0 in stage 36.0 (TID 59)
17/01/13 23:03:30 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:31 INFO Executor: Finished task 3.0 in stage 36.0 (TID 61). 1660 bytes result sent to driver
17/01/13 23:03:31 INFO TaskSetManager: Finished task 3.0 in stage 36.0 (TID 61) in 417 ms on localhost (1/4)
17/01/13 23:03:31 INFO Executor: Finished task 0.0 in stage 36.0 (TID 58). 1660 bytes result sent to driver
17/01/13 23:03:31 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 58) in 484 ms on localhost (2/4)
17/01/13 23:03:31 INFO Executor: Finished task 2.0 in stage 36.0 (TID 60). 1660 bytes result sent to driver
17/01/13 23:03:31 INFO TaskSetManager: Finished task 2.0 in stage 36.0 (TID 60) in 572 ms on localhost (3/4)
17/01/13 23:03:31 INFO Executor: Finished task 1.0 in stage 36.0 (TID 59). 1660 bytes result sent to driver
17/01/13 23:03:31 INFO DAGScheduler: ShuffleMapStage 36 (count at null:-2) finished in 0.614 s
17/01/13 23:03:31 INFO DAGScheduler: looking for newly runnable stages
17/01/13 23:03:31 INFO DAGScheduler: running: Set()
17/01/13 23:03:31 INFO DAGScheduler: waiting: Set(ShuffleMapStage 37, ResultStage 38)
17/01/13 23:03:31 INFO DAGScheduler: failed: Set()
17/01/13 23:03:31 INFO DAGScheduler: Submitting ShuffleMapStage 37 (MapPartitionsRDD[154] at count at null:-2), which has no missing parents
17/01/13 23:03:31 INFO TaskSetManager: Finished task 1.0 in stage 36.0 (TID 59) in 613 ms on localhost (4/4)
17/01/13 23:03:31 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 17.9 KB, free 28.8 MB)
17/01/13 23:03:31 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool 
17/01/13 23:03:31 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 8.1 KB, free 28.8 MB)
17/01/13 23:03:31 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on localhost:63586 (size: 8.1 KB, free: 483.1 MB)
17/01/13 23:03:31 INFO SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:31 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 37 (MapPartitionsRDD[154] at count at null:-2)
17/01/13 23:03:31 INFO TaskSchedulerImpl: Adding task set 37.0 with 4 tasks
17/01/13 23:03:31 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 62, localhost, partition 0,NODE_LOCAL, 2231 bytes)
17/01/13 23:03:31 INFO TaskSetManager: Starting task 1.0 in stage 37.0 (TID 63, localhost, partition 1,NODE_LOCAL, 2231 bytes)
17/01/13 23:03:31 INFO TaskSetManager: Starting task 2.0 in stage 37.0 (TID 64, localhost, partition 2,NODE_LOCAL, 2231 bytes)
17/01/13 23:03:31 INFO TaskSetManager: Starting task 3.0 in stage 37.0 (TID 65, localhost, partition 3,NODE_LOCAL, 2231 bytes)
17/01/13 23:03:31 INFO Executor: Running task 0.0 in stage 37.0 (TID 62)
17/01/13 23:03:31 INFO Executor: Running task 1.0 in stage 37.0 (TID 63)
17/01/13 23:03:31 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/01/13 23:03:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:31 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/01/13 23:03:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:31 INFO Executor: Running task 2.0 in stage 37.0 (TID 64)
17/01/13 23:03:31 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/01/13 23:03:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:31 INFO Executor: Running task 3.0 in stage 37.0 (TID 65)
17/01/13 23:03:31 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/01/13 23:03:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:31 INFO BlockManagerInfo: Removed broadcast_42_piece0 on localhost:63586 in memory (size: 7.2 KB, free: 483.1 MB)
17/01/13 23:03:31 INFO ContextCleaner: Cleaned accumulator 141
17/01/13 23:03:31 INFO BlockManagerInfo: Removed broadcast_45_piece0 on localhost:63586 in memory (size: 5.2 KB, free: 483.1 MB)
17/01/13 23:03:31 INFO BlockManagerInfo: Removed broadcast_44_piece0 on localhost:63586 in memory (size: 11.7 KB, free: 483.1 MB)
17/01/13 23:03:31 INFO BlockManagerInfo: Removed broadcast_43_piece0 on localhost:63586 in memory (size: 10.4 KB, free: 483.1 MB)
17/01/13 23:03:31 INFO ContextCleaner: Cleaned accumulator 158
17/01/13 23:03:31 INFO Executor: Finished task 2.0 in stage 37.0 (TID 64). 1965 bytes result sent to driver
17/01/13 23:03:31 INFO TaskSetManager: Finished task 2.0 in stage 37.0 (TID 64) in 237 ms on localhost (1/4)
17/01/13 23:03:31 INFO Executor: Finished task 1.0 in stage 37.0 (TID 63). 1965 bytes result sent to driver
17/01/13 23:03:31 INFO TaskSetManager: Finished task 1.0 in stage 37.0 (TID 63) in 259 ms on localhost (2/4)
17/01/13 23:03:31 INFO Executor: Finished task 0.0 in stage 37.0 (TID 62). 1965 bytes result sent to driver
17/01/13 23:03:31 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 62) in 270 ms on localhost (3/4)
17/01/13 23:03:31 INFO Executor: Finished task 3.0 in stage 37.0 (TID 65). 1965 bytes result sent to driver
17/01/13 23:03:31 INFO TaskSetManager: Finished task 3.0 in stage 37.0 (TID 65) in 283 ms on localhost (4/4)
17/01/13 23:03:31 INFO DAGScheduler: ShuffleMapStage 37 (count at null:-2) finished in 0.285 s
17/01/13 23:03:31 INFO DAGScheduler: looking for newly runnable stages
17/01/13 23:03:31 INFO DAGScheduler: running: Set()
17/01/13 23:03:31 INFO DAGScheduler: waiting: Set(ResultStage 38)
17/01/13 23:03:31 INFO DAGScheduler: failed: Set()
17/01/13 23:03:31 INFO DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[157] at count at null:-2), which has no missing parents
17/01/13 23:03:31 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool 
17/01/13 23:03:31 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 15.8 KB, free 28.7 MB)
17/01/13 23:03:31 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 7.2 KB, free 28.7 MB)
17/01/13 23:03:31 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on localhost:63586 (size: 7.2 KB, free: 483.1 MB)
17/01/13 23:03:31 INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 38 (MapPartitionsRDD[157] at count at null:-2)
17/01/13 23:03:31 INFO TaskSchedulerImpl: Adding task set 38.0 with 1 tasks
17/01/13 23:03:31 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 66, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 23:03:31 INFO Executor: Running task 0.0 in stage 38.0 (TID 66)
17/01/13 23:03:31 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/01/13 23:03:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:31 INFO Executor: Finished task 0.0 in stage 38.0 (TID 66). 2174 bytes result sent to driver
17/01/13 23:03:31 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 66) in 10 ms on localhost (1/1)
17/01/13 23:03:31 INFO DAGScheduler: ResultStage 38 (count at null:-2) finished in 0.011 s
17/01/13 23:03:31 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool 
17/01/13 23:03:31 INFO DAGScheduler: Job 23 finished: count at null:-2, took 1.205060 s
17/01/13 23:03:31 INFO ParseDriver: Parsing command: SELECT *
FROM (SELECT `playerID`, `yearID`, `teamID`, `G`, `AB`, `R`, `H`
FROM (SELECT `playerID`, `yearID`, `teamID`, `G`, `AB`, `R`, `H`, rank() OVER (PARTITION BY `playerID` ORDER BY `H` DESC) AS `zzz6`
FROM (SELECT *
FROM (SELECT `playerID` AS `playerID`, `yearID` AS `yearID`, `teamID` AS `teamID`, `G` AS `G`, `AB` AS `AB`, `R` AS `R`, `H` AS `H`
FROM `batting`) `xokkyltbqm`
ORDER BY `playerID`, `yearID`, `teamID`) `tcuxppfkce`) `btfatnqtix`
WHERE (`zzz6` <= 2.0 AND `H` > 0.0)) `zhyhslddbe`
LIMIT 10
17/01/13 23:03:31 INFO ParseDriver: Parse Completed
17/01/13 23:03:32 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 23:03:32 INFO DAGScheduler: Got job 24 (collect at utils.scala:195) with 2 output partitions
17/01/13 23:03:32 INFO DAGScheduler: Final stage: ResultStage 39 (collect at utils.scala:195)
17/01/13 23:03:32 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:32 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:32 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[163] at collect at utils.scala:195), which has no missing parents
17/01/13 23:03:32 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 26.4 KB, free 28.7 MB)
17/01/13 23:03:32 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 10.4 KB, free 28.7 MB)
17/01/13 23:03:32 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on localhost:63586 (size: 10.4 KB, free: 483.1 MB)
17/01/13 23:03:32 INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:32 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 39 (MapPartitionsRDD[163] at collect at utils.scala:195)
17/01/13 23:03:32 INFO TaskSchedulerImpl: Adding task set 39.0 with 2 tasks
17/01/13 23:03:32 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 67, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 23:03:32 INFO TaskSetManager: Starting task 1.0 in stage 39.0 (TID 68, localhost, partition 1,PROCESS_LOCAL, 2481 bytes)
17/01/13 23:03:32 INFO Executor: Running task 1.0 in stage 39.0 (TID 68)
17/01/13 23:03:32 INFO Executor: Running task 0.0 in stage 39.0 (TID 67)
17/01/13 23:03:32 INFO BlockManager: Found block rdd_70_1 locally
17/01/13 23:03:32 INFO BlockManager: Found block rdd_70_0 locally
17/01/13 23:03:32 INFO Executor: Finished task 1.0 in stage 39.0 (TID 68). 13625 bytes result sent to driver
17/01/13 23:03:32 INFO Executor: Finished task 0.0 in stage 39.0 (TID 67). 13904 bytes result sent to driver
17/01/13 23:03:32 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 67) in 50 ms on localhost (1/2)
17/01/13 23:03:32 INFO TaskSetManager: Finished task 1.0 in stage 39.0 (TID 68) in 49 ms on localhost (2/2)
17/01/13 23:03:32 INFO DAGScheduler: ResultStage 39 (collect at utils.scala:195) finished in 0.050 s
17/01/13 23:03:32 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool 
17/01/13 23:03:32 INFO DAGScheduler: Job 24 finished: collect at utils.scala:195, took 0.056485 s
17/01/13 23:03:32 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 23:03:32 INFO DAGScheduler: Registering RDD 164 (collect at utils.scala:195)
17/01/13 23:03:32 INFO DAGScheduler: Registering RDD 168 (collect at utils.scala:195)
17/01/13 23:03:32 INFO DAGScheduler: Got job 25 (collect at utils.scala:195) with 1 output partitions
17/01/13 23:03:32 INFO DAGScheduler: Final stage: ResultStage 42 (collect at utils.scala:195)
17/01/13 23:03:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 41)
17/01/13 23:03:32 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 41)
17/01/13 23:03:32 INFO DAGScheduler: Submitting ShuffleMapStage 40 (MapPartitionsRDD[164] at collect at utils.scala:195), which has no missing parents
17/01/13 23:03:32 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 28.9 KB, free 28.8 MB)
17/01/13 23:03:32 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 11.7 KB, free 28.8 MB)
17/01/13 23:03:32 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on localhost:63586 (size: 11.7 KB, free: 483.1 MB)
17/01/13 23:03:32 INFO SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:32 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 40 (MapPartitionsRDD[164] at collect at utils.scala:195)
17/01/13 23:03:32 INFO TaskSchedulerImpl: Adding task set 40.0 with 2 tasks
17/01/13 23:03:32 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 69, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:32 INFO TaskSetManager: Starting task 1.0 in stage 40.0 (TID 70, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:32 INFO Executor: Running task 0.0 in stage 40.0 (TID 69)
17/01/13 23:03:32 INFO Executor: Running task 1.0 in stage 40.0 (TID 70)
17/01/13 23:03:32 INFO BlockManager: Found block rdd_70_0 locally
17/01/13 23:03:32 INFO BlockManager: Found block rdd_70_1 locally
17/01/13 23:03:32 INFO Executor: Finished task 0.0 in stage 40.0 (TID 69). 2580 bytes result sent to driver
17/01/13 23:03:32 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 69) in 157 ms on localhost (1/2)
17/01/13 23:03:32 INFO Executor: Finished task 1.0 in stage 40.0 (TID 70). 2580 bytes result sent to driver
17/01/13 23:03:32 INFO TaskSetManager: Finished task 1.0 in stage 40.0 (TID 70) in 203 ms on localhost (2/2)
17/01/13 23:03:32 INFO DAGScheduler: ShuffleMapStage 40 (collect at utils.scala:195) finished in 0.204 s
17/01/13 23:03:32 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool 
17/01/13 23:03:32 INFO DAGScheduler: looking for newly runnable stages
17/01/13 23:03:32 INFO DAGScheduler: running: Set()
17/01/13 23:03:32 INFO DAGScheduler: waiting: Set(ResultStage 42, ShuffleMapStage 41)
17/01/13 23:03:32 INFO DAGScheduler: failed: Set()
17/01/13 23:03:32 INFO DAGScheduler: Submitting ShuffleMapStage 41 (MapPartitionsRDD[168] at collect at utils.scala:195), which has no missing parents
17/01/13 23:03:32 INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 10.3 KB, free 28.8 MB)
17/01/13 23:03:32 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 5.2 KB, free 28.8 MB)
17/01/13 23:03:32 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on localhost:63586 (size: 5.2 KB, free: 483.1 MB)
17/01/13 23:03:32 INFO SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:32 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 41 (MapPartitionsRDD[168] at collect at utils.scala:195)
17/01/13 23:03:32 INFO TaskSchedulerImpl: Adding task set 41.0 with 4 tasks
17/01/13 23:03:32 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 71, localhost, partition 0,NODE_LOCAL, 2231 bytes)
17/01/13 23:03:32 INFO TaskSetManager: Starting task 1.0 in stage 41.0 (TID 72, localhost, partition 1,NODE_LOCAL, 2231 bytes)
17/01/13 23:03:32 INFO TaskSetManager: Starting task 2.0 in stage 41.0 (TID 73, localhost, partition 2,NODE_LOCAL, 2231 bytes)
17/01/13 23:03:32 INFO TaskSetManager: Starting task 3.0 in stage 41.0 (TID 74, localhost, partition 3,NODE_LOCAL, 2231 bytes)
17/01/13 23:03:32 INFO Executor: Running task 0.0 in stage 41.0 (TID 71)
17/01/13 23:03:32 INFO Executor: Running task 3.0 in stage 41.0 (TID 74)
17/01/13 23:03:32 INFO Executor: Running task 2.0 in stage 41.0 (TID 73)
17/01/13 23:03:32 INFO Executor: Running task 1.0 in stage 41.0 (TID 72)
17/01/13 23:03:32 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:32 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:32 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:32 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:32 INFO Executor: Finished task 1.0 in stage 41.0 (TID 72). 1660 bytes result sent to driver
17/01/13 23:03:32 INFO TaskSetManager: Finished task 1.0 in stage 41.0 (TID 72) in 222 ms on localhost (1/4)
17/01/13 23:03:32 INFO Executor: Finished task 2.0 in stage 41.0 (TID 73). 1660 bytes result sent to driver
17/01/13 23:03:32 INFO TaskSetManager: Finished task 2.0 in stage 41.0 (TID 73) in 244 ms on localhost (2/4)
17/01/13 23:03:32 INFO Executor: Finished task 0.0 in stage 41.0 (TID 71). 1660 bytes result sent to driver
17/01/13 23:03:32 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 71) in 249 ms on localhost (3/4)
17/01/13 23:03:32 INFO Executor: Finished task 3.0 in stage 41.0 (TID 74). 1660 bytes result sent to driver
17/01/13 23:03:32 INFO TaskSetManager: Finished task 3.0 in stage 41.0 (TID 74) in 250 ms on localhost (4/4)
17/01/13 23:03:32 INFO DAGScheduler: ShuffleMapStage 41 (collect at utils.scala:195) finished in 0.251 s
17/01/13 23:03:32 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool 
17/01/13 23:03:32 INFO DAGScheduler: looking for newly runnable stages
17/01/13 23:03:32 INFO DAGScheduler: running: Set()
17/01/13 23:03:32 INFO DAGScheduler: waiting: Set(ResultStage 42)
17/01/13 23:03:32 INFO DAGScheduler: failed: Set()
17/01/13 23:03:32 INFO DAGScheduler: Submitting ResultStage 42 (MapPartitionsRDD[175] at collect at utils.scala:195), which has no missing parents
17/01/13 23:03:32 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 15.6 KB, free 28.8 MB)
17/01/13 23:03:32 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 7.2 KB, free 28.8 MB)
17/01/13 23:03:32 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on localhost:63586 (size: 7.2 KB, free: 483.1 MB)
17/01/13 23:03:32 INFO SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (MapPartitionsRDD[175] at collect at utils.scala:195)
17/01/13 23:03:32 INFO TaskSchedulerImpl: Adding task set 42.0 with 1 tasks
17/01/13 23:03:32 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 75, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 23:03:32 INFO Executor: Running task 0.0 in stage 42.0 (TID 75)
17/01/13 23:03:32 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/01/13 23:03:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:32 INFO Executor: Finished task 0.0 in stage 42.0 (TID 75). 2884 bytes result sent to driver
17/01/13 23:03:32 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 75) in 33 ms on localhost (1/1)
17/01/13 23:03:32 INFO DAGScheduler: ResultStage 42 (collect at utils.scala:195) finished in 0.034 s
17/01/13 23:03:32 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool 
17/01/13 23:03:32 INFO DAGScheduler: Job 25 finished: collect at utils.scala:195, took 0.501392 s
17/01/13 23:03:32 INFO ParseDriver: Parsing command: SELECT * FROM iris WHERE Species = 'virginica' ORDER BY Sepal_Length LIMIT 10
17/01/13 23:03:32 INFO ParseDriver: Parse Completed
17/01/13 23:03:32 INFO InMemoryColumnarTableScan: Predicate (Species#11 = virginica) generates partition filter: ((Species.lowerBound#1831 <= virginica) && (virginica <= Species.upperBound#1830))
17/01/13 23:03:32 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 23:03:32 INFO DAGScheduler: Got job 26 (collect at utils.scala:195) with 2 output partitions
17/01/13 23:03:32 INFO DAGScheduler: Final stage: ResultStage 43 (collect at utils.scala:195)
17/01/13 23:03:32 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:32 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:32 INFO DAGScheduler: Submitting ResultStage 43 (MapPartitionsRDD[180] at collect at utils.scala:195), which has no missing parents
17/01/13 23:03:32 INFO MemoryStore: Block broadcast_52 stored as values in memory (estimated size 16.8 KB, free 28.8 MB)
17/01/13 23:03:32 INFO MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 8.1 KB, free 28.8 MB)
17/01/13 23:03:32 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on localhost:63586 (size: 8.1 KB, free: 483.1 MB)
17/01/13 23:03:32 INFO SparkContext: Created broadcast 52 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:32 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 43 (MapPartitionsRDD[180] at collect at utils.scala:195)
17/01/13 23:03:32 INFO TaskSchedulerImpl: Adding task set 43.0 with 2 tasks
17/01/13 23:03:32 INFO TaskSetManager: Starting task 0.0 in stage 43.0 (TID 76, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 23:03:32 INFO TaskSetManager: Starting task 1.0 in stage 43.0 (TID 77, localhost, partition 1,PROCESS_LOCAL, 2481 bytes)
17/01/13 23:03:32 INFO Executor: Running task 0.0 in stage 43.0 (TID 76)
17/01/13 23:03:32 INFO Executor: Running task 1.0 in stage 43.0 (TID 77)
17/01/13 23:03:32 INFO BlockManager: Found block rdd_14_0 locally
17/01/13 23:03:32 INFO BlockManager: Found block rdd_14_1 locally
17/01/13 23:03:32 INFO GeneratePredicate: Code generated in 6.209701 ms
17/01/13 23:03:32 INFO BlockManagerInfo: Removed broadcast_51_piece0 on localhost:63586 in memory (size: 7.2 KB, free: 483.1 MB)
17/01/13 23:03:32 INFO GenerateColumnAccessor: Code generated in 16.2274 ms
17/01/13 23:03:32 INFO BlockManagerInfo: Removed broadcast_47_piece0 on localhost:63586 in memory (size: 7.2 KB, free: 483.1 MB)
17/01/13 23:03:32 INFO ContextCleaner: Cleaned accumulator 162
17/01/13 23:03:32 INFO ContextCleaner: Cleaned accumulator 174
17/01/13 23:03:32 INFO GeneratePredicate: Code generated in 5.904208 ms
17/01/13 23:03:32 INFO BlockManagerInfo: Removed broadcast_50_piece0 on localhost:63586 in memory (size: 5.2 KB, free: 483.1 MB)
17/01/13 23:03:32 INFO ContextCleaner: Cleaned accumulator 173
17/01/13 23:03:32 INFO GenerateSafeProjection: Code generated in 8.907939 ms
17/01/13 23:03:32 INFO BlockManagerInfo: Removed broadcast_49_piece0 on localhost:63586 in memory (size: 11.7 KB, free: 483.1 MB)
17/01/13 23:03:32 INFO ContextCleaner: Cleaned accumulator 172
17/01/13 23:03:32 INFO ContextCleaner: Cleaned shuffle 16
17/01/13 23:03:32 INFO ContextCleaner: Cleaned shuffle 15
17/01/13 23:03:32 INFO BlockManagerInfo: Removed broadcast_48_piece0 on localhost:63586 in memory (size: 10.4 KB, free: 483.1 MB)
17/01/13 23:03:32 INFO ContextCleaner: Cleaned accumulator 171
17/01/13 23:03:32 INFO InMemoryColumnarTableScan: Skipping partition based on stats Sepal_Length.lowerBound: 4.3, Sepal_Length.upperBound: 7.0, Sepal_Length.nullCount: 0, Sepal_Length.count: 76, Sepal_Length.sizeInBytes: 608, Sepal_Width.lowerBound: 2.0, Sepal_Width.upperBound: 4.4, Sepal_Width.nullCount: 0, Sepal_Width.count: 76, Sepal_Width.sizeInBytes: 608, Petal_Length.lowerBound: 1.0, Petal_Length.upperBound: 4.9, Petal_Length.nullCount: 0, Petal_Length.count: 76, Petal_Length.sizeInBytes: 608, Petal_Width.lowerBound: 0.1, Petal_Width.upperBound: 1.8, Petal_Width.nullCount: 0, Petal_Width.count: 76, Petal_Width.sizeInBytes: 608, Species.lowerBound: setosa, Species.upperBound: versicolor, Species.nullCount: 0, Species.count: 76, Species.sizeInBytes: 864
17/01/13 23:03:32 INFO Executor: Finished task 1.0 in stage 43.0 (TID 77). 4736 bytes result sent to driver
17/01/13 23:03:32 INFO Executor: Finished task 0.0 in stage 43.0 (TID 76). 3530 bytes result sent to driver
17/01/13 23:03:32 INFO TaskSetManager: Finished task 0.0 in stage 43.0 (TID 76) in 113 ms on localhost (1/2)
17/01/13 23:03:32 INFO TaskSetManager: Finished task 1.0 in stage 43.0 (TID 77) in 114 ms on localhost (2/2)
17/01/13 23:03:32 INFO DAGScheduler: ResultStage 43 (collect at utils.scala:195) finished in 0.114 s
17/01/13 23:03:32 INFO TaskSchedulerImpl: Removed TaskSet 43.0, whose tasks have all completed, from pool 
17/01/13 23:03:32 INFO DAGScheduler: Job 26 finished: collect at utils.scala:195, took 0.137673 s
17/01/13 23:03:32 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/01/13 23:03:32 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/01/13 23:03:33 INFO SparkContext: Starting job: collect at utils.scala:59
17/01/13 23:03:33 INFO DAGScheduler: Got job 27 (collect at utils.scala:59) with 1 output partitions
17/01/13 23:03:33 INFO DAGScheduler: Final stage: ResultStage 44 (collect at utils.scala:59)
17/01/13 23:03:33 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:33 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:33 INFO DAGScheduler: Submitting ResultStage 44 (MapPartitionsRDD[184] at map at utils.scala:56), which has no missing parents
17/01/13 23:03:33 INFO MemoryStore: Block broadcast_53 stored as values in memory (estimated size 5.4 KB, free 28.7 MB)
17/01/13 23:03:33 INFO MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 3.0 KB, free 28.7 MB)
17/01/13 23:03:33 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on localhost:63586 (size: 3.0 KB, free: 483.1 MB)
17/01/13 23:03:33 INFO SparkContext: Created broadcast 53 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 44 (MapPartitionsRDD[184] at map at utils.scala:56)
17/01/13 23:03:33 INFO TaskSchedulerImpl: Adding task set 44.0 with 1 tasks
17/01/13 23:03:33 INFO TaskSetManager: Starting task 0.0 in stage 44.0 (TID 78, localhost, partition 0,PROCESS_LOCAL, 2728 bytes)
17/01/13 23:03:33 INFO Executor: Running task 0.0 in stage 44.0 (TID 78)
17/01/13 23:03:33 INFO Executor: Finished task 0.0 in stage 44.0 (TID 78). 1087 bytes result sent to driver
17/01/13 23:03:33 INFO DAGScheduler: ResultStage 44 (collect at utils.scala:59) finished in 0.004 s
17/01/13 23:03:33 INFO DAGScheduler: Job 27 finished: collect at utils.scala:59, took 0.012386 s
17/01/13 23:03:33 INFO TaskSetManager: Finished task 0.0 in stage 44.0 (TID 78) in 4 ms on localhost (1/1)
17/01/13 23:03:33 INFO TaskSchedulerImpl: Removed TaskSet 44.0, whose tasks have all completed, from pool 
17/01/13 23:03:33 INFO MemoryStore: Block broadcast_54 stored as values in memory (estimated size 208.5 KB, free 28.9 MB)
17/01/13 23:03:33 INFO MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 19.3 KB, free 28.9 MB)
17/01/13 23:03:33 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on localhost:63586 (size: 19.3 KB, free: 483.1 MB)
17/01/13 23:03:33 INFO SparkContext: Created broadcast 54 from textFile at TextFile.scala:30
17/01/13 23:03:33 INFO FileInputFormat: Total input paths to process : 1
17/01/13 23:03:33 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/01/13 23:03:33 INFO DAGScheduler: Got job 28 (take at CsvRelation.scala:249) with 1 output partitions
17/01/13 23:03:33 INFO DAGScheduler: Final stage: ResultStage 45 (take at CsvRelation.scala:249)
17/01/13 23:03:33 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:33 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:33 INFO DAGScheduler: Submitting ResultStage 45 (C:\Users\Baoco\AppData\Local\Temp\RtmpOgwhNO/spark_serialize_600df8759318fe0d3e7cd227dfb889264b9395236a2336b9c1d32f83ed509279.csv MapPartitionsRDD[186] at textFile at TextFile.scala:30), which has no missing parents
17/01/13 23:03:33 INFO MemoryStore: Block broadcast_55 stored as values in memory (estimated size 3.2 KB, free 28.9 MB)
17/01/13 23:03:33 INFO MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 1950.0 B, free 28.9 MB)
17/01/13 23:03:33 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on localhost:63586 (size: 1950.0 B, free: 483.1 MB)
17/01/13 23:03:33 INFO SparkContext: Created broadcast 55 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 45 (C:\Users\Baoco\AppData\Local\Temp\RtmpOgwhNO/spark_serialize_600df8759318fe0d3e7cd227dfb889264b9395236a2336b9c1d32f83ed509279.csv MapPartitionsRDD[186] at textFile at TextFile.scala:30)
17/01/13 23:03:33 INFO TaskSchedulerImpl: Adding task set 45.0 with 1 tasks
17/01/13 23:03:33 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 79, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 23:03:33 INFO Executor: Running task 0.0 in stage 45.0 (TID 79)
17/01/13 23:03:33 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/spark_serialize_600df8759318fe0d3e7cd227dfb889264b9395236a2336b9c1d32f83ed509279.csv:0+668
17/01/13 23:03:33 INFO Executor: Finished task 0.0 in stage 45.0 (TID 79). 2478 bytes result sent to driver
17/01/13 23:03:33 INFO DAGScheduler: ResultStage 45 (take at CsvRelation.scala:249) finished in 0.009 s
17/01/13 23:03:33 INFO DAGScheduler: Job 28 finished: take at CsvRelation.scala:249, took 0.012980 s
17/01/13 23:03:33 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 79) in 9 ms on localhost (1/1)
17/01/13 23:03:33 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool 
17/01/13 23:03:33 INFO ParseDriver: Parsing command: SELECT * FROM  `mtcars`
17/01/13 23:03:33 INFO ParseDriver: Parse Completed
17/01/13 23:03:33 INFO MemoryStore: Block broadcast_56 stored as values in memory (estimated size 208.5 KB, free 29.1 MB)
17/01/13 23:03:33 INFO MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 19.3 KB, free 29.2 MB)
17/01/13 23:03:33 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on localhost:63586 (size: 19.3 KB, free: 483.1 MB)
17/01/13 23:03:33 INFO SparkContext: Created broadcast 56 from textFile at TextFile.scala:30
17/01/13 23:03:33 INFO FileInputFormat: Total input paths to process : 1
17/01/13 23:03:33 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/01/13 23:03:33 INFO DAGScheduler: Got job 29 (take at CsvRelation.scala:249) with 1 output partitions
17/01/13 23:03:33 INFO DAGScheduler: Final stage: ResultStage 46 (take at CsvRelation.scala:249)
17/01/13 23:03:33 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:33 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:33 INFO DAGScheduler: Submitting ResultStage 46 (C:\Users\Baoco\AppData\Local\Temp\RtmpOgwhNO/spark_serialize_600df8759318fe0d3e7cd227dfb889264b9395236a2336b9c1d32f83ed509279.csv MapPartitionsRDD[188] at textFile at TextFile.scala:30), which has no missing parents
17/01/13 23:03:33 INFO MemoryStore: Block broadcast_57 stored as values in memory (estimated size 3.2 KB, free 29.2 MB)
17/01/13 23:03:33 INFO MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 1956.0 B, free 29.2 MB)
17/01/13 23:03:33 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on localhost:63586 (size: 1956.0 B, free: 483.1 MB)
17/01/13 23:03:33 INFO SparkContext: Created broadcast 57 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 46 (C:\Users\Baoco\AppData\Local\Temp\RtmpOgwhNO/spark_serialize_600df8759318fe0d3e7cd227dfb889264b9395236a2336b9c1d32f83ed509279.csv MapPartitionsRDD[188] at textFile at TextFile.scala:30)
17/01/13 23:03:33 INFO TaskSchedulerImpl: Adding task set 46.0 with 1 tasks
17/01/13 23:03:33 INFO TaskSetManager: Starting task 0.0 in stage 46.0 (TID 80, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 23:03:33 INFO Executor: Running task 0.0 in stage 46.0 (TID 80)
17/01/13 23:03:33 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/spark_serialize_600df8759318fe0d3e7cd227dfb889264b9395236a2336b9c1d32f83ed509279.csv:0+668
17/01/13 23:03:33 INFO Executor: Finished task 0.0 in stage 46.0 (TID 80). 2478 bytes result sent to driver
17/01/13 23:03:33 INFO DAGScheduler: ResultStage 46 (take at CsvRelation.scala:249) finished in 0.013 s
17/01/13 23:03:33 INFO DAGScheduler: Job 29 finished: take at CsvRelation.scala:249, took 0.016353 s
17/01/13 23:03:33 INFO MemoryStore: Block broadcast_58 stored as values in memory (estimated size 208.5 KB, free 29.4 MB)
17/01/13 23:03:33 INFO TaskSetManager: Finished task 0.0 in stage 46.0 (TID 80) in 12 ms on localhost (1/1)
17/01/13 23:03:33 INFO TaskSchedulerImpl: Removed TaskSet 46.0, whose tasks have all completed, from pool 
17/01/13 23:03:33 INFO MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 19.3 KB, free 29.4 MB)
17/01/13 23:03:33 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on localhost:63586 (size: 19.3 KB, free: 483.1 MB)
17/01/13 23:03:33 INFO SparkContext: Created broadcast 58 from textFile at TextFile.scala:30
17/01/13 23:03:33 INFO FileInputFormat: Total input paths to process : 1
17/01/13 23:03:33 INFO SparkContext: Starting job: sql at null:-1
17/01/13 23:03:33 INFO DAGScheduler: Registering RDD 198 (sql at null:-1)
17/01/13 23:03:33 INFO DAGScheduler: Got job 30 (sql at null:-1) with 1 output partitions
17/01/13 23:03:33 INFO DAGScheduler: Final stage: ResultStage 48 (sql at null:-1)
17/01/13 23:03:33 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 47)
17/01/13 23:03:33 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 47)
17/01/13 23:03:33 INFO DAGScheduler: Submitting ShuffleMapStage 47 (MapPartitionsRDD[198] at sql at null:-1), which has no missing parents
17/01/13 23:03:33 INFO MemoryStore: Block broadcast_59 stored as values in memory (estimated size 21.5 KB, free 29.4 MB)
17/01/13 23:03:33 INFO MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 9.6 KB, free 29.4 MB)
17/01/13 23:03:33 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on localhost:63586 (size: 9.6 KB, free: 483.1 MB)
17/01/13 23:03:33 INFO SparkContext: Created broadcast 59 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:33 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 47 (MapPartitionsRDD[198] at sql at null:-1)
17/01/13 23:03:33 INFO TaskSchedulerImpl: Adding task set 47.0 with 2 tasks
17/01/13 23:03:33 INFO TaskSetManager: Starting task 0.0 in stage 47.0 (TID 81, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:33 INFO TaskSetManager: Starting task 1.0 in stage 47.0 (TID 82, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:33 INFO Executor: Running task 0.0 in stage 47.0 (TID 81)
17/01/13 23:03:33 INFO Executor: Running task 1.0 in stage 47.0 (TID 82)
17/01/13 23:03:33 INFO CacheManager: Partition rdd_195_0 not found, computing it
17/01/13 23:03:33 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/spark_serialize_600df8759318fe0d3e7cd227dfb889264b9395236a2336b9c1d32f83ed509279.csv:0+668
17/01/13 23:03:33 INFO CacheManager: Partition rdd_195_1 not found, computing it
17/01/13 23:03:33 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/spark_serialize_600df8759318fe0d3e7cd227dfb889264b9395236a2336b9c1d32f83ed509279.csv:668+668
17/01/13 23:03:33 INFO GenerateUnsafeProjection: Code generated in 17.272305 ms
17/01/13 23:03:33 INFO MemoryStore: Block rdd_195_1 stored as values in memory (estimated size 2.8 KB, free 29.4 MB)
17/01/13 23:03:33 INFO MemoryStore: Block rdd_195_0 stored as values in memory (estimated size 2.8 KB, free 29.4 MB)
17/01/13 23:03:33 INFO BlockManagerInfo: Added rdd_195_1 in memory on localhost:63586 (size: 2.8 KB, free: 483.1 MB)
17/01/13 23:03:33 INFO BlockManagerInfo: Added rdd_195_0 in memory on localhost:63586 (size: 2.8 KB, free: 483.1 MB)
17/01/13 23:03:33 INFO Executor: Finished task 0.0 in stage 47.0 (TID 81). 4031 bytes result sent to driver
17/01/13 23:03:33 INFO Executor: Finished task 1.0 in stage 47.0 (TID 82). 4031 bytes result sent to driver
17/01/13 23:03:33 INFO TaskSetManager: Finished task 0.0 in stage 47.0 (TID 81) in 64 ms on localhost (1/2)
17/01/13 23:03:33 INFO DAGScheduler: ShuffleMapStage 47 (sql at null:-1) finished in 0.066 s
17/01/13 23:03:33 INFO DAGScheduler: looking for newly runnable stages
17/01/13 23:03:33 INFO DAGScheduler: running: Set()
17/01/13 23:03:33 INFO DAGScheduler: waiting: Set(ResultStage 48)
17/01/13 23:03:33 INFO DAGScheduler: failed: Set()
17/01/13 23:03:33 INFO DAGScheduler: Submitting ResultStage 48 (MapPartitionsRDD[201] at sql at null:-1), which has no missing parents
17/01/13 23:03:33 INFO MemoryStore: Block broadcast_60 stored as values in memory (estimated size 9.3 KB, free 29.4 MB)
17/01/13 23:03:33 INFO MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 4.6 KB, free 29.4 MB)
17/01/13 23:03:33 INFO TaskSetManager: Finished task 1.0 in stage 47.0 (TID 82) in 64 ms on localhost (2/2)
17/01/13 23:03:33 INFO TaskSchedulerImpl: Removed TaskSet 47.0, whose tasks have all completed, from pool 
17/01/13 23:03:33 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on localhost:63586 (size: 4.6 KB, free: 483.1 MB)
17/01/13 23:03:33 INFO SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 48 (MapPartitionsRDD[201] at sql at null:-1)
17/01/13 23:03:33 INFO TaskSchedulerImpl: Adding task set 48.0 with 1 tasks
17/01/13 23:03:33 INFO TaskSetManager: Starting task 0.0 in stage 48.0 (TID 83, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 23:03:33 INFO Executor: Running task 0.0 in stage 48.0 (TID 83)
17/01/13 23:03:33 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:33 INFO Executor: Finished task 0.0 in stage 48.0 (TID 83). 1830 bytes result sent to driver
17/01/13 23:03:33 INFO DAGScheduler: ResultStage 48 (sql at null:-1) finished in 0.013 s
17/01/13 23:03:33 INFO DAGScheduler: Job 30 finished: sql at null:-1, took 0.092754 s
17/01/13 23:03:33 INFO TaskSetManager: Finished task 0.0 in stage 48.0 (TID 83) in 13 ms on localhost (1/1)
17/01/13 23:03:33 INFO TaskSchedulerImpl: Removed TaskSet 48.0, whose tasks have all completed, from pool 
17/01/13 23:03:33 INFO ParseDriver: Parsing command: SELECT count(*) FROM  `mtcars`
17/01/13 23:03:33 INFO ParseDriver: Parse Completed
17/01/13 23:03:33 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 23:03:33 INFO DAGScheduler: Registering RDD 205 (collect at utils.scala:195)
17/01/13 23:03:33 INFO DAGScheduler: Got job 31 (collect at utils.scala:195) with 1 output partitions
17/01/13 23:03:33 INFO DAGScheduler: Final stage: ResultStage 50 (collect at utils.scala:195)
17/01/13 23:03:33 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 49)
17/01/13 23:03:33 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 49)
17/01/13 23:03:33 INFO DAGScheduler: Submitting ShuffleMapStage 49 (MapPartitionsRDD[205] at collect at utils.scala:195), which has no missing parents
17/01/13 23:03:33 INFO MemoryStore: Block broadcast_61 stored as values in memory (estimated size 21.5 KB, free 29.5 MB)
17/01/13 23:03:33 INFO MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 9.6 KB, free 29.5 MB)
17/01/13 23:03:33 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on localhost:63586 (size: 9.6 KB, free: 483.0 MB)
17/01/13 23:03:33 INFO SparkContext: Created broadcast 61 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:33 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 49 (MapPartitionsRDD[205] at collect at utils.scala:195)
17/01/13 23:03:33 INFO TaskSchedulerImpl: Adding task set 49.0 with 2 tasks
17/01/13 23:03:33 INFO TaskSetManager: Starting task 0.0 in stage 49.0 (TID 84, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:33 INFO TaskSetManager: Starting task 1.0 in stage 49.0 (TID 85, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:33 INFO Executor: Running task 0.0 in stage 49.0 (TID 84)
17/01/13 23:03:33 INFO BlockManager: Found block rdd_195_0 locally
17/01/13 23:03:33 INFO Executor: Running task 1.0 in stage 49.0 (TID 85)
17/01/13 23:03:33 INFO BlockManager: Found block rdd_195_1 locally
17/01/13 23:03:33 INFO Executor: Finished task 0.0 in stage 49.0 (TID 84). 2679 bytes result sent to driver
17/01/13 23:03:33 INFO TaskSetManager: Finished task 0.0 in stage 49.0 (TID 84) in 26 ms on localhost (1/2)
17/01/13 23:03:33 INFO Executor: Finished task 1.0 in stage 49.0 (TID 85). 2679 bytes result sent to driver
17/01/13 23:03:33 INFO DAGScheduler: ShuffleMapStage 49 (collect at utils.scala:195) finished in 0.029 s
17/01/13 23:03:33 INFO DAGScheduler: looking for newly runnable stages
17/01/13 23:03:33 INFO DAGScheduler: running: Set()
17/01/13 23:03:33 INFO DAGScheduler: waiting: Set(ResultStage 50)
17/01/13 23:03:33 INFO DAGScheduler: failed: Set()
17/01/13 23:03:33 INFO DAGScheduler: Submitting ResultStage 50 (MapPartitionsRDD[208] at collect at utils.scala:195), which has no missing parents
17/01/13 23:03:33 INFO MemoryStore: Block broadcast_62 stored as values in memory (estimated size 9.4 KB, free 29.5 MB)
17/01/13 23:03:33 INFO TaskSetManager: Finished task 1.0 in stage 49.0 (TID 85) in 27 ms on localhost (2/2)
17/01/13 23:03:33 INFO TaskSchedulerImpl: Removed TaskSet 49.0, whose tasks have all completed, from pool 
17/01/13 23:03:33 INFO MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 4.7 KB, free 29.5 MB)
17/01/13 23:03:33 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on localhost:63586 (size: 4.7 KB, free: 483.0 MB)
17/01/13 23:03:33 INFO SparkContext: Created broadcast 62 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 50 (MapPartitionsRDD[208] at collect at utils.scala:195)
17/01/13 23:03:33 INFO TaskSchedulerImpl: Adding task set 50.0 with 1 tasks
17/01/13 23:03:33 INFO TaskSetManager: Starting task 0.0 in stage 50.0 (TID 86, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 23:03:33 INFO Executor: Running task 0.0 in stage 50.0 (TID 86)
17/01/13 23:03:33 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/01/13 23:03:33 INFO Executor: Finished task 0.0 in stage 50.0 (TID 86). 1830 bytes result sent to driver
17/01/13 23:03:33 INFO DAGScheduler: ResultStage 50 (collect at utils.scala:195) finished in 0.013 s
17/01/13 23:03:33 INFO DAGScheduler: Job 31 finished: collect at utils.scala:195, took 0.050606 s
17/01/13 23:03:33 INFO TaskSetManager: Finished task 0.0 in stage 50.0 (TID 86) in 13 ms on localhost (1/1)
17/01/13 23:03:33 INFO TaskSchedulerImpl: Removed TaskSet 50.0, whose tasks have all completed, from pool 
17/01/13 23:03:33 INFO ParseDriver: Parsing command: SELECT *
FROM `mtcars` AS `zzz7`
WHERE (0 = 1)
17/01/13 23:03:33 INFO ParseDriver: Parse Completed
17/01/13 23:03:33 INFO ParseDriver: Parsing command: SELECT `mpg`, `cyl`, `disp`, `hp`, `drat`, `wt`, `qsec`, `vs`, `am`, `gear`, `carb`, `cyl` = 8.0 AS `cyl8`
FROM (SELECT *
FROM `mtcars`
WHERE (`hp` >= 100.0)) `zysvzntujb`
17/01/13 23:03:33 INFO ParseDriver: Parse Completed
17/01/13 23:03:33 INFO ParseDriver: Parsing command: SELECT *
FROM `sparklyr_tmp_22ac40bf7173` AS `zzz8`
WHERE (0 = 1)
17/01/13 23:03:33 INFO ParseDriver: Parse Completed
17/01/13 23:03:33 INFO ParseDriver: Parsing command: SELECT *
FROM `sparklyr_tmp_22ac5f706310` AS `zzz9`
WHERE (0 = 1)
17/01/13 23:03:33 INFO ParseDriver: Parse Completed
17/01/13 23:03:33 INFO ParseDriver: Parsing command: SELECT *
FROM `sparklyr_tmp_22ac40bf7173`
17/01/13 23:03:33 INFO ParseDriver: Parse Completed
17/01/13 23:03:33 INFO InMemoryColumnarTableScan: Predicate (hp#1851 >= 100.0) generates partition filter: (100.0 <= hp.upperBound#2214)
17/01/13 23:03:33 INFO SparkContext: Starting job: count at null:-2
17/01/13 23:03:33 INFO DAGScheduler: Registering RDD 216 (count at null:-2)
17/01/13 23:03:33 INFO DAGScheduler: Got job 32 (count at null:-2) with 1 output partitions
17/01/13 23:03:33 INFO DAGScheduler: Final stage: ResultStage 52 (count at null:-2)
17/01/13 23:03:33 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 51)
17/01/13 23:03:33 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 51)
17/01/13 23:03:33 INFO DAGScheduler: Submitting ShuffleMapStage 51 (MapPartitionsRDD[216] at count at null:-2), which has no missing parents
17/01/13 23:03:34 INFO MemoryStore: Block broadcast_63 stored as values in memory (estimated size 26.9 KB, free 29.5 MB)
17/01/13 23:03:34 INFO MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 11.5 KB, free 29.5 MB)
17/01/13 23:03:34 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on localhost:63586 (size: 11.5 KB, free: 483.0 MB)
17/01/13 23:03:34 INFO SparkContext: Created broadcast 63 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:34 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 51 (MapPartitionsRDD[216] at count at null:-2)
17/01/13 23:03:34 INFO TaskSchedulerImpl: Adding task set 51.0 with 2 tasks
17/01/13 23:03:34 INFO TaskSetManager: Starting task 0.0 in stage 51.0 (TID 87, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:34 INFO TaskSetManager: Starting task 1.0 in stage 51.0 (TID 88, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:34 INFO Executor: Running task 0.0 in stage 51.0 (TID 87)
17/01/13 23:03:34 INFO Executor: Running task 1.0 in stage 51.0 (TID 88)
17/01/13 23:03:34 INFO BlockManager: Found block rdd_195_0 locally
17/01/13 23:03:34 INFO BlockManager: Found block rdd_195_1 locally
17/01/13 23:03:34 INFO GeneratePredicate: Code generated in 5.794555 ms
17/01/13 23:03:34 INFO GenerateColumnAccessor: Code generated in 9.991245 ms
17/01/13 23:03:34 INFO GeneratePredicate: Code generated in 6.792101 ms
17/01/13 23:03:34 INFO GenerateUnsafeProjection: Code generated in 8.031567 ms
17/01/13 23:03:34 INFO GenerateOrdering: Code generated in 11.20255 ms
17/01/13 23:03:34 INFO GenerateUnsafeProjection: Code generated in 6.537381 ms
17/01/13 23:03:34 INFO Executor: Finished task 0.0 in stage 51.0 (TID 87). 2937 bytes result sent to driver
17/01/13 23:03:34 INFO TaskSetManager: Finished task 0.0 in stage 51.0 (TID 87) in 105 ms on localhost (1/2)
17/01/13 23:03:34 INFO Executor: Finished task 1.0 in stage 51.0 (TID 88). 2937 bytes result sent to driver
17/01/13 23:03:34 INFO DAGScheduler: ShuffleMapStage 51 (count at null:-2) finished in 0.113 s
17/01/13 23:03:34 INFO TaskSetManager: Finished task 1.0 in stage 51.0 (TID 88) in 112 ms on localhost (2/2)
17/01/13 23:03:34 INFO DAGScheduler: looking for newly runnable stages
17/01/13 23:03:34 INFO DAGScheduler: running: Set()
17/01/13 23:03:34 INFO DAGScheduler: waiting: Set(ResultStage 52)
17/01/13 23:03:34 INFO DAGScheduler: failed: Set()
17/01/13 23:03:34 INFO TaskSchedulerImpl: Removed TaskSet 51.0, whose tasks have all completed, from pool 
17/01/13 23:03:34 INFO DAGScheduler: Submitting ResultStage 52 (MapPartitionsRDD[219] at count at null:-2), which has no missing parents
17/01/13 23:03:34 INFO MemoryStore: Block broadcast_64 stored as values in memory (estimated size 13.2 KB, free 29.5 MB)
17/01/13 23:03:34 INFO MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 6.1 KB, free 29.5 MB)
17/01/13 23:03:34 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on localhost:63586 (size: 6.1 KB, free: 483.0 MB)
17/01/13 23:03:34 INFO SparkContext: Created broadcast 64 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 52 (MapPartitionsRDD[219] at count at null:-2)
17/01/13 23:03:34 INFO TaskSchedulerImpl: Adding task set 52.0 with 1 tasks
17/01/13 23:03:34 INFO TaskSetManager: Starting task 0.0 in stage 52.0 (TID 89, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 23:03:34 INFO Executor: Running task 0.0 in stage 52.0 (TID 89)
17/01/13 23:03:34 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:34 INFO Executor: Finished task 0.0 in stage 52.0 (TID 89). 2088 bytes result sent to driver
17/01/13 23:03:34 INFO DAGScheduler: ResultStage 52 (count at null:-2) finished in 0.008 s
17/01/13 23:03:34 INFO DAGScheduler: Job 32 finished: count at null:-2, took 0.132559 s
17/01/13 23:03:34 INFO TaskSetManager: Finished task 0.0 in stage 52.0 (TID 89) in 8 ms on localhost (1/1)
17/01/13 23:03:34 INFO TaskSchedulerImpl: Removed TaskSet 52.0, whose tasks have all completed, from pool 
17/01/13 23:03:34 INFO InMemoryColumnarTableScan: Predicate (hp#1851 >= 100.0) generates partition filter: (100.0 <= hp.upperBound#2273)
17/01/13 23:03:34 INFO SparkContext: Starting job: count at null:-2
17/01/13 23:03:34 INFO DAGScheduler: Registering RDD 228 (count at null:-2)
17/01/13 23:03:34 INFO DAGScheduler: Got job 33 (count at null:-2) with 1 output partitions
17/01/13 23:03:34 INFO DAGScheduler: Final stage: ResultStage 54 (count at null:-2)
17/01/13 23:03:34 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 53)
17/01/13 23:03:34 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 53)
17/01/13 23:03:34 INFO DAGScheduler: Submitting ShuffleMapStage 53 (MapPartitionsRDD[228] at count at null:-2), which has no missing parents
17/01/13 23:03:34 INFO MemoryStore: Block broadcast_65 stored as values in memory (estimated size 27.4 KB, free 29.6 MB)
17/01/13 23:03:34 INFO MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 11.7 KB, free 29.6 MB)
17/01/13 23:03:34 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on localhost:63586 (size: 11.7 KB, free: 483.0 MB)
17/01/13 23:03:34 INFO SparkContext: Created broadcast 65 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:34 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 53 (MapPartitionsRDD[228] at count at null:-2)
17/01/13 23:03:34 INFO TaskSchedulerImpl: Adding task set 53.0 with 2 tasks
17/01/13 23:03:34 INFO TaskSetManager: Starting task 0.0 in stage 53.0 (TID 90, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:34 INFO TaskSetManager: Starting task 1.0 in stage 53.0 (TID 91, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:34 INFO Executor: Running task 1.0 in stage 53.0 (TID 91)
17/01/13 23:03:34 INFO Executor: Running task 0.0 in stage 53.0 (TID 90)
17/01/13 23:03:34 INFO BlockManager: Found block rdd_195_1 locally
17/01/13 23:03:34 INFO BlockManager: Found block rdd_195_0 locally
17/01/13 23:03:34 INFO GeneratePredicate: Code generated in 4.133543 ms
17/01/13 23:03:34 INFO Executor: Finished task 1.0 in stage 53.0 (TID 91). 3023 bytes result sent to driver
17/01/13 23:03:34 INFO TaskSetManager: Finished task 1.0 in stage 53.0 (TID 91) in 29 ms on localhost (1/2)
17/01/13 23:03:34 INFO Executor: Finished task 0.0 in stage 53.0 (TID 90). 3023 bytes result sent to driver
17/01/13 23:03:34 INFO DAGScheduler: ShuffleMapStage 53 (count at null:-2) finished in 0.032 s
17/01/13 23:03:34 INFO TaskSetManager: Finished task 0.0 in stage 53.0 (TID 90) in 32 ms on localhost (2/2)
17/01/13 23:03:34 INFO DAGScheduler: looking for newly runnable stages
17/01/13 23:03:34 INFO DAGScheduler: running: Set()
17/01/13 23:03:34 INFO DAGScheduler: waiting: Set(ResultStage 54)
17/01/13 23:03:34 INFO DAGScheduler: failed: Set()
17/01/13 23:03:34 INFO TaskSchedulerImpl: Removed TaskSet 53.0, whose tasks have all completed, from pool 
17/01/13 23:03:34 INFO DAGScheduler: Submitting ResultStage 54 (MapPartitionsRDD[231] at count at null:-2), which has no missing parents
17/01/13 23:03:34 INFO MemoryStore: Block broadcast_66 stored as values in memory (estimated size 13.6 KB, free 29.6 MB)
17/01/13 23:03:34 INFO MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 6.3 KB, free 29.6 MB)
17/01/13 23:03:34 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on localhost:63586 (size: 6.3 KB, free: 483.0 MB)
17/01/13 23:03:34 INFO SparkContext: Created broadcast 66 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 54 (MapPartitionsRDD[231] at count at null:-2)
17/01/13 23:03:34 INFO TaskSchedulerImpl: Adding task set 54.0 with 1 tasks
17/01/13 23:03:34 INFO TaskSetManager: Starting task 0.0 in stage 54.0 (TID 92, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 23:03:34 INFO Executor: Running task 0.0 in stage 54.0 (TID 92)
17/01/13 23:03:34 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:34 INFO Executor: Finished task 0.0 in stage 54.0 (TID 92). 2174 bytes result sent to driver
17/01/13 23:03:34 INFO DAGScheduler: ResultStage 54 (count at null:-2) finished in 0.007 s
17/01/13 23:03:34 INFO TaskSetManager: Finished task 0.0 in stage 54.0 (TID 92) in 7 ms on localhost (1/1)
17/01/13 23:03:34 INFO DAGScheduler: Job 33 finished: count at null:-2, took 0.048529 s
17/01/13 23:03:34 INFO TaskSchedulerImpl: Removed TaskSet 54.0, whose tasks have all completed, from pool 
17/01/13 23:03:34 INFO ParseDriver: Parsing command: SELECT *
FROM `sparklyr_tmp_22ac45cd4625` AS `zzz10`
WHERE (0 = 1)
17/01/13 23:03:34 INFO ParseDriver: Parse Completed
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 220
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 206
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 205
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 204
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 203
17/01/13 23:03:34 INFO BlockManagerInfo: Removed broadcast_62_piece0 on localhost:63586 in memory (size: 4.7 KB, free: 483.0 MB)
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 202
17/01/13 23:03:34 INFO BlockManagerInfo: Removed broadcast_61_piece0 on localhost:63586 in memory (size: 9.6 KB, free: 483.0 MB)
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 201
17/01/13 23:03:34 INFO ContextCleaner: Cleaned shuffle 18
17/01/13 23:03:34 INFO BlockManagerInfo: Removed broadcast_60_piece0 on localhost:63586 in memory (size: 4.6 KB, free: 483.0 MB)
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 192
17/01/13 23:03:34 INFO BlockManagerInfo: Removed broadcast_59_piece0 on localhost:63586 in memory (size: 9.6 KB, free: 483.0 MB)
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 191
17/01/13 23:03:34 INFO ContextCleaner: Cleaned shuffle 17
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 190
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 189
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 188
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 187
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 186
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 185
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 184
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 183
17/01/13 23:03:34 INFO BlockManagerInfo: Removed broadcast_57_piece0 on localhost:63586 in memory (size: 1956.0 B, free: 483.0 MB)
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 181
17/01/13 23:03:34 INFO BlockManagerInfo: Removed broadcast_56_piece0 on localhost:63586 in memory (size: 19.3 KB, free: 483.1 MB)
17/01/13 23:03:34 INFO BlockManagerInfo: Removed broadcast_55_piece0 on localhost:63586 in memory (size: 1950.0 B, free: 483.1 MB)
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 180
17/01/13 23:03:34 INFO BlockManagerInfo: Removed broadcast_54_piece0 on localhost:63586 in memory (size: 19.3 KB, free: 483.1 MB)
17/01/13 23:03:34 INFO BlockManagerInfo: Removed broadcast_53_piece0 on localhost:63586 in memory (size: 3.0 KB, free: 483.1 MB)
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 179
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 178
17/01/13 23:03:34 INFO BlockManagerInfo: Removed broadcast_52_piece0 on localhost:63586 in memory (size: 8.1 KB, free: 483.1 MB)
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 177
17/01/13 23:03:34 INFO BlockManagerInfo: Removed broadcast_66_piece0 on localhost:63586 in memory (size: 6.3 KB, free: 483.1 MB)
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 236
17/01/13 23:03:34 INFO BlockManagerInfo: Removed broadcast_65_piece0 on localhost:63586 in memory (size: 11.7 KB, free: 483.1 MB)
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 235
17/01/13 23:03:34 INFO ContextCleaner: Cleaned shuffle 20
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 234
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 233
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 232
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 231
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 230
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 229
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 228
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 227
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 226
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 225
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 224
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 223
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 222
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 221
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 219
17/01/13 23:03:34 INFO BlockManagerInfo: Removed broadcast_64_piece0 on localhost:63586 in memory (size: 6.1 KB, free: 483.1 MB)
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 218
17/01/13 23:03:34 INFO ParseDriver: Parsing command: SELECT *
FROM `sparklyr_tmp_22ac45cd4625`
17/01/13 23:03:34 INFO BlockManagerInfo: Removed broadcast_63_piece0 on localhost:63586 in memory (size: 11.5 KB, free: 483.1 MB)
17/01/13 23:03:34 INFO ParseDriver: Parse Completed
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 217
17/01/13 23:03:34 INFO ContextCleaner: Cleaned shuffle 19
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 216
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 215
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 214
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 213
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 212
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 211
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 210
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 209
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 208
17/01/13 23:03:34 INFO ContextCleaner: Cleaned accumulator 207
17/01/13 23:03:34 INFO InMemoryColumnarTableScan: Predicate (hp#1851 >= 100.0) generates partition filter: (100.0 <= hp.upperBound#2387)
17/01/13 23:03:34 INFO ParseDriver: Parsing command: SELECT *
FROM `sparklyr_tmp_22ac1ddc5d19` AS `zzz11`
WHERE (0 = 1)
17/01/13 23:03:34 INFO ParseDriver: Parse Completed
17/01/13 23:03:34 INFO ParseDriver: Parsing command: SELECT *
FROM `sparklyr_tmp_22ac1ddc5d19`
17/01/13 23:03:34 INFO ParseDriver: Parse Completed
17/01/13 23:03:34 INFO ParseDriver: Parsing command: SELECT *
FROM `sparklyr_tmp_22ac289f2b52` AS `zzz12`
WHERE (0 = 1)
17/01/13 23:03:34 INFO ParseDriver: Parse Completed
17/01/13 23:03:34 INFO ParseDriver: Parsing command: SELECT *
FROM `sparklyr_tmp_22ac289f2b52`
17/01/13 23:03:34 INFO ParseDriver: Parse Completed
17/01/13 23:03:35 INFO ParseDriver: Parsing command: SELECT *
FROM `sparklyr_tmp_22ac42b40db` AS `zzz13`
WHERE (0 = 1)
17/01/13 23:03:35 INFO ParseDriver: Parse Completed
17/01/13 23:03:35 INFO ParseDriver: Parsing command: SELECT *
FROM `sparklyr_tmp_22ac42b40db`
17/01/13 23:03:35 INFO ParseDriver: Parse Completed
17/01/13 23:03:35 INFO SparkContext: Starting job: first at LinearRegression.scala:163
17/01/13 23:03:35 INFO DAGScheduler: Registering RDD 241 (map at LinearRegression.scala:161)
17/01/13 23:03:35 INFO DAGScheduler: Got job 34 (first at LinearRegression.scala:163) with 1 output partitions
17/01/13 23:03:35 INFO DAGScheduler: Final stage: ResultStage 56 (first at LinearRegression.scala:163)
17/01/13 23:03:35 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 55)
17/01/13 23:03:35 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 55)
17/01/13 23:03:35 INFO DAGScheduler: Submitting ShuffleMapStage 55 (MapPartitionsRDD[241] at map at LinearRegression.scala:161), which has no missing parents
17/01/13 23:03:35 INFO MemoryStore: Block broadcast_67 stored as values in memory (estimated size 32.8 KB, free 28.9 MB)
17/01/13 23:03:35 INFO MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 13.3 KB, free 29.0 MB)
17/01/13 23:03:35 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on localhost:63586 (size: 13.3 KB, free: 483.1 MB)
17/01/13 23:03:35 INFO SparkContext: Created broadcast 67 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:35 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 55 (MapPartitionsRDD[241] at map at LinearRegression.scala:161)
17/01/13 23:03:35 INFO TaskSchedulerImpl: Adding task set 55.0 with 2 tasks
17/01/13 23:03:35 INFO TaskSetManager: Starting task 0.0 in stage 55.0 (TID 93, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:35 INFO TaskSetManager: Starting task 1.0 in stage 55.0 (TID 94, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:35 INFO Executor: Running task 0.0 in stage 55.0 (TID 93)
17/01/13 23:03:35 INFO Executor: Running task 1.0 in stage 55.0 (TID 94)
17/01/13 23:03:35 INFO CacheManager: Partition rdd_237_0 not found, computing it
17/01/13 23:03:35 INFO CacheManager: Partition rdd_237_1 not found, computing it
17/01/13 23:03:35 INFO BlockManager: Found block rdd_195_0 locally
17/01/13 23:03:35 INFO BlockManager: Found block rdd_195_1 locally
17/01/13 23:03:35 INFO GeneratePredicate: Code generated in 3.745277 ms
17/01/13 23:03:35 INFO GenerateUnsafeProjection: Code generated in 8.873806 ms
17/01/13 23:03:35 INFO MemoryStore: Block rdd_237_0 stored as values in memory (estimated size 1904.0 B, free 29.0 MB)
17/01/13 23:03:35 INFO MemoryStore: Block rdd_237_1 stored as values in memory (estimated size 2.0 KB, free 29.0 MB)
17/01/13 23:03:35 INFO BlockManagerInfo: Added rdd_237_0 in memory on localhost:63586 (size: 1904.0 B, free: 483.1 MB)
17/01/13 23:03:35 INFO BlockManagerInfo: Added rdd_237_1 in memory on localhost:63586 (size: 2.0 KB, free: 483.1 MB)
17/01/13 23:03:35 INFO GenerateColumnAccessor: Code generated in 5.915302 ms
17/01/13 23:03:35 INFO GenerateUnsafeProjection: Code generated in 16.20948 ms
17/01/13 23:03:35 INFO GenerateSafeProjection: Code generated in 6.275835 ms
17/01/13 23:03:36 INFO Executor: Finished task 1.0 in stage 55.0 (TID 94). 4194 bytes result sent to driver
17/01/13 23:03:36 INFO TaskSetManager: Finished task 1.0 in stage 55.0 (TID 94) in 133 ms on localhost (1/2)
17/01/13 23:03:36 INFO Executor: Finished task 0.0 in stage 55.0 (TID 93). 4194 bytes result sent to driver
17/01/13 23:03:36 INFO DAGScheduler: ShuffleMapStage 55 (map at LinearRegression.scala:161) finished in 0.135 s
17/01/13 23:03:36 INFO DAGScheduler: looking for newly runnable stages
17/01/13 23:03:36 INFO DAGScheduler: running: Set()
17/01/13 23:03:36 INFO DAGScheduler: waiting: Set(ResultStage 56)
17/01/13 23:03:36 INFO DAGScheduler: failed: Set()
17/01/13 23:03:36 INFO TaskSetManager: Finished task 0.0 in stage 55.0 (TID 93) in 135 ms on localhost (2/2)
17/01/13 23:03:36 INFO TaskSchedulerImpl: Removed TaskSet 55.0, whose tasks have all completed, from pool 
17/01/13 23:03:36 INFO DAGScheduler: Submitting ResultStage 56 (MapPartitionsRDD[245] at map at LinearRegression.scala:161), which has no missing parents
17/01/13 23:03:36 INFO MemoryStore: Block broadcast_68 stored as values in memory (estimated size 9.4 KB, free 29.0 MB)
17/01/13 23:03:36 INFO MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 5.0 KB, free 29.0 MB)
17/01/13 23:03:36 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on localhost:63586 (size: 5.0 KB, free: 483.1 MB)
17/01/13 23:03:36 INFO SparkContext: Created broadcast 68 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 56 (MapPartitionsRDD[245] at map at LinearRegression.scala:161)
17/01/13 23:03:36 INFO TaskSchedulerImpl: Adding task set 56.0 with 1 tasks
17/01/13 23:03:36 INFO TaskSetManager: Starting task 0.0 in stage 56.0 (TID 95, localhost, partition 0,NODE_LOCAL, 2137 bytes)
17/01/13 23:03:36 INFO Executor: Running task 0.0 in stage 56.0 (TID 95)
17/01/13 23:03:36 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:36 INFO Executor: Finished task 0.0 in stage 56.0 (TID 95). 1047 bytes result sent to driver
17/01/13 23:03:36 INFO DAGScheduler: ResultStage 56 (first at LinearRegression.scala:163) finished in 0.013 s
17/01/13 23:03:36 INFO DAGScheduler: Job 34 finished: first at LinearRegression.scala:163, took 0.161595 s
17/01/13 23:03:36 INFO TaskSetManager: Finished task 0.0 in stage 56.0 (TID 95) in 13 ms on localhost (1/1)
17/01/13 23:03:36 INFO TaskSchedulerImpl: Removed TaskSet 56.0, whose tasks have all completed, from pool 
17/01/13 23:03:36 WARN WeightedLeastSquares: regParam is zero, which might cause numerical instability and overfitting.
17/01/13 23:03:36 INFO SparkContext: Starting job: treeAggregate at WeightedLeastSquares.scala:75
17/01/13 23:03:36 INFO DAGScheduler: Got job 35 (treeAggregate at WeightedLeastSquares.scala:75) with 2 output partitions
17/01/13 23:03:36 INFO DAGScheduler: Final stage: ResultStage 57 (treeAggregate at WeightedLeastSquares.scala:75)
17/01/13 23:03:36 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:36 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:36 INFO DAGScheduler: Submitting ResultStage 57 (MapPartitionsRDD[250] at treeAggregate at WeightedLeastSquares.scala:75), which has no missing parents
17/01/13 23:03:36 INFO MemoryStore: Block broadcast_69 stored as values in memory (estimated size 32.8 KB, free 29.0 MB)
17/01/13 23:03:36 INFO MemoryStore: Block broadcast_69_piece0 stored as bytes in memory (estimated size 13.3 KB, free 29.0 MB)
17/01/13 23:03:36 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on localhost:63586 (size: 13.3 KB, free: 483.1 MB)
17/01/13 23:03:36 INFO SparkContext: Created broadcast 69 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:36 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 57 (MapPartitionsRDD[250] at treeAggregate at WeightedLeastSquares.scala:75)
17/01/13 23:03:36 INFO TaskSchedulerImpl: Adding task set 57.0 with 2 tasks
17/01/13 23:03:36 INFO TaskSetManager: Starting task 0.0 in stage 57.0 (TID 96, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 23:03:36 INFO TaskSetManager: Starting task 1.0 in stage 57.0 (TID 97, localhost, partition 1,PROCESS_LOCAL, 2481 bytes)
17/01/13 23:03:36 INFO Executor: Running task 0.0 in stage 57.0 (TID 96)
17/01/13 23:03:36 INFO Executor: Running task 1.0 in stage 57.0 (TID 97)
17/01/13 23:03:36 INFO BlockManager: Found block rdd_237_1 locally
17/01/13 23:03:36 INFO BlockManager: Found block rdd_237_0 locally
17/01/13 23:03:36 INFO GenerateColumnAccessor: Code generated in 7.861753 ms
17/01/13 23:03:36 INFO GenerateUnsafeProjection: Code generated in 12.590069 ms
17/01/13 23:03:36 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
17/01/13 23:03:36 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
17/01/13 23:03:36 INFO Executor: Finished task 0.0 in stage 57.0 (TID 96). 3096 bytes result sent to driver
17/01/13 23:03:36 INFO Executor: Finished task 1.0 in stage 57.0 (TID 97). 3096 bytes result sent to driver
17/01/13 23:03:36 INFO TaskSetManager: Finished task 0.0 in stage 57.0 (TID 96) in 56 ms on localhost (1/2)
17/01/13 23:03:36 INFO DAGScheduler: ResultStage 57 (treeAggregate at WeightedLeastSquares.scala:75) finished in 0.058 s
17/01/13 23:03:36 INFO TaskSetManager: Finished task 1.0 in stage 57.0 (TID 97) in 57 ms on localhost (2/2)
17/01/13 23:03:36 INFO TaskSchedulerImpl: Removed TaskSet 57.0, whose tasks have all completed, from pool 
17/01/13 23:03:36 INFO DAGScheduler: Job 35 finished: treeAggregate at WeightedLeastSquares.scala:75, took 0.063370 s
17/01/13 23:03:36 INFO WeightedLeastSquares: Number of instances: 12.
17/01/13 23:03:36 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK
17/01/13 23:03:36 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK
17/01/13 23:03:36 INFO SparkContext: Starting job: aggregate at RegressionMetrics.scala:50
17/01/13 23:03:36 INFO DAGScheduler: Got job 36 (aggregate at RegressionMetrics.scala:50) with 2 output partitions
17/01/13 23:03:36 INFO DAGScheduler: Final stage: ResultStage 58 (aggregate at RegressionMetrics.scala:50)
17/01/13 23:03:36 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:36 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:36 INFO DAGScheduler: Submitting ResultStage 58 (MapPartitionsRDD[255] at map at RegressionMetrics.scala:48), which has no missing parents
17/01/13 23:03:36 INFO MemoryStore: Block broadcast_70 stored as values in memory (estimated size 35.8 KB, free 29.1 MB)
17/01/13 23:03:36 INFO MemoryStore: Block broadcast_70_piece0 stored as bytes in memory (estimated size 15.0 KB, free 29.1 MB)
17/01/13 23:03:36 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on localhost:63586 (size: 15.0 KB, free: 483.1 MB)
17/01/13 23:03:36 INFO SparkContext: Created broadcast 70 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:36 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 58 (MapPartitionsRDD[255] at map at RegressionMetrics.scala:48)
17/01/13 23:03:36 INFO TaskSchedulerImpl: Adding task set 58.0 with 2 tasks
17/01/13 23:03:36 INFO TaskSetManager: Starting task 0.0 in stage 58.0 (TID 98, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 23:03:36 INFO TaskSetManager: Starting task 1.0 in stage 58.0 (TID 99, localhost, partition 1,PROCESS_LOCAL, 2481 bytes)
17/01/13 23:03:36 INFO Executor: Running task 0.0 in stage 58.0 (TID 98)
17/01/13 23:03:36 INFO Executor: Running task 1.0 in stage 58.0 (TID 99)
17/01/13 23:03:36 INFO BlockManager: Found block rdd_237_1 locally
17/01/13 23:03:36 INFO BlockManager: Found block rdd_237_0 locally
17/01/13 23:03:36 INFO GenerateUnsafeProjection: Code generated in 11.324577 ms
17/01/13 23:03:36 INFO Executor: Finished task 1.0 in stage 58.0 (TID 99). 3068 bytes result sent to driver
17/01/13 23:03:36 INFO Executor: Finished task 0.0 in stage 58.0 (TID 98). 3068 bytes result sent to driver
17/01/13 23:03:36 INFO TaskSetManager: Finished task 1.0 in stage 58.0 (TID 99) in 34 ms on localhost (1/2)
17/01/13 23:03:36 INFO DAGScheduler: ResultStage 58 (aggregate at RegressionMetrics.scala:50) finished in 0.037 s
17/01/13 23:03:36 INFO TaskSetManager: Finished task 0.0 in stage 58.0 (TID 98) in 36 ms on localhost (2/2)
17/01/13 23:03:36 INFO TaskSchedulerImpl: Removed TaskSet 58.0, whose tasks have all completed, from pool 
17/01/13 23:03:36 INFO DAGScheduler: Job 36 finished: aggregate at RegressionMetrics.scala:50, took 0.042989 s
17/01/13 23:03:36 INFO SparkContext: Starting job: sum at RegressionMetrics.scala:62
17/01/13 23:03:36 INFO DAGScheduler: Got job 37 (sum at RegressionMetrics.scala:62) with 2 output partitions
17/01/13 23:03:36 INFO DAGScheduler: Final stage: ResultStage 59 (sum at RegressionMetrics.scala:62)
17/01/13 23:03:36 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:36 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:36 INFO DAGScheduler: Submitting ResultStage 59 (MapPartitionsRDD[256] at map at RegressionMetrics.scala:60), which has no missing parents
17/01/13 23:03:36 INFO MemoryStore: Block broadcast_71 stored as values in memory (estimated size 35.4 KB, free 29.1 MB)
17/01/13 23:03:36 INFO MemoryStore: Block broadcast_71_piece0 stored as bytes in memory (estimated size 15.0 KB, free 29.1 MB)
17/01/13 23:03:36 INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on localhost:63586 (size: 15.0 KB, free: 483.1 MB)
17/01/13 23:03:36 INFO SparkContext: Created broadcast 71 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:36 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 59 (MapPartitionsRDD[256] at map at RegressionMetrics.scala:60)
17/01/13 23:03:36 INFO TaskSchedulerImpl: Adding task set 59.0 with 2 tasks
17/01/13 23:03:36 INFO TaskSetManager: Starting task 0.0 in stage 59.0 (TID 100, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 23:03:36 INFO TaskSetManager: Starting task 1.0 in stage 59.0 (TID 101, localhost, partition 1,PROCESS_LOCAL, 2481 bytes)
17/01/13 23:03:36 INFO Executor: Running task 1.0 in stage 59.0 (TID 101)
17/01/13 23:03:36 INFO Executor: Running task 0.0 in stage 59.0 (TID 100)
17/01/13 23:03:36 INFO BlockManager: Found block rdd_237_0 locally
17/01/13 23:03:36 INFO BlockManager: Found block rdd_237_1 locally
17/01/13 23:03:36 INFO Executor: Finished task 1.0 in stage 59.0 (TID 101). 2646 bytes result sent to driver
17/01/13 23:03:36 INFO TaskSetManager: Finished task 1.0 in stage 59.0 (TID 101) in 9 ms on localhost (1/2)
17/01/13 23:03:36 INFO Executor: Finished task 0.0 in stage 59.0 (TID 100). 2646 bytes result sent to driver
17/01/13 23:03:36 INFO DAGScheduler: ResultStage 59 (sum at RegressionMetrics.scala:62) finished in 0.011 s
17/01/13 23:03:36 INFO TaskSetManager: Finished task 0.0 in stage 59.0 (TID 100) in 11 ms on localhost (2/2)
17/01/13 23:03:36 INFO DAGScheduler: Job 37 finished: sum at RegressionMetrics.scala:62, took 0.016095 s
17/01/13 23:03:36 INFO TaskSchedulerImpl: Removed TaskSet 59.0, whose tasks have all completed, from pool 
17/01/13 23:03:36 INFO SparkContext: Starting job: count at LinearRegression.scala:598
17/01/13 23:03:36 INFO DAGScheduler: Registering RDD 259 (count at LinearRegression.scala:598)
17/01/13 23:03:36 INFO DAGScheduler: Got job 38 (count at LinearRegression.scala:598) with 1 output partitions
17/01/13 23:03:36 INFO DAGScheduler: Final stage: ResultStage 61 (count at LinearRegression.scala:598)
17/01/13 23:03:36 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 60)
17/01/13 23:03:36 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 60)
17/01/13 23:03:36 INFO DAGScheduler: Submitting ShuffleMapStage 60 (MapPartitionsRDD[259] at count at LinearRegression.scala:598), which has no missing parents
17/01/13 23:03:36 INFO MemoryStore: Block broadcast_72 stored as values in memory (estimated size 33.1 KB, free 29.1 MB)
17/01/13 23:03:36 INFO MemoryStore: Block broadcast_72_piece0 stored as bytes in memory (estimated size 13.1 KB, free 29.2 MB)
17/01/13 23:03:36 INFO BlockManagerInfo: Added broadcast_72_piece0 in memory on localhost:63586 (size: 13.1 KB, free: 483.0 MB)
17/01/13 23:03:36 INFO SparkContext: Created broadcast 72 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:36 INFO BlockManagerInfo: Removed broadcast_71_piece0 on localhost:63586 in memory (size: 15.0 KB, free: 483.1 MB)
17/01/13 23:03:36 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 60 (MapPartitionsRDD[259] at count at LinearRegression.scala:598)
17/01/13 23:03:36 INFO TaskSchedulerImpl: Adding task set 60.0 with 2 tasks
17/01/13 23:03:36 INFO ContextCleaner: Cleaned accumulator 250
17/01/13 23:03:36 INFO TaskSetManager: Starting task 0.0 in stage 60.0 (TID 102, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:36 INFO TaskSetManager: Starting task 1.0 in stage 60.0 (TID 103, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:36 INFO Executor: Running task 0.0 in stage 60.0 (TID 102)
17/01/13 23:03:36 INFO Executor: Running task 1.0 in stage 60.0 (TID 103)
17/01/13 23:03:36 INFO BlockManager: Found block rdd_237_0 locally
17/01/13 23:03:36 INFO BlockManagerInfo: Removed broadcast_70_piece0 on localhost:63586 in memory (size: 15.0 KB, free: 483.1 MB)
17/01/13 23:03:36 INFO BlockManager: Found block rdd_237_1 locally
17/01/13 23:03:36 INFO Executor: Finished task 1.0 in stage 60.0 (TID 103). 2944 bytes result sent to driver
17/01/13 23:03:36 INFO ContextCleaner: Cleaned accumulator 249
17/01/13 23:03:36 INFO TaskSetManager: Finished task 1.0 in stage 60.0 (TID 103) in 20 ms on localhost (1/2)
17/01/13 23:03:36 INFO BlockManagerInfo: Removed broadcast_69_piece0 on localhost:63586 in memory (size: 13.3 KB, free: 483.1 MB)
17/01/13 23:03:36 INFO Executor: Finished task 0.0 in stage 60.0 (TID 102). 2944 bytes result sent to driver
17/01/13 23:03:36 INFO ContextCleaner: Cleaned accumulator 247
17/01/13 23:03:36 INFO DAGScheduler: ShuffleMapStage 60 (count at LinearRegression.scala:598) finished in 0.024 s
17/01/13 23:03:36 INFO DAGScheduler: looking for newly runnable stages
17/01/13 23:03:36 INFO DAGScheduler: running: Set()
17/01/13 23:03:36 INFO DAGScheduler: waiting: Set(ResultStage 61)
17/01/13 23:03:36 INFO DAGScheduler: failed: Set()
17/01/13 23:03:36 INFO DAGScheduler: Submitting ResultStage 61 (MapPartitionsRDD[262] at count at LinearRegression.scala:598), which has no missing parents
17/01/13 23:03:36 INFO TaskSetManager: Finished task 0.0 in stage 60.0 (TID 102) in 24 ms on localhost (2/2)
17/01/13 23:03:36 INFO TaskSchedulerImpl: Removed TaskSet 60.0, whose tasks have all completed, from pool 
17/01/13 23:03:36 INFO MemoryStore: Block broadcast_73 stored as values in memory (estimated size 9.3 KB, free 29.0 MB)
17/01/13 23:03:36 INFO BlockManagerInfo: Removed broadcast_68_piece0 on localhost:63586 in memory (size: 5.0 KB, free: 483.1 MB)
17/01/13 23:03:36 INFO MemoryStore: Block broadcast_73_piece0 stored as bytes in memory (estimated size 4.6 KB, free 29.0 MB)
17/01/13 23:03:36 INFO BlockManagerInfo: Added broadcast_73_piece0 in memory on localhost:63586 (size: 4.6 KB, free: 483.1 MB)
17/01/13 23:03:36 INFO ContextCleaner: Cleaned accumulator 245
17/01/13 23:03:36 INFO SparkContext: Created broadcast 73 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 61 (MapPartitionsRDD[262] at count at LinearRegression.scala:598)
17/01/13 23:03:36 INFO TaskSchedulerImpl: Adding task set 61.0 with 1 tasks
17/01/13 23:03:36 INFO TaskSetManager: Starting task 0.0 in stage 61.0 (TID 104, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 23:03:36 INFO BlockManagerInfo: Removed broadcast_67_piece0 on localhost:63586 in memory (size: 13.3 KB, free: 483.1 MB)
17/01/13 23:03:36 INFO Executor: Running task 0.0 in stage 61.0 (TID 104)
17/01/13 23:03:36 INFO ContextCleaner: Cleaned accumulator 244
17/01/13 23:03:36 INFO ContextCleaner: Cleaned shuffle 21
17/01/13 23:03:36 INFO ContextCleaner: Cleaned accumulator 243
17/01/13 23:03:36 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:36 INFO Executor: Finished task 0.0 in stage 61.0 (TID 104). 1830 bytes result sent to driver
17/01/13 23:03:36 INFO DAGScheduler: ResultStage 61 (count at LinearRegression.scala:598) finished in 0.009 s
17/01/13 23:03:36 INFO TaskSetManager: Finished task 0.0 in stage 61.0 (TID 104) in 9 ms on localhost (1/1)
17/01/13 23:03:36 INFO DAGScheduler: Job 38 finished: count at LinearRegression.scala:598, took 0.058202 s
17/01/13 23:03:36 INFO TaskSchedulerImpl: Removed TaskSet 61.0, whose tasks have all completed, from pool 
17/01/13 23:03:37 INFO SparkContext: Starting job: count at null:-2
17/01/13 23:03:37 INFO DAGScheduler: Registering RDD 265 (count at null:-2)
17/01/13 23:03:37 INFO DAGScheduler: Got job 39 (count at null:-2) with 1 output partitions
17/01/13 23:03:37 INFO DAGScheduler: Final stage: ResultStage 63 (count at null:-2)
17/01/13 23:03:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 62)
17/01/13 23:03:37 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 62)
17/01/13 23:03:37 INFO DAGScheduler: Submitting ShuffleMapStage 62 (MapPartitionsRDD[265] at count at null:-2), which has no missing parents
17/01/13 23:03:37 INFO MemoryStore: Block broadcast_74 stored as values in memory (estimated size 33.1 KB, free 29.0 MB)
17/01/13 23:03:37 INFO MemoryStore: Block broadcast_74_piece0 stored as bytes in memory (estimated size 13.2 KB, free 29.0 MB)
17/01/13 23:03:37 INFO BlockManagerInfo: Added broadcast_74_piece0 in memory on localhost:63586 (size: 13.2 KB, free: 483.1 MB)
17/01/13 23:03:37 INFO SparkContext: Created broadcast 74 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:37 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 62 (MapPartitionsRDD[265] at count at null:-2)
17/01/13 23:03:37 INFO TaskSchedulerImpl: Adding task set 62.0 with 2 tasks
17/01/13 23:03:37 INFO TaskSetManager: Starting task 0.0 in stage 62.0 (TID 105, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:37 INFO TaskSetManager: Starting task 1.0 in stage 62.0 (TID 106, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:37 INFO Executor: Running task 1.0 in stage 62.0 (TID 106)
17/01/13 23:03:37 INFO Executor: Running task 0.0 in stage 62.0 (TID 105)
17/01/13 23:03:37 INFO BlockManager: Found block rdd_237_0 locally
17/01/13 23:03:37 INFO BlockManager: Found block rdd_237_1 locally
17/01/13 23:03:37 INFO Executor: Finished task 0.0 in stage 62.0 (TID 105). 2944 bytes result sent to driver
17/01/13 23:03:37 INFO TaskSetManager: Finished task 0.0 in stage 62.0 (TID 105) in 17 ms on localhost (1/2)
17/01/13 23:03:37 INFO Executor: Finished task 1.0 in stage 62.0 (TID 106). 2944 bytes result sent to driver
17/01/13 23:03:37 INFO DAGScheduler: ShuffleMapStage 62 (count at null:-2) finished in 0.019 s
17/01/13 23:03:37 INFO TaskSetManager: Finished task 1.0 in stage 62.0 (TID 106) in 18 ms on localhost (2/2)
17/01/13 23:03:37 INFO DAGScheduler: looking for newly runnable stages
17/01/13 23:03:37 INFO DAGScheduler: running: Set()
17/01/13 23:03:37 INFO DAGScheduler: waiting: Set(ResultStage 63)
17/01/13 23:03:37 INFO DAGScheduler: failed: Set()
17/01/13 23:03:37 INFO DAGScheduler: Submitting ResultStage 63 (MapPartitionsRDD[268] at count at null:-2), which has no missing parents
17/01/13 23:03:37 INFO TaskSchedulerImpl: Removed TaskSet 62.0, whose tasks have all completed, from pool 
17/01/13 23:03:37 INFO MemoryStore: Block broadcast_75 stored as values in memory (estimated size 9.3 KB, free 29.0 MB)
17/01/13 23:03:37 INFO MemoryStore: Block broadcast_75_piece0 stored as bytes in memory (estimated size 4.6 KB, free 29.0 MB)
17/01/13 23:03:37 INFO BlockManagerInfo: Added broadcast_75_piece0 in memory on localhost:63586 (size: 4.6 KB, free: 483.1 MB)
17/01/13 23:03:37 INFO SparkContext: Created broadcast 75 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 63 (MapPartitionsRDD[268] at count at null:-2)
17/01/13 23:03:37 INFO TaskSchedulerImpl: Adding task set 63.0 with 1 tasks
17/01/13 23:03:37 INFO TaskSetManager: Starting task 0.0 in stage 63.0 (TID 107, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 23:03:37 INFO Executor: Running task 0.0 in stage 63.0 (TID 107)
17/01/13 23:03:37 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:37 INFO Executor: Finished task 0.0 in stage 63.0 (TID 107). 1830 bytes result sent to driver
17/01/13 23:03:37 INFO TaskSetManager: Finished task 0.0 in stage 63.0 (TID 107) in 6 ms on localhost (1/1)
17/01/13 23:03:37 INFO DAGScheduler: ResultStage 63 (count at null:-2) finished in 0.006 s
17/01/13 23:03:37 INFO TaskSchedulerImpl: Removed TaskSet 63.0, whose tasks have all completed, from pool 
17/01/13 23:03:37 INFO DAGScheduler: Job 39 finished: count at null:-2, took 0.035578 s
17/01/13 23:03:37 INFO SparkContext: Starting job: collect at utils.scala:52
17/01/13 23:03:37 INFO DAGScheduler: Got job 40 (collect at utils.scala:52) with 2 output partitions
17/01/13 23:03:37 INFO DAGScheduler: Final stage: ResultStage 64 (collect at utils.scala:52)
17/01/13 23:03:37 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:37 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:37 INFO DAGScheduler: Submitting ResultStage 64 (MapPartitionsRDD[272] at map at utils.scala:49), which has no missing parents
17/01/13 23:03:37 INFO MemoryStore: Block broadcast_76 stored as values in memory (estimated size 36.6 KB, free 29.1 MB)
17/01/13 23:03:37 INFO MemoryStore: Block broadcast_76_piece0 stored as bytes in memory (estimated size 15.6 KB, free 29.1 MB)
17/01/13 23:03:37 INFO BlockManagerInfo: Added broadcast_76_piece0 in memory on localhost:63586 (size: 15.6 KB, free: 483.1 MB)
17/01/13 23:03:37 INFO SparkContext: Created broadcast 76 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:37 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 64 (MapPartitionsRDD[272] at map at utils.scala:49)
17/01/13 23:03:37 INFO TaskSchedulerImpl: Adding task set 64.0 with 2 tasks
17/01/13 23:03:37 INFO TaskSetManager: Starting task 0.0 in stage 64.0 (TID 108, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 23:03:37 INFO TaskSetManager: Starting task 1.0 in stage 64.0 (TID 109, localhost, partition 1,PROCESS_LOCAL, 2481 bytes)
17/01/13 23:03:37 INFO Executor: Running task 1.0 in stage 64.0 (TID 109)
17/01/13 23:03:37 INFO Executor: Running task 0.0 in stage 64.0 (TID 108)
17/01/13 23:03:37 INFO BlockManager: Found block rdd_237_0 locally
17/01/13 23:03:37 INFO BlockManager: Found block rdd_237_1 locally
17/01/13 23:03:37 INFO GenerateUnsafeProjection: Code generated in 14.970868 ms
17/01/13 23:03:37 INFO Executor: Finished task 0.0 in stage 64.0 (TID 108). 2629 bytes result sent to driver
17/01/13 23:03:37 INFO Executor: Finished task 1.0 in stage 64.0 (TID 109). 2645 bytes result sent to driver
17/01/13 23:03:37 INFO TaskSetManager: Finished task 0.0 in stage 64.0 (TID 108) in 29 ms on localhost (1/2)
17/01/13 23:03:37 INFO DAGScheduler: ResultStage 64 (collect at utils.scala:52) finished in 0.030 s
17/01/13 23:03:37 INFO DAGScheduler: Job 40 finished: collect at utils.scala:52, took 0.034812 s
17/01/13 23:03:37 INFO TaskSetManager: Finished task 1.0 in stage 64.0 (TID 109) in 29 ms on localhost (2/2)
17/01/13 23:03:37 INFO TaskSchedulerImpl: Removed TaskSet 64.0, whose tasks have all completed, from pool 
17/01/13 23:03:37 INFO ParseDriver: Parsing command: SELECT *
FROM `iris`
17/01/13 23:03:37 INFO ParseDriver: Parse Completed
17/01/13 23:03:37 INFO SparkContext: Starting job: saveAsTextFile at package.scala:180
17/01/13 23:03:37 INFO DAGScheduler: Got job 41 (saveAsTextFile at package.scala:180) with 2 output partitions
17/01/13 23:03:37 INFO DAGScheduler: Final stage: ResultStage 65 (saveAsTextFile at package.scala:180)
17/01/13 23:03:37 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:37 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:37 INFO DAGScheduler: Submitting ResultStage 65 (MapPartitionsRDD[276] at saveAsTextFile at package.scala:180), which has no missing parents
17/01/13 23:03:37 INFO MemoryStore: Block broadcast_77 stored as values in memory (estimated size 76.1 KB, free 29.2 MB)
17/01/13 23:03:37 INFO MemoryStore: Block broadcast_77_piece0 stored as bytes in memory (estimated size 28.2 KB, free 29.2 MB)
17/01/13 23:03:37 INFO BlockManagerInfo: Added broadcast_77_piece0 in memory on localhost:63586 (size: 28.2 KB, free: 483.0 MB)
17/01/13 23:03:37 INFO SparkContext: Created broadcast 77 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:37 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 65 (MapPartitionsRDD[276] at saveAsTextFile at package.scala:180)
17/01/13 23:03:37 INFO TaskSchedulerImpl: Adding task set 65.0 with 2 tasks
17/01/13 23:03:37 INFO TaskSetManager: Starting task 0.0 in stage 65.0 (TID 110, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 23:03:37 INFO TaskSetManager: Starting task 1.0 in stage 65.0 (TID 111, localhost, partition 1,PROCESS_LOCAL, 2481 bytes)
17/01/13 23:03:37 INFO Executor: Running task 1.0 in stage 65.0 (TID 111)
17/01/13 23:03:37 INFO Executor: Running task 0.0 in stage 65.0 (TID 110)
17/01/13 23:03:37 INFO BlockManager: Found block rdd_14_0 locally
17/01/13 23:03:37 INFO BlockManager: Found block rdd_14_1 locally
17/01/13 23:03:37 INFO FileOutputCommitter: Saved output of task 'attempt_201701132303_0065_m_000000_110' to file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/file22acfadaa8.csv/_temporary/0/task_201701132303_0065_m_000000
17/01/13 23:03:37 INFO SparkHadoopMapRedUtil: attempt_201701132303_0065_m_000000_110: Committed
17/01/13 23:03:37 INFO Executor: Finished task 0.0 in stage 65.0 (TID 110). 2516 bytes result sent to driver
17/01/13 23:03:37 INFO TaskSetManager: Finished task 0.0 in stage 65.0 (TID 110) in 124 ms on localhost (1/2)
17/01/13 23:03:37 INFO FileOutputCommitter: Saved output of task 'attempt_201701132303_0065_m_000001_111' to file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/file22acfadaa8.csv/_temporary/0/task_201701132303_0065_m_000001
17/01/13 23:03:37 INFO SparkHadoopMapRedUtil: attempt_201701132303_0065_m_000001_111: Committed
17/01/13 23:03:37 INFO Executor: Finished task 1.0 in stage 65.0 (TID 111). 2516 bytes result sent to driver
17/01/13 23:03:37 INFO TaskSetManager: Finished task 1.0 in stage 65.0 (TID 111) in 133 ms on localhost (2/2)
17/01/13 23:03:37 INFO DAGScheduler: ResultStage 65 (saveAsTextFile at package.scala:180) finished in 0.133 s
17/01/13 23:03:37 INFO TaskSchedulerImpl: Removed TaskSet 65.0, whose tasks have all completed, from pool 
17/01/13 23:03:37 INFO DAGScheduler: Job 41 finished: saveAsTextFile at package.scala:180, took 0.147282 s
17/01/13 23:03:37 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/01/13 23:03:37 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/01/13 23:03:37 INFO SparkContext: Starting job: collect at utils.scala:59
17/01/13 23:03:37 INFO DAGScheduler: Got job 42 (collect at utils.scala:59) with 1 output partitions
17/01/13 23:03:37 INFO DAGScheduler: Final stage: ResultStage 66 (collect at utils.scala:59)
17/01/13 23:03:37 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:37 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:37 INFO DAGScheduler: Submitting ResultStage 66 (MapPartitionsRDD[280] at map at utils.scala:56), which has no missing parents
17/01/13 23:03:37 INFO MemoryStore: Block broadcast_78 stored as values in memory (estimated size 5.4 KB, free 29.2 MB)
17/01/13 23:03:37 INFO MemoryStore: Block broadcast_78_piece0 stored as bytes in memory (estimated size 3.0 KB, free 29.2 MB)
17/01/13 23:03:37 INFO BlockManagerInfo: Added broadcast_78_piece0 in memory on localhost:63586 (size: 3.0 KB, free: 483.0 MB)
17/01/13 23:03:37 INFO SparkContext: Created broadcast 78 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 66 (MapPartitionsRDD[280] at map at utils.scala:56)
17/01/13 23:03:37 INFO TaskSchedulerImpl: Adding task set 66.0 with 1 tasks
17/01/13 23:03:37 INFO TaskSetManager: Starting task 0.0 in stage 66.0 (TID 112, localhost, partition 0,PROCESS_LOCAL, 3121 bytes)
17/01/13 23:03:37 INFO Executor: Running task 0.0 in stage 66.0 (TID 112)
17/01/13 23:03:37 INFO Executor: Finished task 0.0 in stage 66.0 (TID 112). 1266 bytes result sent to driver
17/01/13 23:03:37 INFO TaskSetManager: Finished task 0.0 in stage 66.0 (TID 112) in 2 ms on localhost (1/1)
17/01/13 23:03:37 INFO DAGScheduler: ResultStage 66 (collect at utils.scala:59) finished in 0.003 s
17/01/13 23:03:37 INFO TaskSchedulerImpl: Removed TaskSet 66.0, whose tasks have all completed, from pool 
17/01/13 23:03:37 INFO DAGScheduler: Job 42 finished: collect at utils.scala:59, took 0.005070 s
17/01/13 23:03:37 INFO MemoryStore: Block broadcast_79 stored as values in memory (estimated size 208.5 KB, free 29.4 MB)
17/01/13 23:03:37 INFO MemoryStore: Block broadcast_79_piece0 stored as bytes in memory (estimated size 19.3 KB, free 29.4 MB)
17/01/13 23:03:37 INFO BlockManagerInfo: Added broadcast_79_piece0 in memory on localhost:63586 (size: 19.3 KB, free: 483.0 MB)
17/01/13 23:03:37 INFO SparkContext: Created broadcast 79 from textFile at TextFile.scala:30
17/01/13 23:03:37 INFO FileInputFormat: Total input paths to process : 2
17/01/13 23:03:37 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/01/13 23:03:37 INFO DAGScheduler: Got job 43 (take at CsvRelation.scala:249) with 1 output partitions
17/01/13 23:03:37 INFO DAGScheduler: Final stage: ResultStage 67 (take at CsvRelation.scala:249)
17/01/13 23:03:37 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:37 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:37 INFO DAGScheduler: Submitting ResultStage 67 (C:\Users\Baoco\AppData\Local\Temp\RtmpOgwhNO\file22acfadaa8.csv MapPartitionsRDD[282] at textFile at TextFile.scala:30), which has no missing parents
17/01/13 23:03:37 INFO MemoryStore: Block broadcast_80 stored as values in memory (estimated size 3.1 KB, free 29.4 MB)
17/01/13 23:03:37 INFO MemoryStore: Block broadcast_80_piece0 stored as bytes in memory (estimated size 1887.0 B, free 29.4 MB)
17/01/13 23:03:37 INFO BlockManagerInfo: Added broadcast_80_piece0 in memory on localhost:63586 (size: 1887.0 B, free: 483.0 MB)
17/01/13 23:03:37 INFO SparkContext: Created broadcast 80 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 67 (C:\Users\Baoco\AppData\Local\Temp\RtmpOgwhNO\file22acfadaa8.csv MapPartitionsRDD[282] at textFile at TextFile.scala:30)
17/01/13 23:03:37 INFO TaskSchedulerImpl: Adding task set 67.0 with 1 tasks
17/01/13 23:03:37 INFO TaskSetManager: Starting task 0.0 in stage 67.0 (TID 113, localhost, partition 0,PROCESS_LOCAL, 2425 bytes)
17/01/13 23:03:37 INFO Executor: Running task 0.0 in stage 67.0 (TID 113)
17/01/13 23:03:37 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/file22acfadaa8.csv/part-00000:0+1911
17/01/13 23:03:37 INFO Executor: Finished task 0.0 in stage 67.0 (TID 113). 2332 bytes result sent to driver
17/01/13 23:03:37 INFO TaskSetManager: Finished task 0.0 in stage 67.0 (TID 113) in 5 ms on localhost (1/1)
17/01/13 23:03:37 INFO DAGScheduler: ResultStage 67 (take at CsvRelation.scala:249) finished in 0.005 s
17/01/13 23:03:37 INFO TaskSchedulerImpl: Removed TaskSet 67.0, whose tasks have all completed, from pool 
17/01/13 23:03:37 INFO DAGScheduler: Job 43 finished: take at CsvRelation.scala:249, took 0.007998 s
17/01/13 23:03:37 INFO MemoryStore: Block broadcast_81 stored as values in memory (estimated size 208.5 KB, free 29.6 MB)
17/01/13 23:03:37 INFO MemoryStore: Block broadcast_81_piece0 stored as bytes in memory (estimated size 19.3 KB, free 29.6 MB)
17/01/13 23:03:37 INFO BlockManagerInfo: Added broadcast_81_piece0 in memory on localhost:63586 (size: 19.3 KB, free: 483.0 MB)
17/01/13 23:03:37 INFO SparkContext: Created broadcast 81 from textFile at TextFile.scala:30
17/01/13 23:03:37 INFO FileInputFormat: Total input paths to process : 2
17/01/13 23:03:37 INFO SparkContext: Starting job: aggregate at InferSchema.scala:36
17/01/13 23:03:37 INFO DAGScheduler: Got job 44 (aggregate at InferSchema.scala:36) with 2 output partitions
17/01/13 23:03:37 INFO DAGScheduler: Final stage: ResultStage 68 (aggregate at InferSchema.scala:36)
17/01/13 23:03:37 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:37 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:37 INFO DAGScheduler: Submitting ResultStage 68 (MapPartitionsRDD[285] at mapPartitions at CsvRelation.scala:90), which has no missing parents
17/01/13 23:03:37 INFO MemoryStore: Block broadcast_82 stored as values in memory (estimated size 6.0 KB, free 29.6 MB)
17/01/13 23:03:37 INFO MemoryStore: Block broadcast_82_piece0 stored as bytes in memory (estimated size 3.5 KB, free 29.7 MB)
17/01/13 23:03:37 INFO BlockManagerInfo: Added broadcast_82_piece0 in memory on localhost:63586 (size: 3.5 KB, free: 483.0 MB)
17/01/13 23:03:37 INFO SparkContext: Created broadcast 82 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:37 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 68 (MapPartitionsRDD[285] at mapPartitions at CsvRelation.scala:90)
17/01/13 23:03:37 INFO TaskSchedulerImpl: Adding task set 68.0 with 2 tasks
17/01/13 23:03:37 INFO TaskSetManager: Starting task 0.0 in stage 68.0 (TID 114, localhost, partition 0,PROCESS_LOCAL, 2425 bytes)
17/01/13 23:03:37 INFO TaskSetManager: Starting task 1.0 in stage 68.0 (TID 115, localhost, partition 1,PROCESS_LOCAL, 2425 bytes)
17/01/13 23:03:37 INFO Executor: Running task 1.0 in stage 68.0 (TID 115)
17/01/13 23:03:37 INFO Executor: Running task 0.0 in stage 68.0 (TID 114)
17/01/13 23:03:37 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/file22acfadaa8.csv/part-00001:0+2007
17/01/13 23:03:37 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/file22acfadaa8.csv/part-00000:0+1911
17/01/13 23:03:37 INFO Executor: Finished task 0.0 in stage 68.0 (TID 114). 2188 bytes result sent to driver
17/01/13 23:03:37 INFO Executor: Finished task 1.0 in stage 68.0 (TID 115). 2188 bytes result sent to driver
17/01/13 23:03:37 INFO TaskSetManager: Finished task 0.0 in stage 68.0 (TID 114) in 10 ms on localhost (1/2)
17/01/13 23:03:37 INFO TaskSetManager: Finished task 1.0 in stage 68.0 (TID 115) in 11 ms on localhost (2/2)
17/01/13 23:03:37 INFO TaskSchedulerImpl: Removed TaskSet 68.0, whose tasks have all completed, from pool 
17/01/13 23:03:37 INFO DAGScheduler: ResultStage 68 (aggregate at InferSchema.scala:36) finished in 0.012 s
17/01/13 23:03:37 INFO DAGScheduler: Job 44 finished: aggregate at InferSchema.scala:36, took 0.014799 s
17/01/13 23:03:37 INFO MemoryStore: Block broadcast_83 stored as values in memory (estimated size 208.5 KB, free 29.9 MB)
17/01/13 23:03:37 INFO MemoryStore: Block broadcast_83_piece0 stored as bytes in memory (estimated size 19.3 KB, free 29.9 MB)
17/01/13 23:03:37 INFO BlockManagerInfo: Added broadcast_83_piece0 in memory on localhost:63586 (size: 19.3 KB, free: 483.0 MB)
17/01/13 23:03:37 INFO SparkContext: Created broadcast 83 from textFile at TextFile.scala:30
17/01/13 23:03:38 INFO FileInputFormat: Total input paths to process : 2
17/01/13 23:03:38 INFO SparkContext: Starting job: sql at null:-1
17/01/13 23:03:38 INFO DAGScheduler: Registering RDD 295 (sql at null:-1)
17/01/13 23:03:38 INFO DAGScheduler: Got job 45 (sql at null:-1) with 1 output partitions
17/01/13 23:03:38 INFO DAGScheduler: Final stage: ResultStage 70 (sql at null:-1)
17/01/13 23:03:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 69)
17/01/13 23:03:38 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 69)
17/01/13 23:03:38 INFO DAGScheduler: Submitting ShuffleMapStage 69 (MapPartitionsRDD[295] at sql at null:-1), which has no missing parents
17/01/13 23:03:38 INFO MemoryStore: Block broadcast_84 stored as values in memory (estimated size 18.0 KB, free 29.9 MB)
17/01/13 23:03:38 INFO MemoryStore: Block broadcast_84_piece0 stored as bytes in memory (estimated size 8.5 KB, free 29.9 MB)
17/01/13 23:03:38 INFO BlockManagerInfo: Added broadcast_84_piece0 in memory on localhost:63586 (size: 8.5 KB, free: 483.0 MB)
17/01/13 23:03:38 INFO SparkContext: Created broadcast 84 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:38 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 69 (MapPartitionsRDD[295] at sql at null:-1)
17/01/13 23:03:38 INFO TaskSchedulerImpl: Adding task set 69.0 with 2 tasks
17/01/13 23:03:38 INFO TaskSetManager: Starting task 0.0 in stage 69.0 (TID 116, localhost, partition 0,PROCESS_LOCAL, 2414 bytes)
17/01/13 23:03:38 INFO TaskSetManager: Starting task 1.0 in stage 69.0 (TID 117, localhost, partition 1,PROCESS_LOCAL, 2414 bytes)
17/01/13 23:03:38 INFO Executor: Running task 0.0 in stage 69.0 (TID 116)
17/01/13 23:03:38 INFO Executor: Running task 1.0 in stage 69.0 (TID 117)
17/01/13 23:03:38 INFO CacheManager: Partition rdd_292_0 not found, computing it
17/01/13 23:03:38 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/file22acfadaa8.csv/part-00000:0+1911
17/01/13 23:03:38 INFO CacheManager: Partition rdd_292_1 not found, computing it
17/01/13 23:03:38 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/file22acfadaa8.csv/part-00001:0+2007
17/01/13 23:03:38 INFO MemoryStore: Block rdd_292_1 stored as values in memory (estimated size 3.1 KB, free 29.9 MB)
17/01/13 23:03:38 INFO MemoryStore: Block rdd_292_0 stored as values in memory (estimated size 3.2 KB, free 29.9 MB)
17/01/13 23:03:38 INFO BlockManagerInfo: Added rdd_292_1 in memory on localhost:63586 (size: 3.1 KB, free: 483.0 MB)
17/01/13 23:03:38 INFO BlockManagerInfo: Added rdd_292_0 in memory on localhost:63586 (size: 3.2 KB, free: 483.0 MB)
17/01/13 23:03:38 INFO Executor: Finished task 1.0 in stage 69.0 (TID 117). 3787 bytes result sent to driver
17/01/13 23:03:38 INFO TaskSetManager: Finished task 1.0 in stage 69.0 (TID 117) in 23 ms on localhost (1/2)
17/01/13 23:03:38 INFO Executor: Finished task 0.0 in stage 69.0 (TID 116). 3784 bytes result sent to driver
17/01/13 23:03:38 INFO TaskSetManager: Finished task 0.0 in stage 69.0 (TID 116) in 26 ms on localhost (2/2)
17/01/13 23:03:38 INFO DAGScheduler: ShuffleMapStage 69 (sql at null:-1) finished in 0.026 s
17/01/13 23:03:38 INFO TaskSchedulerImpl: Removed TaskSet 69.0, whose tasks have all completed, from pool 
17/01/13 23:03:38 INFO DAGScheduler: looking for newly runnable stages
17/01/13 23:03:38 INFO DAGScheduler: running: Set()
17/01/13 23:03:38 INFO DAGScheduler: waiting: Set(ResultStage 70)
17/01/13 23:03:38 INFO DAGScheduler: failed: Set()
17/01/13 23:03:38 INFO DAGScheduler: Submitting ResultStage 70 (MapPartitionsRDD[298] at sql at null:-1), which has no missing parents
17/01/13 23:03:38 INFO MemoryStore: Block broadcast_85 stored as values in memory (estimated size 9.3 KB, free 29.9 MB)
17/01/13 23:03:38 INFO MemoryStore: Block broadcast_85_piece0 stored as bytes in memory (estimated size 4.6 KB, free 29.9 MB)
17/01/13 23:03:38 INFO BlockManagerInfo: Added broadcast_85_piece0 in memory on localhost:63586 (size: 4.6 KB, free: 483.0 MB)
17/01/13 23:03:38 INFO SparkContext: Created broadcast 85 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 70 (MapPartitionsRDD[298] at sql at null:-1)
17/01/13 23:03:38 INFO TaskSchedulerImpl: Adding task set 70.0 with 1 tasks
17/01/13 23:03:38 INFO TaskSetManager: Starting task 0.0 in stage 70.0 (TID 118, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 23:03:38 INFO Executor: Running task 0.0 in stage 70.0 (TID 118)
17/01/13 23:03:38 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:38 INFO Executor: Finished task 0.0 in stage 70.0 (TID 118). 1830 bytes result sent to driver
17/01/13 23:03:38 INFO DAGScheduler: ResultStage 70 (sql at null:-1) finished in 0.006 s
17/01/13 23:03:38 INFO DAGScheduler: Job 45 finished: sql at null:-1, took 0.036825 s
17/01/13 23:03:38 INFO TaskSetManager: Finished task 0.0 in stage 70.0 (TID 118) in 6 ms on localhost (1/1)
17/01/13 23:03:38 INFO TaskSchedulerImpl: Removed TaskSet 70.0, whose tasks have all completed, from pool 
17/01/13 23:03:38 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 23:03:38 INFO DAGScheduler: Got job 46 (collect at utils.scala:195) with 1 output partitions
17/01/13 23:03:38 INFO DAGScheduler: Final stage: ResultStage 71 (collect at utils.scala:195)
17/01/13 23:03:38 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:38 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:38 INFO DAGScheduler: Submitting ResultStage 71 (MapPartitionsRDD[300] at collect at utils.scala:195), which has no missing parents
17/01/13 23:03:38 INFO MemoryStore: Block broadcast_86 stored as values in memory (estimated size 1968.0 B, free 29.9 MB)
17/01/13 23:03:38 INFO MemoryStore: Block broadcast_86_piece0 stored as bytes in memory (estimated size 1225.0 B, free 29.9 MB)
17/01/13 23:03:38 INFO BlockManagerInfo: Added broadcast_86_piece0 in memory on localhost:63586 (size: 1225.0 B, free: 483.0 MB)
17/01/13 23:03:38 INFO SparkContext: Created broadcast 86 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 71 (MapPartitionsRDD[300] at collect at utils.scala:195)
17/01/13 23:03:38 INFO TaskSchedulerImpl: Adding task set 71.0 with 1 tasks
17/01/13 23:03:38 INFO TaskSetManager: Starting task 0.0 in stage 71.0 (TID 119, localhost, partition 0,PROCESS_LOCAL, 2348 bytes)
17/01/13 23:03:38 INFO Executor: Running task 0.0 in stage 71.0 (TID 119)
17/01/13 23:03:38 INFO Executor: Finished task 0.0 in stage 71.0 (TID 119). 940 bytes result sent to driver
17/01/13 23:03:38 INFO DAGScheduler: ResultStage 71 (collect at utils.scala:195) finished in 0.003 s
17/01/13 23:03:38 INFO DAGScheduler: Job 46 finished: collect at utils.scala:195, took 0.007649 s
17/01/13 23:03:38 INFO TaskSetManager: Finished task 0.0 in stage 71.0 (TID 119) in 3 ms on localhost (1/1)
17/01/13 23:03:38 INFO TaskSchedulerImpl: Removed TaskSet 71.0, whose tasks have all completed, from pool 
17/01/13 23:03:38 INFO ParseDriver: Parsing command: SELECT count(*) FROM `iris_csv`
17/01/13 23:03:38 INFO ParseDriver: Parse Completed
17/01/13 23:03:38 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 23:03:38 INFO DAGScheduler: Registering RDD 303 (collect at utils.scala:195)
17/01/13 23:03:38 INFO DAGScheduler: Got job 47 (collect at utils.scala:195) with 1 output partitions
17/01/13 23:03:38 INFO DAGScheduler: Final stage: ResultStage 73 (collect at utils.scala:195)
17/01/13 23:03:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 72)
17/01/13 23:03:38 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 72)
17/01/13 23:03:38 INFO DAGScheduler: Submitting ShuffleMapStage 72 (MapPartitionsRDD[303] at collect at utils.scala:195), which has no missing parents
17/01/13 23:03:38 INFO MemoryStore: Block broadcast_87 stored as values in memory (estimated size 18.0 KB, free 29.9 MB)
17/01/13 23:03:38 INFO MemoryStore: Block broadcast_87_piece0 stored as bytes in memory (estimated size 8.5 KB, free 29.9 MB)
17/01/13 23:03:38 INFO BlockManagerInfo: Added broadcast_87_piece0 in memory on localhost:63586 (size: 8.5 KB, free: 482.9 MB)
17/01/13 23:03:38 INFO SparkContext: Created broadcast 87 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:38 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 72 (MapPartitionsRDD[303] at collect at utils.scala:195)
17/01/13 23:03:38 INFO TaskSchedulerImpl: Adding task set 72.0 with 2 tasks
17/01/13 23:03:38 INFO TaskSetManager: Starting task 0.0 in stage 72.0 (TID 120, localhost, partition 0,PROCESS_LOCAL, 2414 bytes)
17/01/13 23:03:38 INFO TaskSetManager: Starting task 1.0 in stage 72.0 (TID 121, localhost, partition 1,PROCESS_LOCAL, 2414 bytes)
17/01/13 23:03:38 INFO Executor: Running task 1.0 in stage 72.0 (TID 121)
17/01/13 23:03:38 INFO Executor: Running task 0.0 in stage 72.0 (TID 120)
17/01/13 23:03:38 INFO BlockManager: Found block rdd_292_1 locally
17/01/13 23:03:38 INFO BlockManager: Found block rdd_292_0 locally
17/01/13 23:03:38 INFO Executor: Finished task 0.0 in stage 72.0 (TID 120). 2679 bytes result sent to driver
17/01/13 23:03:38 INFO TaskSetManager: Finished task 0.0 in stage 72.0 (TID 120) in 11 ms on localhost (1/2)
17/01/13 23:03:38 INFO Executor: Finished task 1.0 in stage 72.0 (TID 121). 2679 bytes result sent to driver
17/01/13 23:03:38 INFO DAGScheduler: ShuffleMapStage 72 (collect at utils.scala:195) finished in 0.012 s
17/01/13 23:03:38 INFO TaskSetManager: Finished task 1.0 in stage 72.0 (TID 121) in 12 ms on localhost (2/2)
17/01/13 23:03:38 INFO DAGScheduler: looking for newly runnable stages
17/01/13 23:03:38 INFO DAGScheduler: running: Set()
17/01/13 23:03:38 INFO TaskSchedulerImpl: Removed TaskSet 72.0, whose tasks have all completed, from pool 
17/01/13 23:03:38 INFO DAGScheduler: waiting: Set(ResultStage 73)
17/01/13 23:03:38 INFO DAGScheduler: failed: Set()
17/01/13 23:03:38 INFO DAGScheduler: Submitting ResultStage 73 (MapPartitionsRDD[306] at collect at utils.scala:195), which has no missing parents
17/01/13 23:03:38 INFO MemoryStore: Block broadcast_88 stored as values in memory (estimated size 9.4 KB, free 30.0 MB)
17/01/13 23:03:38 INFO MemoryStore: Block broadcast_88_piece0 stored as bytes in memory (estimated size 4.6 KB, free 30.0 MB)
17/01/13 23:03:38 INFO BlockManagerInfo: Added broadcast_88_piece0 in memory on localhost:63586 (size: 4.6 KB, free: 482.9 MB)
17/01/13 23:03:38 INFO SparkContext: Created broadcast 88 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 73 (MapPartitionsRDD[306] at collect at utils.scala:195)
17/01/13 23:03:38 INFO TaskSchedulerImpl: Adding task set 73.0 with 1 tasks
17/01/13 23:03:38 INFO TaskSetManager: Starting task 0.0 in stage 73.0 (TID 122, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 23:03:38 INFO Executor: Running task 0.0 in stage 73.0 (TID 122)
17/01/13 23:03:38 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:38 INFO Executor: Finished task 0.0 in stage 73.0 (TID 122). 1830 bytes result sent to driver
17/01/13 23:03:38 INFO TaskSetManager: Finished task 0.0 in stage 73.0 (TID 122) in 7 ms on localhost (1/1)
17/01/13 23:03:38 INFO DAGScheduler: ResultStage 73 (collect at utils.scala:195) finished in 0.007 s
17/01/13 23:03:38 INFO TaskSchedulerImpl: Removed TaskSet 73.0, whose tasks have all completed, from pool 
17/01/13 23:03:38 INFO DAGScheduler: Job 47 finished: collect at utils.scala:195, took 0.027846 s
17/01/13 23:03:38 INFO ParseDriver: Parsing command: SELECT *
FROM `iris_csv` AS `zzz14`
WHERE (0 = 1)
17/01/13 23:03:38 INFO ParseDriver: Parse Completed
17/01/13 23:03:38 INFO ParseDriver: Parsing command: SELECT *
FROM `iris`
17/01/13 23:03:38 INFO ParseDriver: Parse Completed
17/01/13 23:03:38 INFO ParquetRelation: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
17/01/13 23:03:38 INFO DefaultWriterContainer: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
17/01/13 23:03:38 INFO SparkContext: Starting job: parquet at null:-2
17/01/13 23:03:38 INFO DAGScheduler: Got job 48 (parquet at null:-2) with 2 output partitions
17/01/13 23:03:38 INFO DAGScheduler: Final stage: ResultStage 74 (parquet at null:-2)
17/01/13 23:03:38 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:38 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:38 INFO DAGScheduler: Submitting ResultStage 74 (MapPartitionsRDD[307] at parquet at null:-2), which has no missing parents
17/01/13 23:03:38 INFO MemoryStore: Block broadcast_89 stored as values in memory (estimated size 76.1 KB, free 30.0 MB)
17/01/13 23:03:38 INFO MemoryStore: Block broadcast_89_piece0 stored as bytes in memory (estimated size 28.3 KB, free 30.1 MB)
17/01/13 23:03:38 INFO BlockManagerInfo: Added broadcast_89_piece0 in memory on localhost:63586 (size: 28.3 KB, free: 482.9 MB)
17/01/13 23:03:38 INFO SparkContext: Created broadcast 89 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:38 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 74 (MapPartitionsRDD[307] at parquet at null:-2)
17/01/13 23:03:38 INFO TaskSchedulerImpl: Adding task set 74.0 with 2 tasks
17/01/13 23:03:38 INFO TaskSetManager: Starting task 0.0 in stage 74.0 (TID 123, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 23:03:38 INFO TaskSetManager: Starting task 1.0 in stage 74.0 (TID 124, localhost, partition 1,PROCESS_LOCAL, 2481 bytes)
17/01/13 23:03:38 INFO Executor: Running task 1.0 in stage 74.0 (TID 124)
17/01/13 23:03:38 INFO Executor: Running task 0.0 in stage 74.0 (TID 123)
17/01/13 23:03:38 INFO BlockManager: Found block rdd_14_0 locally
17/01/13 23:03:38 INFO BlockManager: Found block rdd_14_1 locally
17/01/13 23:03:38 INFO DefaultWriterContainer: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
17/01/13 23:03:38 INFO DefaultWriterContainer: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
17/01/13 23:03:38 INFO CatalystWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "Sepal_Length",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "Sepal_Width",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "Petal_Length",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "Petal_Width",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "Species",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional double Sepal_Length;
  optional double Sepal_Width;
  optional double Petal_Length;
  optional double Petal_Width;
  optional binary Species (UTF8);
}

       
17/01/13 23:03:38 INFO CatalystWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "Sepal_Length",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "Sepal_Width",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "Petal_Length",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "Petal_Width",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "Species",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional double Sepal_Length;
  optional double Sepal_Width;
  optional double Petal_Length;
  optional double Petal_Width;
  optional binary Species (UTF8);
}

       
17/01/13 23:03:38 INFO CodecPool: Got brand-new compressor [.gz]
17/01/13 23:03:38 INFO CodecPool: Got brand-new compressor [.gz]
17/01/13 23:03:39 INFO FileOutputCommitter: Saved output of task 'attempt_201701132303_0074_m_000000_0' to file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/file22ac640f2a27.parquet/_temporary/0/task_201701132303_0074_m_000000
17/01/13 23:03:39 INFO SparkHadoopMapRedUtil: attempt_201701132303_0074_m_000000_0: Committed
17/01/13 23:03:39 INFO Executor: Finished task 0.0 in stage 74.0 (TID 123). 2223 bytes result sent to driver
17/01/13 23:03:39 INFO TaskSetManager: Finished task 0.0 in stage 74.0 (TID 123) in 479 ms on localhost (1/2)
17/01/13 23:03:39 INFO FileOutputCommitter: Saved output of task 'attempt_201701132303_0074_m_000001_0' to file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/file22ac640f2a27.parquet/_temporary/0/task_201701132303_0074_m_000001
17/01/13 23:03:39 INFO SparkHadoopMapRedUtil: attempt_201701132303_0074_m_000001_0: Committed
17/01/13 23:03:39 INFO Executor: Finished task 1.0 in stage 74.0 (TID 124). 2223 bytes result sent to driver
17/01/13 23:03:39 INFO DAGScheduler: ResultStage 74 (parquet at null:-2) finished in 0.510 s
17/01/13 23:03:39 INFO TaskSetManager: Finished task 1.0 in stage 74.0 (TID 124) in 509 ms on localhost (2/2)
17/01/13 23:03:39 INFO DAGScheduler: Job 48 finished: parquet at null:-2, took 0.531966 s
17/01/13 23:03:39 INFO TaskSchedulerImpl: Removed TaskSet 74.0, whose tasks have all completed, from pool 
17/01/13 23:03:39 INFO DefaultWriterContainer: Job job_201701132303_0000 committed.
17/01/13 23:03:39 INFO ParquetRelation: Listing file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/file22ac640f2a27.parquet on driver
17/01/13 23:03:39 INFO ParquetRelation: Listing file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/file22ac640f2a27.parquet on driver
17/01/13 23:03:39 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/01/13 23:03:39 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/01/13 23:03:39 INFO SparkContext: Starting job: collect at utils.scala:59
17/01/13 23:03:39 INFO DAGScheduler: Got job 49 (collect at utils.scala:59) with 1 output partitions
17/01/13 23:03:39 INFO DAGScheduler: Final stage: ResultStage 75 (collect at utils.scala:59)
17/01/13 23:03:39 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:39 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:39 INFO DAGScheduler: Submitting ResultStage 75 (MapPartitionsRDD[312] at map at utils.scala:56), which has no missing parents
17/01/13 23:03:39 INFO MemoryStore: Block broadcast_90 stored as values in memory (estimated size 5.4 KB, free 30.1 MB)
17/01/13 23:03:39 INFO MemoryStore: Block broadcast_90_piece0 stored as bytes in memory (estimated size 3.0 KB, free 30.1 MB)
17/01/13 23:03:39 INFO BlockManagerInfo: Added broadcast_90_piece0 in memory on localhost:63586 (size: 3.0 KB, free: 482.9 MB)
17/01/13 23:03:39 INFO SparkContext: Created broadcast 90 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 75 (MapPartitionsRDD[312] at map at utils.scala:56)
17/01/13 23:03:39 INFO TaskSchedulerImpl: Adding task set 75.0 with 1 tasks
17/01/13 23:03:39 INFO TaskSetManager: Starting task 0.0 in stage 75.0 (TID 125, localhost, partition 0,PROCESS_LOCAL, 3163 bytes)
17/01/13 23:03:39 INFO Executor: Running task 0.0 in stage 75.0 (TID 125)
17/01/13 23:03:39 INFO Executor: Finished task 0.0 in stage 75.0 (TID 125). 1277 bytes result sent to driver
17/01/13 23:03:39 INFO DAGScheduler: ResultStage 75 (collect at utils.scala:59) finished in 0.004 s
17/01/13 23:03:39 INFO DAGScheduler: Job 49 finished: collect at utils.scala:59, took 0.007460 s
17/01/13 23:03:39 INFO TaskSetManager: Finished task 0.0 in stage 75.0 (TID 125) in 4 ms on localhost (1/1)
17/01/13 23:03:39 INFO TaskSchedulerImpl: Removed TaskSet 75.0, whose tasks have all completed, from pool 
17/01/13 23:03:39 INFO ParquetRelation: Listing file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/file22ac640f2a27.parquet on driver
17/01/13 23:03:39 INFO SparkContext: Starting job: parquet at null:-2
17/01/13 23:03:39 INFO DAGScheduler: Got job 50 (parquet at null:-2) with 4 output partitions
17/01/13 23:03:39 INFO DAGScheduler: Final stage: ResultStage 76 (parquet at null:-2)
17/01/13 23:03:39 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:39 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:39 INFO DAGScheduler: Submitting ResultStage 76 (MapPartitionsRDD[314] at parquet at null:-2), which has no missing parents
17/01/13 23:03:39 INFO MemoryStore: Block broadcast_91 stored as values in memory (estimated size 61.5 KB, free 30.1 MB)
17/01/13 23:03:39 INFO MemoryStore: Block broadcast_91_piece0 stored as bytes in memory (estimated size 20.6 KB, free 30.2 MB)
17/01/13 23:03:39 INFO BlockManagerInfo: Added broadcast_91_piece0 in memory on localhost:63586 (size: 20.6 KB, free: 482.9 MB)
17/01/13 23:03:39 INFO SparkContext: Created broadcast 91 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:39 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 76 (MapPartitionsRDD[314] at parquet at null:-2)
17/01/13 23:03:39 INFO TaskSchedulerImpl: Adding task set 76.0 with 4 tasks
17/01/13 23:03:39 INFO TaskSetManager: Starting task 0.0 in stage 76.0 (TID 126, localhost, partition 0,PROCESS_LOCAL, 2319 bytes)
17/01/13 23:03:39 INFO TaskSetManager: Starting task 1.0 in stage 76.0 (TID 127, localhost, partition 1,PROCESS_LOCAL, 2319 bytes)
17/01/13 23:03:39 INFO TaskSetManager: Starting task 2.0 in stage 76.0 (TID 128, localhost, partition 2,PROCESS_LOCAL, 2319 bytes)
17/01/13 23:03:39 INFO TaskSetManager: Starting task 3.0 in stage 76.0 (TID 129, localhost, partition 3,PROCESS_LOCAL, 2477 bytes)
17/01/13 23:03:39 INFO Executor: Running task 0.0 in stage 76.0 (TID 126)
17/01/13 23:03:39 INFO Executor: Running task 1.0 in stage 76.0 (TID 127)
17/01/13 23:03:39 INFO Executor: Running task 3.0 in stage 76.0 (TID 129)
17/01/13 23:03:39 INFO Executor: Running task 2.0 in stage 76.0 (TID 128)
17/01/13 23:03:39 INFO Executor: Finished task 1.0 in stage 76.0 (TID 127). 936 bytes result sent to driver
17/01/13 23:03:39 INFO Executor: Finished task 2.0 in stage 76.0 (TID 128). 936 bytes result sent to driver
17/01/13 23:03:39 INFO Executor: Finished task 0.0 in stage 76.0 (TID 126). 936 bytes result sent to driver
17/01/13 23:03:39 INFO TaskSetManager: Finished task 1.0 in stage 76.0 (TID 127) in 24 ms on localhost (1/4)
17/01/13 23:03:39 INFO TaskSetManager: Finished task 2.0 in stage 76.0 (TID 128) in 24 ms on localhost (2/4)
17/01/13 23:03:39 INFO TaskSetManager: Finished task 0.0 in stage 76.0 (TID 126) in 25 ms on localhost (3/4)
17/01/13 23:03:39 INFO Executor: Finished task 3.0 in stage 76.0 (TID 129). 1824 bytes result sent to driver
17/01/13 23:03:39 INFO DAGScheduler: ResultStage 76 (parquet at null:-2) finished in 0.051 s
17/01/13 23:03:39 INFO TaskSetManager: Finished task 3.0 in stage 76.0 (TID 129) in 51 ms on localhost (4/4)
17/01/13 23:03:39 INFO DAGScheduler: Job 50 finished: parquet at null:-2, took 0.062833 s
17/01/13 23:03:39 INFO TaskSchedulerImpl: Removed TaskSet 76.0, whose tasks have all completed, from pool 
17/01/13 23:03:39 INFO MemoryStore: Block broadcast_92 stored as values in memory (estimated size 61.8 KB, free 30.2 MB)
17/01/13 23:03:39 INFO MemoryStore: Block broadcast_92_piece0 stored as bytes in memory (estimated size 19.3 KB, free 30.2 MB)
17/01/13 23:03:39 INFO BlockManagerInfo: Added broadcast_92_piece0 in memory on localhost:63586 (size: 19.3 KB, free: 482.9 MB)
17/01/13 23:03:39 INFO SparkContext: Created broadcast 92 from sql at null:-1
17/01/13 23:03:39 INFO deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
17/01/13 23:03:39 INFO ParquetRelation: Reading Parquet file(s) from file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/file22ac640f2a27.parquet/part-r-00000-a29d17d0-2f8d-4d0f-bb1b-bfb127545f42.gz.parquet, file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/file22ac640f2a27.parquet/part-r-00001-a29d17d0-2f8d-4d0f-bb1b-bfb127545f42.gz.parquet
17/01/13 23:03:39 INFO SparkContext: Starting job: sql at null:-1
17/01/13 23:03:39 INFO DAGScheduler: Registering RDD 319 (sql at null:-1)
17/01/13 23:03:39 INFO DAGScheduler: Got job 51 (sql at null:-1) with 1 output partitions
17/01/13 23:03:39 INFO DAGScheduler: Final stage: ResultStage 78 (sql at null:-1)
17/01/13 23:03:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 77)
17/01/13 23:03:39 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 77)
17/01/13 23:03:39 INFO DAGScheduler: Submitting ShuffleMapStage 77 (MapPartitionsRDD[319] at sql at null:-1), which has no missing parents
17/01/13 23:03:39 INFO MemoryStore: Block broadcast_93 stored as values in memory (estimated size 14.6 KB, free 30.2 MB)
17/01/13 23:03:39 INFO MemoryStore: Block broadcast_93_piece0 stored as bytes in memory (estimated size 6.8 KB, free 30.3 MB)
17/01/13 23:03:39 INFO BlockManagerInfo: Added broadcast_93_piece0 in memory on localhost:63586 (size: 6.8 KB, free: 482.9 MB)
17/01/13 23:03:39 INFO SparkContext: Created broadcast 93 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:39 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 77 (MapPartitionsRDD[319] at sql at null:-1)
17/01/13 23:03:39 INFO TaskSchedulerImpl: Adding task set 77.0 with 2 tasks
17/01/13 23:03:39 INFO TaskSetManager: Starting task 0.0 in stage 77.0 (TID 130, localhost, partition 0,PROCESS_LOCAL, 2499 bytes)
17/01/13 23:03:39 INFO TaskSetManager: Starting task 1.0 in stage 77.0 (TID 131, localhost, partition 1,PROCESS_LOCAL, 2501 bytes)
17/01/13 23:03:39 INFO Executor: Running task 0.0 in stage 77.0 (TID 130)
17/01/13 23:03:39 INFO Executor: Running task 1.0 in stage 77.0 (TID 131)
17/01/13 23:03:39 INFO CacheManager: Partition rdd_316_0 not found, computing it
17/01/13 23:03:39 INFO CacheManager: Partition rdd_316_1 not found, computing it
17/01/13 23:03:39 INFO ParquetRelation$$anonfun$buildInternalScan$1$$anon$1: Input split: ParquetInputSplit{part: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/file22ac640f2a27.parquet/part-r-00000-a29d17d0-2f8d-4d0f-bb1b-bfb127545f42.gz.parquet start: 0 end: 1943 length: 1943 hosts: []}
17/01/13 23:03:39 INFO ParquetRelation$$anonfun$buildInternalScan$1$$anon$1: Input split: ParquetInputSplit{part: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/file22ac640f2a27.parquet/part-r-00001-a29d17d0-2f8d-4d0f-bb1b-bfb127545f42.gz.parquet start: 0 end: 1953 length: 1953 hosts: []}
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 267
17/01/13 23:03:40 INFO BlockManagerInfo: Removed broadcast_91_piece0 on localhost:63586 in memory (size: 20.6 KB, free: 482.9 MB)
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 303
17/01/13 23:03:40 INFO BlockManagerInfo: Removed broadcast_90_piece0 on localhost:63586 in memory (size: 3.0 KB, free: 482.9 MB)
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 302
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 301
17/01/13 23:03:40 INFO BlockManagerInfo: Removed broadcast_89_piece0 on localhost:63586 in memory (size: 28.3 KB, free: 482.9 MB)
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 300
17/01/13 23:03:40 INFO BlockManagerInfo: Removed broadcast_88_piece0 on localhost:63586 in memory (size: 4.6 KB, free: 482.9 MB)
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 299
17/01/13 23:03:40 INFO BlockManagerInfo: Removed broadcast_87_piece0 on localhost:63586 in memory (size: 8.5 KB, free: 482.9 MB)
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 298
17/01/13 23:03:40 INFO ContextCleaner: Cleaned shuffle 25
17/01/13 23:03:40 INFO BlockManagerInfo: Removed broadcast_86_piece0 on localhost:63586 in memory (size: 1225.0 B, free: 482.9 MB)
17/01/13 23:03:40 INFO CodecPool: Got brand-new decompressor [.gz]
17/01/13 23:03:40 INFO CodecPool: Got brand-new decompressor [.gz]
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 289
17/01/13 23:03:40 INFO BlockManagerInfo: Removed broadcast_85_piece0 on localhost:63586 in memory (size: 4.6 KB, free: 482.9 MB)
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 288
17/01/13 23:03:40 INFO BlockManagerInfo: Removed broadcast_84_piece0 on localhost:63586 in memory (size: 8.5 KB, free: 482.9 MB)
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 287
17/01/13 23:03:40 INFO ContextCleaner: Cleaned shuffle 24
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 286
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 285
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 284
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 283
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 282
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 281
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 280
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 279
17/01/13 23:03:40 INFO BlockManagerInfo: Removed broadcast_82_piece0 on localhost:63586 in memory (size: 3.5 KB, free: 482.9 MB)
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 277
17/01/13 23:03:40 INFO MemoryStore: Block rdd_316_1 stored as values in memory (estimated size 3.1 KB, free 30.0 MB)
17/01/13 23:03:40 INFO MemoryStore: Block rdd_316_0 stored as values in memory (estimated size 3.2 KB, free 30.0 MB)
17/01/13 23:03:40 INFO BlockManagerInfo: Removed broadcast_81_piece0 on localhost:63586 in memory (size: 19.3 KB, free: 483.0 MB)
17/01/13 23:03:40 INFO BlockManagerInfo: Added rdd_316_1 in memory on localhost:63586 (size: 3.1 KB, free: 483.0 MB)
17/01/13 23:03:40 INFO BlockManagerInfo: Added rdd_316_0 in memory on localhost:63586 (size: 3.2 KB, free: 483.0 MB)
17/01/13 23:03:40 INFO BlockManagerInfo: Removed broadcast_80_piece0 on localhost:63586 in memory (size: 1887.0 B, free: 483.0 MB)
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 276
17/01/13 23:03:40 INFO Executor: Finished task 0.0 in stage 77.0 (TID 130). 3784 bytes result sent to driver
17/01/13 23:03:40 INFO BlockManagerInfo: Removed broadcast_79_piece0 on localhost:63586 in memory (size: 19.3 KB, free: 483.0 MB)
17/01/13 23:03:40 INFO TaskSetManager: Finished task 0.0 in stage 77.0 (TID 130) in 151 ms on localhost (1/2)
17/01/13 23:03:40 INFO Executor: Finished task 1.0 in stage 77.0 (TID 131). 3787 bytes result sent to driver
17/01/13 23:03:40 INFO DAGScheduler: ShuffleMapStage 77 (sql at null:-1) finished in 0.154 s
17/01/13 23:03:40 INFO DAGScheduler: looking for newly runnable stages
17/01/13 23:03:40 INFO DAGScheduler: running: Set()
17/01/13 23:03:40 INFO DAGScheduler: waiting: Set(ResultStage 78)
17/01/13 23:03:40 INFO DAGScheduler: failed: Set()
17/01/13 23:03:40 INFO TaskSetManager: Finished task 1.0 in stage 77.0 (TID 131) in 152 ms on localhost (2/2)
17/01/13 23:03:40 INFO DAGScheduler: Submitting ResultStage 78 (MapPartitionsRDD[322] at sql at null:-1), which has no missing parents
17/01/13 23:03:40 INFO BlockManagerInfo: Removed broadcast_78_piece0 on localhost:63586 in memory (size: 3.0 KB, free: 483.0 MB)
17/01/13 23:03:40 INFO MemoryStore: Block broadcast_94 stored as values in memory (estimated size 9.3 KB, free 29.5 MB)
17/01/13 23:03:40 INFO MemoryStore: Block broadcast_94_piece0 stored as bytes in memory (estimated size 4.6 KB, free 29.5 MB)
17/01/13 23:03:40 INFO TaskSchedulerImpl: Removed TaskSet 77.0, whose tasks have all completed, from pool 
17/01/13 23:03:40 INFO BlockManagerInfo: Added broadcast_94_piece0 in memory on localhost:63586 (size: 4.6 KB, free: 483.0 MB)
17/01/13 23:03:40 INFO SparkContext: Created broadcast 94 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 78 (MapPartitionsRDD[322] at sql at null:-1)
17/01/13 23:03:40 INFO TaskSchedulerImpl: Adding task set 78.0 with 1 tasks
17/01/13 23:03:40 INFO TaskSetManager: Starting task 0.0 in stage 78.0 (TID 132, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 23:03:40 INFO Executor: Running task 0.0 in stage 78.0 (TID 132)
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 275
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 274
17/01/13 23:03:40 INFO BlockManagerInfo: Removed broadcast_77_piece0 on localhost:63586 in memory (size: 28.2 KB, free: 483.0 MB)
17/01/13 23:03:40 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 273
17/01/13 23:03:40 INFO BlockManagerInfo: Removed broadcast_76_piece0 on localhost:63586 in memory (size: 15.6 KB, free: 483.0 MB)
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 272
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 271
17/01/13 23:03:40 INFO Executor: Finished task 0.0 in stage 78.0 (TID 132). 1830 bytes result sent to driver
17/01/13 23:03:40 INFO BlockManagerInfo: Removed broadcast_75_piece0 on localhost:63586 in memory (size: 4.6 KB, free: 483.0 MB)
17/01/13 23:03:40 INFO DAGScheduler: ResultStage 78 (sql at null:-1) finished in 0.009 s
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 270
17/01/13 23:03:40 INFO DAGScheduler: Job 51 finished: sql at null:-1, took 0.176309 s
17/01/13 23:03:40 INFO TaskSetManager: Finished task 0.0 in stage 78.0 (TID 132) in 8 ms on localhost (1/1)
17/01/13 23:03:40 INFO TaskSchedulerImpl: Removed TaskSet 78.0, whose tasks have all completed, from pool 
17/01/13 23:03:40 INFO BlockManagerInfo: Removed broadcast_74_piece0 on localhost:63586 in memory (size: 13.2 KB, free: 483.0 MB)
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 269
17/01/13 23:03:40 INFO ContextCleaner: Cleaned shuffle 23
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 268
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 266
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 265
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 264
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 263
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 262
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 261
17/01/13 23:03:40 INFO BlockManagerInfo: Removed broadcast_73_piece0 on localhost:63586 in memory (size: 4.6 KB, free: 483.0 MB)
17/01/13 23:03:40 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 23:03:40 INFO DAGScheduler: Got job 52 (collect at utils.scala:195) with 1 output partitions
17/01/13 23:03:40 INFO DAGScheduler: Final stage: ResultStage 79 (collect at utils.scala:195)
17/01/13 23:03:40 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:40 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:40 INFO DAGScheduler: Submitting ResultStage 79 (MapPartitionsRDD[324] at collect at utils.scala:195), which has no missing parents
17/01/13 23:03:40 INFO MemoryStore: Block broadcast_95 stored as values in memory (estimated size 1968.0 B, free 29.3 MB)
17/01/13 23:03:40 INFO MemoryStore: Block broadcast_95_piece0 stored as bytes in memory (estimated size 1224.0 B, free 29.3 MB)
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 260
17/01/13 23:03:40 INFO BlockManagerInfo: Added broadcast_95_piece0 in memory on localhost:63586 (size: 1224.0 B, free: 483.0 MB)
17/01/13 23:03:40 INFO SparkContext: Created broadcast 95 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 79 (MapPartitionsRDD[324] at collect at utils.scala:195)
17/01/13 23:03:40 INFO TaskSchedulerImpl: Adding task set 79.0 with 1 tasks
17/01/13 23:03:40 INFO TaskSetManager: Starting task 0.0 in stage 79.0 (TID 133, localhost, partition 0,PROCESS_LOCAL, 2348 bytes)
17/01/13 23:03:40 INFO Executor: Running task 0.0 in stage 79.0 (TID 133)
17/01/13 23:03:40 INFO BlockManagerInfo: Removed broadcast_72_piece0 on localhost:63586 in memory (size: 13.1 KB, free: 483.1 MB)
17/01/13 23:03:40 INFO Executor: Finished task 0.0 in stage 79.0 (TID 133). 940 bytes result sent to driver
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 259
17/01/13 23:03:40 INFO DAGScheduler: ResultStage 79 (collect at utils.scala:195) finished in 0.003 s
17/01/13 23:03:40 INFO DAGScheduler: Job 52 finished: collect at utils.scala:195, took 0.008174 s
17/01/13 23:03:40 INFO TaskSetManager: Finished task 0.0 in stage 79.0 (TID 133) in 2 ms on localhost (1/1)
17/01/13 23:03:40 INFO TaskSchedulerImpl: Removed TaskSet 79.0, whose tasks have all completed, from pool 
17/01/13 23:03:40 INFO ContextCleaner: Cleaned shuffle 22
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 258
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 257
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 256
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 255
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 254
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 253
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 252
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 251
17/01/13 23:03:40 INFO ContextCleaner: Cleaned accumulator 246
17/01/13 23:03:40 INFO ParseDriver: Parsing command: SELECT count(*) FROM `iris_parquet`
17/01/13 23:03:40 INFO ParseDriver: Parse Completed
17/01/13 23:03:40 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 23:03:40 INFO DAGScheduler: Registering RDD 327 (collect at utils.scala:195)
17/01/13 23:03:40 INFO DAGScheduler: Got job 53 (collect at utils.scala:195) with 1 output partitions
17/01/13 23:03:40 INFO DAGScheduler: Final stage: ResultStage 81 (collect at utils.scala:195)
17/01/13 23:03:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 80)
17/01/13 23:03:40 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 80)
17/01/13 23:03:40 INFO DAGScheduler: Submitting ShuffleMapStage 80 (MapPartitionsRDD[327] at collect at utils.scala:195), which has no missing parents
17/01/13 23:03:40 INFO MemoryStore: Block broadcast_96 stored as values in memory (estimated size 14.6 KB, free 29.3 MB)
17/01/13 23:03:40 INFO MemoryStore: Block broadcast_96_piece0 stored as bytes in memory (estimated size 6.8 KB, free 29.3 MB)
17/01/13 23:03:40 INFO BlockManagerInfo: Added broadcast_96_piece0 in memory on localhost:63586 (size: 6.8 KB, free: 483.0 MB)
17/01/13 23:03:40 INFO SparkContext: Created broadcast 96 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:40 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 80 (MapPartitionsRDD[327] at collect at utils.scala:195)
17/01/13 23:03:40 INFO TaskSchedulerImpl: Adding task set 80.0 with 2 tasks
17/01/13 23:03:40 INFO TaskSetManager: Starting task 0.0 in stage 80.0 (TID 134, localhost, partition 0,PROCESS_LOCAL, 2499 bytes)
17/01/13 23:03:40 INFO TaskSetManager: Starting task 1.0 in stage 80.0 (TID 135, localhost, partition 1,PROCESS_LOCAL, 2501 bytes)
17/01/13 23:03:40 INFO Executor: Running task 0.0 in stage 80.0 (TID 134)
17/01/13 23:03:40 INFO Executor: Running task 1.0 in stage 80.0 (TID 135)
17/01/13 23:03:40 INFO BlockManager: Found block rdd_316_0 locally
17/01/13 23:03:40 INFO BlockManager: Found block rdd_316_1 locally
17/01/13 23:03:40 INFO Executor: Finished task 1.0 in stage 80.0 (TID 135). 2679 bytes result sent to driver
17/01/13 23:03:40 INFO TaskSetManager: Finished task 1.0 in stage 80.0 (TID 135) in 27 ms on localhost (1/2)
17/01/13 23:03:40 INFO Executor: Finished task 0.0 in stage 80.0 (TID 134). 2679 bytes result sent to driver
17/01/13 23:03:40 INFO DAGScheduler: ShuffleMapStage 80 (collect at utils.scala:195) finished in 0.031 s
17/01/13 23:03:40 INFO DAGScheduler: looking for newly runnable stages
17/01/13 23:03:40 INFO DAGScheduler: running: Set()
17/01/13 23:03:40 INFO DAGScheduler: waiting: Set(ResultStage 81)
17/01/13 23:03:40 INFO DAGScheduler: failed: Set()
17/01/13 23:03:40 INFO DAGScheduler: Submitting ResultStage 81 (MapPartitionsRDD[330] at collect at utils.scala:195), which has no missing parents
17/01/13 23:03:40 INFO MemoryStore: Block broadcast_97 stored as values in memory (estimated size 9.4 KB, free 29.3 MB)
17/01/13 23:03:40 INFO TaskSetManager: Finished task 0.0 in stage 80.0 (TID 134) in 30 ms on localhost (2/2)
17/01/13 23:03:40 INFO TaskSchedulerImpl: Removed TaskSet 80.0, whose tasks have all completed, from pool 
17/01/13 23:03:40 INFO MemoryStore: Block broadcast_97_piece0 stored as bytes in memory (estimated size 4.7 KB, free 29.3 MB)
17/01/13 23:03:40 INFO BlockManagerInfo: Added broadcast_97_piece0 in memory on localhost:63586 (size: 4.7 KB, free: 483.0 MB)
17/01/13 23:03:40 INFO SparkContext: Created broadcast 97 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 81 (MapPartitionsRDD[330] at collect at utils.scala:195)
17/01/13 23:03:40 INFO TaskSchedulerImpl: Adding task set 81.0 with 1 tasks
17/01/13 23:03:40 INFO TaskSetManager: Starting task 0.0 in stage 81.0 (TID 136, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 23:03:40 INFO Executor: Running task 0.0 in stage 81.0 (TID 136)
17/01/13 23:03:40 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/01/13 23:03:40 INFO Executor: Finished task 0.0 in stage 81.0 (TID 136). 1830 bytes result sent to driver
17/01/13 23:03:40 INFO DAGScheduler: ResultStage 81 (collect at utils.scala:195) finished in 0.011 s
17/01/13 23:03:40 INFO TaskSetManager: Finished task 0.0 in stage 81.0 (TID 136) in 9 ms on localhost (1/1)
17/01/13 23:03:40 INFO DAGScheduler: Job 53 finished: collect at utils.scala:195, took 0.050284 s
17/01/13 23:03:40 INFO TaskSchedulerImpl: Removed TaskSet 81.0, whose tasks have all completed, from pool 
17/01/13 23:03:40 INFO ParseDriver: Parsing command: SELECT *
FROM `iris_parquet` AS `zzz15`
WHERE (0 = 1)
17/01/13 23:03:40 INFO ParseDriver: Parse Completed
17/01/13 23:03:40 INFO ParseDriver: Parsing command: SELECT *
FROM `iris`
17/01/13 23:03:40 INFO ParseDriver: Parse Completed
17/01/13 23:03:40 INFO DefaultWriterContainer: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17/01/13 23:03:40 INFO SparkContext: Starting job: json at null:-2
17/01/13 23:03:40 INFO DAGScheduler: Got job 54 (json at null:-2) with 2 output partitions
17/01/13 23:03:40 INFO DAGScheduler: Final stage: ResultStage 82 (json at null:-2)
17/01/13 23:03:40 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:40 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:40 INFO DAGScheduler: Submitting ResultStage 82 (MapPartitionsRDD[331] at json at null:-2), which has no missing parents
17/01/13 23:03:40 INFO MemoryStore: Block broadcast_98 stored as values in memory (estimated size 74.9 KB, free 29.4 MB)
17/01/13 23:03:40 INFO MemoryStore: Block broadcast_98_piece0 stored as bytes in memory (estimated size 27.8 KB, free 29.4 MB)
17/01/13 23:03:40 INFO BlockManagerInfo: Added broadcast_98_piece0 in memory on localhost:63586 (size: 27.8 KB, free: 483.0 MB)
17/01/13 23:03:40 INFO SparkContext: Created broadcast 98 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:40 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 82 (MapPartitionsRDD[331] at json at null:-2)
17/01/13 23:03:40 INFO TaskSchedulerImpl: Adding task set 82.0 with 2 tasks
17/01/13 23:03:40 INFO TaskSetManager: Starting task 0.0 in stage 82.0 (TID 137, localhost, partition 0,PROCESS_LOCAL, 2481 bytes)
17/01/13 23:03:40 INFO TaskSetManager: Starting task 1.0 in stage 82.0 (TID 138, localhost, partition 1,PROCESS_LOCAL, 2481 bytes)
17/01/13 23:03:40 INFO Executor: Running task 1.0 in stage 82.0 (TID 138)
17/01/13 23:03:40 INFO Executor: Running task 0.0 in stage 82.0 (TID 137)
17/01/13 23:03:40 INFO BlockManager: Found block rdd_14_1 locally
17/01/13 23:03:40 INFO DefaultWriterContainer: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17/01/13 23:03:40 INFO BlockManager: Found block rdd_14_0 locally
17/01/13 23:03:40 INFO DefaultWriterContainer: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17/01/13 23:03:40 INFO FileOutputCommitter: Saved output of task 'attempt_201701132303_0082_m_000001_0' to file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/file22ac1a064885.json/_temporary/0/task_201701132303_0082_m_000001
17/01/13 23:03:40 INFO SparkHadoopMapRedUtil: attempt_201701132303_0082_m_000001_0: Committed
17/01/13 23:03:40 INFO Executor: Finished task 1.0 in stage 82.0 (TID 138). 2223 bytes result sent to driver
17/01/13 23:03:40 INFO TaskSetManager: Finished task 1.0 in stage 82.0 (TID 138) in 111 ms on localhost (1/2)
17/01/13 23:03:40 INFO FileOutputCommitter: Saved output of task 'attempt_201701132303_0082_m_000000_0' to file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/file22ac1a064885.json/_temporary/0/task_201701132303_0082_m_000000
17/01/13 23:03:40 INFO SparkHadoopMapRedUtil: attempt_201701132303_0082_m_000000_0: Committed
17/01/13 23:03:40 INFO Executor: Finished task 0.0 in stage 82.0 (TID 137). 2223 bytes result sent to driver
17/01/13 23:03:40 INFO DAGScheduler: ResultStage 82 (json at null:-2) finished in 0.137 s
17/01/13 23:03:40 INFO TaskSetManager: Finished task 0.0 in stage 82.0 (TID 137) in 137 ms on localhost (2/2)
17/01/13 23:03:40 INFO DAGScheduler: Job 54 finished: json at null:-2, took 0.154623 s
17/01/13 23:03:40 INFO TaskSchedulerImpl: Removed TaskSet 82.0, whose tasks have all completed, from pool 
17/01/13 23:03:40 INFO DefaultWriterContainer: Job job_201701132303_0000 committed.
17/01/13 23:03:40 INFO JSONRelation: Listing file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/file22ac1a064885.json on driver
17/01/13 23:03:40 INFO JSONRelation: Listing file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/file22ac1a064885.json on driver
17/01/13 23:03:40 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/01/13 23:03:40 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/01/13 23:03:40 INFO SparkContext: Starting job: collect at utils.scala:59
17/01/13 23:03:40 INFO DAGScheduler: Got job 55 (collect at utils.scala:59) with 1 output partitions
17/01/13 23:03:40 INFO DAGScheduler: Final stage: ResultStage 83 (collect at utils.scala:59)
17/01/13 23:03:40 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:40 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:40 INFO DAGScheduler: Submitting ResultStage 83 (MapPartitionsRDD[336] at map at utils.scala:56), which has no missing parents
17/01/13 23:03:40 INFO MemoryStore: Block broadcast_99 stored as values in memory (estimated size 5.4 KB, free 29.4 MB)
17/01/13 23:03:40 INFO MemoryStore: Block broadcast_99_piece0 stored as bytes in memory (estimated size 3.0 KB, free 29.4 MB)
17/01/13 23:03:40 INFO BlockManagerInfo: Added broadcast_99_piece0 in memory on localhost:63586 (size: 3.0 KB, free: 483.0 MB)
17/01/13 23:03:40 INFO SparkContext: Created broadcast 99 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 83 (MapPartitionsRDD[336] at map at utils.scala:56)
17/01/13 23:03:40 INFO TaskSchedulerImpl: Adding task set 83.0 with 1 tasks
17/01/13 23:03:40 INFO TaskSetManager: Starting task 0.0 in stage 83.0 (TID 139, localhost, partition 0,PROCESS_LOCAL, 3209 bytes)
17/01/13 23:03:40 INFO Executor: Running task 0.0 in stage 83.0 (TID 139)
17/01/13 23:03:40 INFO Executor: Finished task 0.0 in stage 83.0 (TID 139). 1292 bytes result sent to driver
17/01/13 23:03:40 INFO TaskSetManager: Finished task 0.0 in stage 83.0 (TID 139) in 3 ms on localhost (1/1)
17/01/13 23:03:40 INFO TaskSchedulerImpl: Removed TaskSet 83.0, whose tasks have all completed, from pool 
17/01/13 23:03:40 INFO DAGScheduler: ResultStage 83 (collect at utils.scala:59) finished in 0.003 s
17/01/13 23:03:40 INFO DAGScheduler: Job 55 finished: collect at utils.scala:59, took 0.007040 s
17/01/13 23:03:40 INFO JSONRelation: Listing file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/file22ac1a064885.json on driver
17/01/13 23:03:40 INFO MemoryStore: Block broadcast_100 stored as values in memory (estimated size 209.4 KB, free 29.6 MB)
17/01/13 23:03:40 INFO MemoryStore: Block broadcast_100_piece0 stored as bytes in memory (estimated size 19.5 KB, free 29.6 MB)
17/01/13 23:03:40 INFO BlockManagerInfo: Added broadcast_100_piece0 in memory on localhost:63586 (size: 19.5 KB, free: 483.0 MB)
17/01/13 23:03:40 INFO SparkContext: Created broadcast 100 from json at null:-2
17/01/13 23:03:40 INFO FileInputFormat: Total input paths to process : 2
17/01/13 23:03:40 INFO SparkContext: Starting job: json at null:-2
17/01/13 23:03:40 INFO DAGScheduler: Got job 56 (json at null:-2) with 2 output partitions
17/01/13 23:03:40 INFO DAGScheduler: Final stage: ResultStage 84 (json at null:-2)
17/01/13 23:03:40 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:40 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:40 INFO DAGScheduler: Submitting ResultStage 84 (MapPartitionsRDD[340] at json at null:-2), which has no missing parents
17/01/13 23:03:40 INFO MemoryStore: Block broadcast_101 stored as values in memory (estimated size 4.4 KB, free 29.6 MB)
17/01/13 23:03:40 INFO MemoryStore: Block broadcast_101_piece0 stored as bytes in memory (estimated size 2.5 KB, free 29.6 MB)
17/01/13 23:03:40 INFO BlockManagerInfo: Added broadcast_101_piece0 in memory on localhost:63586 (size: 2.5 KB, free: 483.0 MB)
17/01/13 23:03:40 INFO SparkContext: Created broadcast 101 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:40 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 84 (MapPartitionsRDD[340] at json at null:-2)
17/01/13 23:03:40 INFO TaskSchedulerImpl: Adding task set 84.0 with 2 tasks
17/01/13 23:03:40 INFO TaskSetManager: Starting task 0.0 in stage 84.0 (TID 140, localhost, partition 0,PROCESS_LOCAL, 2467 bytes)
17/01/13 23:03:40 INFO TaskSetManager: Starting task 1.0 in stage 84.0 (TID 141, localhost, partition 1,PROCESS_LOCAL, 2467 bytes)
17/01/13 23:03:40 INFO Executor: Running task 1.0 in stage 84.0 (TID 141)
17/01/13 23:03:40 INFO Executor: Running task 0.0 in stage 84.0 (TID 140)
17/01/13 23:03:40 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/file22ac1a064885.json/part-r-00000-d9823662-aaad-45de-b178-355c11377657:0+7324
17/01/13 23:03:40 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/file22ac1a064885.json/part-r-00001-d9823662-aaad-45de-b178-355c11377657:0+7276
17/01/13 23:03:40 INFO Executor: Finished task 0.0 in stage 84.0 (TID 140). 2971 bytes result sent to driver
17/01/13 23:03:40 INFO TaskSetManager: Finished task 0.0 in stage 84.0 (TID 140) in 20 ms on localhost (1/2)
17/01/13 23:03:40 INFO Executor: Finished task 1.0 in stage 84.0 (TID 141). 2971 bytes result sent to driver
17/01/13 23:03:40 INFO DAGScheduler: ResultStage 84 (json at null:-2) finished in 0.024 s
17/01/13 23:03:40 INFO DAGScheduler: Job 56 finished: json at null:-2, took 0.027072 s
17/01/13 23:03:40 INFO TaskSetManager: Finished task 1.0 in stage 84.0 (TID 141) in 24 ms on localhost (2/2)
17/01/13 23:03:40 INFO TaskSchedulerImpl: Removed TaskSet 84.0, whose tasks have all completed, from pool 
17/01/13 23:03:40 INFO MemoryStore: Block broadcast_102 stored as values in memory (estimated size 208.5 KB, free 29.8 MB)
17/01/13 23:03:40 INFO MemoryStore: Block broadcast_102_piece0 stored as bytes in memory (estimated size 19.3 KB, free 29.9 MB)
17/01/13 23:03:40 INFO BlockManagerInfo: Added broadcast_102_piece0 in memory on localhost:63586 (size: 19.3 KB, free: 483.0 MB)
17/01/13 23:03:40 INFO SparkContext: Created broadcast 102 from sql at null:-1
17/01/13 23:03:40 INFO MemoryStore: Block broadcast_103 stored as values in memory (estimated size 209.4 KB, free 30.1 MB)
17/01/13 23:03:40 INFO MemoryStore: Block broadcast_103_piece0 stored as bytes in memory (estimated size 19.5 KB, free 30.1 MB)
17/01/13 23:03:40 INFO BlockManagerInfo: Added broadcast_103_piece0 in memory on localhost:63586 (size: 19.5 KB, free: 483.0 MB)
17/01/13 23:03:40 INFO SparkContext: Created broadcast 103 from sql at null:-1
17/01/13 23:03:40 INFO FileInputFormat: Total input paths to process : 2
17/01/13 23:03:40 INFO SparkContext: Starting job: sql at null:-1
17/01/13 23:03:40 INFO DAGScheduler: Registering RDD 348 (sql at null:-1)
17/01/13 23:03:40 INFO DAGScheduler: Got job 57 (sql at null:-1) with 1 output partitions
17/01/13 23:03:40 INFO DAGScheduler: Final stage: ResultStage 86 (sql at null:-1)
17/01/13 23:03:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 85)
17/01/13 23:03:40 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 85)
17/01/13 23:03:40 INFO DAGScheduler: Submitting ShuffleMapStage 85 (MapPartitionsRDD[348] at sql at null:-1), which has no missing parents
17/01/13 23:03:40 INFO MemoryStore: Block broadcast_104 stored as values in memory (estimated size 14.8 KB, free 30.1 MB)
17/01/13 23:03:40 INFO MemoryStore: Block broadcast_104_piece0 stored as bytes in memory (estimated size 6.9 KB, free 30.1 MB)
17/01/13 23:03:40 INFO BlockManagerInfo: Added broadcast_104_piece0 in memory on localhost:63586 (size: 6.9 KB, free: 482.9 MB)
17/01/13 23:03:40 INFO SparkContext: Created broadcast 104 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:40 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 85 (MapPartitionsRDD[348] at sql at null:-1)
17/01/13 23:03:40 INFO TaskSchedulerImpl: Adding task set 85.0 with 2 tasks
17/01/13 23:03:40 INFO TaskSetManager: Starting task 0.0 in stage 85.0 (TID 142, localhost, partition 0,PROCESS_LOCAL, 2456 bytes)
17/01/13 23:03:40 INFO TaskSetManager: Starting task 1.0 in stage 85.0 (TID 143, localhost, partition 1,PROCESS_LOCAL, 2456 bytes)
17/01/13 23:03:40 INFO Executor: Running task 0.0 in stage 85.0 (TID 142)
17/01/13 23:03:40 INFO Executor: Running task 1.0 in stage 85.0 (TID 143)
17/01/13 23:03:40 INFO CacheManager: Partition rdd_345_1 not found, computing it
17/01/13 23:03:40 INFO CacheManager: Partition rdd_345_0 not found, computing it
17/01/13 23:03:40 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/file22ac1a064885.json/part-r-00000-d9823662-aaad-45de-b178-355c11377657:0+7324
17/01/13 23:03:40 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/file22ac1a064885.json/part-r-00001-d9823662-aaad-45de-b178-355c11377657:0+7276
17/01/13 23:03:40 INFO MemoryStore: Block rdd_345_1 stored as values in memory (estimated size 3.1 KB, free 30.1 MB)
17/01/13 23:03:40 INFO MemoryStore: Block rdd_345_0 stored as values in memory (estimated size 3.2 KB, free 30.1 MB)
17/01/13 23:03:40 INFO BlockManagerInfo: Added rdd_345_1 in memory on localhost:63586 (size: 3.1 KB, free: 482.9 MB)
17/01/13 23:03:40 INFO BlockManagerInfo: Added rdd_345_0 in memory on localhost:63586 (size: 3.2 KB, free: 482.9 MB)
17/01/13 23:03:40 INFO Executor: Finished task 1.0 in stage 85.0 (TID 143). 3787 bytes result sent to driver
17/01/13 23:03:40 INFO TaskSetManager: Finished task 1.0 in stage 85.0 (TID 143) in 36 ms on localhost (1/2)
17/01/13 23:03:40 INFO Executor: Finished task 0.0 in stage 85.0 (TID 142). 3784 bytes result sent to driver
17/01/13 23:03:40 INFO DAGScheduler: ShuffleMapStage 85 (sql at null:-1) finished in 0.045 s
17/01/13 23:03:40 INFO DAGScheduler: looking for newly runnable stages
17/01/13 23:03:40 INFO DAGScheduler: running: Set()
17/01/13 23:03:40 INFO DAGScheduler: waiting: Set(ResultStage 86)
17/01/13 23:03:40 INFO TaskSetManager: Finished task 0.0 in stage 85.0 (TID 142) in 44 ms on localhost (2/2)
17/01/13 23:03:40 INFO DAGScheduler: failed: Set()
17/01/13 23:03:40 INFO TaskSchedulerImpl: Removed TaskSet 85.0, whose tasks have all completed, from pool 
17/01/13 23:03:40 INFO DAGScheduler: Submitting ResultStage 86 (MapPartitionsRDD[351] at sql at null:-1), which has no missing parents
17/01/13 23:03:40 INFO MemoryStore: Block broadcast_105 stored as values in memory (estimated size 9.3 KB, free 30.1 MB)
17/01/13 23:03:40 INFO MemoryStore: Block broadcast_105_piece0 stored as bytes in memory (estimated size 4.6 KB, free 30.1 MB)
17/01/13 23:03:40 INFO BlockManagerInfo: Added broadcast_105_piece0 in memory on localhost:63586 (size: 4.6 KB, free: 482.9 MB)
17/01/13 23:03:40 INFO SparkContext: Created broadcast 105 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 86 (MapPartitionsRDD[351] at sql at null:-1)
17/01/13 23:03:40 INFO TaskSchedulerImpl: Adding task set 86.0 with 1 tasks
17/01/13 23:03:40 INFO TaskSetManager: Starting task 0.0 in stage 86.0 (TID 144, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 23:03:40 INFO Executor: Running task 0.0 in stage 86.0 (TID 144)
17/01/13 23:03:40 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:40 INFO Executor: Finished task 0.0 in stage 86.0 (TID 144). 1830 bytes result sent to driver
17/01/13 23:03:40 INFO DAGScheduler: ResultStage 86 (sql at null:-1) finished in 0.006 s
17/01/13 23:03:40 INFO TaskSetManager: Finished task 0.0 in stage 86.0 (TID 144) in 6 ms on localhost (1/1)
17/01/13 23:03:40 INFO DAGScheduler: Job 57 finished: sql at null:-1, took 0.062981 s
17/01/13 23:03:40 INFO TaskSchedulerImpl: Removed TaskSet 86.0, whose tasks have all completed, from pool 
17/01/13 23:03:40 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 23:03:40 INFO DAGScheduler: Got job 58 (collect at utils.scala:195) with 1 output partitions
17/01/13 23:03:40 INFO DAGScheduler: Final stage: ResultStage 87 (collect at utils.scala:195)
17/01/13 23:03:40 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:40 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:40 INFO DAGScheduler: Submitting ResultStage 87 (MapPartitionsRDD[353] at collect at utils.scala:195), which has no missing parents
17/01/13 23:03:40 INFO MemoryStore: Block broadcast_106 stored as values in memory (estimated size 1968.0 B, free 30.1 MB)
17/01/13 23:03:40 INFO MemoryStore: Block broadcast_106_piece0 stored as bytes in memory (estimated size 1224.0 B, free 30.1 MB)
17/01/13 23:03:40 INFO BlockManagerInfo: Added broadcast_106_piece0 in memory on localhost:63586 (size: 1224.0 B, free: 482.9 MB)
17/01/13 23:03:40 INFO SparkContext: Created broadcast 106 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 87 (MapPartitionsRDD[353] at collect at utils.scala:195)
17/01/13 23:03:40 INFO TaskSchedulerImpl: Adding task set 87.0 with 1 tasks
17/01/13 23:03:40 INFO TaskSetManager: Starting task 0.0 in stage 87.0 (TID 145, localhost, partition 0,PROCESS_LOCAL, 2348 bytes)
17/01/13 23:03:40 INFO Executor: Running task 0.0 in stage 87.0 (TID 145)
17/01/13 23:03:40 INFO Executor: Finished task 0.0 in stage 87.0 (TID 145). 940 bytes result sent to driver
17/01/13 23:03:40 INFO DAGScheduler: ResultStage 87 (collect at utils.scala:195) finished in 0.002 s
17/01/13 23:03:40 INFO DAGScheduler: Job 58 finished: collect at utils.scala:195, took 0.005305 s
17/01/13 23:03:40 INFO TaskSetManager: Finished task 0.0 in stage 87.0 (TID 145) in 2 ms on localhost (1/1)
17/01/13 23:03:40 INFO TaskSchedulerImpl: Removed TaskSet 87.0, whose tasks have all completed, from pool 
17/01/13 23:03:40 INFO ParseDriver: Parsing command: SELECT count(*) FROM `iris_json`
17/01/13 23:03:40 INFO ParseDriver: Parse Completed
17/01/13 23:03:40 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 23:03:40 INFO DAGScheduler: Registering RDD 356 (collect at utils.scala:195)
17/01/13 23:03:40 INFO DAGScheduler: Got job 59 (collect at utils.scala:195) with 1 output partitions
17/01/13 23:03:40 INFO DAGScheduler: Final stage: ResultStage 89 (collect at utils.scala:195)
17/01/13 23:03:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 88)
17/01/13 23:03:40 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 88)
17/01/13 23:03:40 INFO DAGScheduler: Submitting ShuffleMapStage 88 (MapPartitionsRDD[356] at collect at utils.scala:195), which has no missing parents
17/01/13 23:03:40 INFO MemoryStore: Block broadcast_107 stored as values in memory (estimated size 14.9 KB, free 30.1 MB)
17/01/13 23:03:40 INFO MemoryStore: Block broadcast_107_piece0 stored as bytes in memory (estimated size 6.9 KB, free 30.2 MB)
17/01/13 23:03:40 INFO BlockManagerInfo: Added broadcast_107_piece0 in memory on localhost:63586 (size: 6.9 KB, free: 482.9 MB)
17/01/13 23:03:40 INFO SparkContext: Created broadcast 107 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:40 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 88 (MapPartitionsRDD[356] at collect at utils.scala:195)
17/01/13 23:03:40 INFO TaskSchedulerImpl: Adding task set 88.0 with 2 tasks
17/01/13 23:03:40 INFO TaskSetManager: Starting task 0.0 in stage 88.0 (TID 146, localhost, partition 0,PROCESS_LOCAL, 2456 bytes)
17/01/13 23:03:40 INFO TaskSetManager: Starting task 1.0 in stage 88.0 (TID 147, localhost, partition 1,PROCESS_LOCAL, 2456 bytes)
17/01/13 23:03:40 INFO Executor: Running task 1.0 in stage 88.0 (TID 147)
17/01/13 23:03:40 INFO Executor: Running task 0.0 in stage 88.0 (TID 146)
17/01/13 23:03:40 INFO BlockManager: Found block rdd_345_1 locally
17/01/13 23:03:40 INFO BlockManager: Found block rdd_345_0 locally
17/01/13 23:03:40 INFO Executor: Finished task 1.0 in stage 88.0 (TID 147). 2679 bytes result sent to driver
17/01/13 23:03:40 INFO TaskSetManager: Finished task 1.0 in stage 88.0 (TID 147) in 12 ms on localhost (1/2)
17/01/13 23:03:40 INFO Executor: Finished task 0.0 in stage 88.0 (TID 146). 2679 bytes result sent to driver
17/01/13 23:03:40 INFO TaskSetManager: Finished task 0.0 in stage 88.0 (TID 146) in 46 ms on localhost (2/2)
17/01/13 23:03:40 INFO DAGScheduler: ShuffleMapStage 88 (collect at utils.scala:195) finished in 0.046 s
17/01/13 23:03:40 INFO TaskSchedulerImpl: Removed TaskSet 88.0, whose tasks have all completed, from pool 
17/01/13 23:03:40 INFO DAGScheduler: looking for newly runnable stages
17/01/13 23:03:40 INFO DAGScheduler: running: Set()
17/01/13 23:03:40 INFO DAGScheduler: waiting: Set(ResultStage 89)
17/01/13 23:03:40 INFO DAGScheduler: failed: Set()
17/01/13 23:03:40 INFO DAGScheduler: Submitting ResultStage 89 (MapPartitionsRDD[359] at collect at utils.scala:195), which has no missing parents
17/01/13 23:03:40 INFO MemoryStore: Block broadcast_108 stored as values in memory (estimated size 9.4 KB, free 30.2 MB)
17/01/13 23:03:40 INFO MemoryStore: Block broadcast_108_piece0 stored as bytes in memory (estimated size 4.6 KB, free 30.2 MB)
17/01/13 23:03:40 INFO BlockManagerInfo: Added broadcast_108_piece0 in memory on localhost:63586 (size: 4.6 KB, free: 482.9 MB)
17/01/13 23:03:40 INFO SparkContext: Created broadcast 108 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 89 (MapPartitionsRDD[359] at collect at utils.scala:195)
17/01/13 23:03:40 INFO TaskSchedulerImpl: Adding task set 89.0 with 1 tasks
17/01/13 23:03:40 INFO TaskSetManager: Starting task 0.0 in stage 89.0 (TID 148, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 23:03:40 INFO Executor: Running task 0.0 in stage 89.0 (TID 148)
17/01/13 23:03:40 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:40 INFO Executor: Finished task 0.0 in stage 89.0 (TID 148). 1830 bytes result sent to driver
17/01/13 23:03:40 INFO TaskSetManager: Finished task 0.0 in stage 89.0 (TID 148) in 5 ms on localhost (1/1)
17/01/13 23:03:40 INFO DAGScheduler: ResultStage 89 (collect at utils.scala:195) finished in 0.005 s
17/01/13 23:03:40 INFO DAGScheduler: Job 59 finished: collect at utils.scala:195, took 0.056248 s
17/01/13 23:03:40 INFO TaskSchedulerImpl: Removed TaskSet 89.0, whose tasks have all completed, from pool 
17/01/13 23:03:40 INFO ParseDriver: Parsing command: SELECT *
FROM `iris_json` AS `zzz16`
WHERE (0 = 1)
17/01/13 23:03:40 INFO ParseDriver: Parse Completed
17/01/13 23:03:40 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/01/13 23:03:40 INFO audit: ugi=Baoco	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/01/13 23:03:40 INFO SparkContext: Starting job: collect at utils.scala:59
17/01/13 23:03:40 INFO DAGScheduler: Got job 60 (collect at utils.scala:59) with 1 output partitions
17/01/13 23:03:40 INFO DAGScheduler: Final stage: ResultStage 90 (collect at utils.scala:59)
17/01/13 23:03:40 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:40 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:40 INFO DAGScheduler: Submitting ResultStage 90 (MapPartitionsRDD[363] at map at utils.scala:56), which has no missing parents
17/01/13 23:03:40 INFO MemoryStore: Block broadcast_109 stored as values in memory (estimated size 5.4 KB, free 30.2 MB)
17/01/13 23:03:40 INFO MemoryStore: Block broadcast_109_piece0 stored as bytes in memory (estimated size 3.0 KB, free 30.2 MB)
17/01/13 23:03:40 INFO BlockManagerInfo: Added broadcast_109_piece0 in memory on localhost:63586 (size: 3.0 KB, free: 482.9 MB)
17/01/13 23:03:40 INFO SparkContext: Created broadcast 109 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 90 (MapPartitionsRDD[363] at map at utils.scala:56)
17/01/13 23:03:40 INFO TaskSchedulerImpl: Adding task set 90.0 with 1 tasks
17/01/13 23:03:40 INFO TaskSetManager: Starting task 0.0 in stage 90.0 (TID 149, localhost, partition 0,PROCESS_LOCAL, 3252 bytes)
17/01/13 23:03:40 INFO Executor: Running task 0.0 in stage 90.0 (TID 149)
17/01/13 23:03:40 INFO Executor: Finished task 0.0 in stage 90.0 (TID 149). 1304 bytes result sent to driver
17/01/13 23:03:41 INFO TaskSetManager: Finished task 0.0 in stage 90.0 (TID 149) in 3 ms on localhost (1/1)
17/01/13 23:03:41 INFO DAGScheduler: ResultStage 90 (collect at utils.scala:59) finished in 0.003 s
17/01/13 23:03:41 INFO TaskSchedulerImpl: Removed TaskSet 90.0, whose tasks have all completed, from pool 
17/01/13 23:03:41 INFO DAGScheduler: Job 60 finished: collect at utils.scala:59, took 0.004847 s
17/01/13 23:03:49 INFO MemoryStore: Block broadcast_110 stored as values in memory (estimated size 208.5 KB, free 30.4 MB)
17/01/13 23:03:49 INFO MemoryStore: Block broadcast_110_piece0 stored as bytes in memory (estimated size 19.3 KB, free 30.4 MB)
17/01/13 23:03:49 INFO BlockManagerInfo: Added broadcast_110_piece0 in memory on localhost:63586 (size: 19.3 KB, free: 482.9 MB)
17/01/13 23:03:49 INFO SparkContext: Created broadcast 110 from textFile at null:-2
17/01/13 23:03:50 INFO FileInputFormat: Total input paths to process : 1
17/01/13 23:03:50 INFO SparkContext: Starting job: count at null:-2
17/01/13 23:03:50 INFO DAGScheduler: Got job 61 (count at null:-2) with 1 output partitions
17/01/13 23:03:50 INFO DAGScheduler: Final stage: ResultStage 91 (count at null:-2)
17/01/13 23:03:50 INFO DAGScheduler: Parents of final stage: List()
17/01/13 23:03:50 INFO DAGScheduler: Missing parents: List()
17/01/13 23:03:50 INFO DAGScheduler: Submitting ResultStage 91 (C:\Users\Baoco\AppData\Local\Temp\RtmpOgwhNO\file22ac1c592532.csv MapPartitionsRDD[365] at textFile at null:-2), which has no missing parents
17/01/13 23:03:50 INFO MemoryStore: Block broadcast_111 stored as values in memory (estimated size 3.0 KB, free 30.4 MB)
17/01/13 23:03:50 INFO MemoryStore: Block broadcast_111_piece0 stored as bytes in memory (estimated size 1824.0 B, free 30.4 MB)
17/01/13 23:03:50 INFO BlockManagerInfo: Added broadcast_111_piece0 in memory on localhost:63586 (size: 1824.0 B, free: 482.9 MB)
17/01/13 23:03:50 INFO SparkContext: Created broadcast 111 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 91 (C:\Users\Baoco\AppData\Local\Temp\RtmpOgwhNO\file22ac1c592532.csv MapPartitionsRDD[365] at textFile at null:-2)
17/01/13 23:03:50 INFO TaskSchedulerImpl: Adding task set 91.0 with 1 tasks
17/01/13 23:03:50 INFO TaskSetManager: Starting task 0.0 in stage 91.0 (TID 150, localhost, partition 0,PROCESS_LOCAL, 2416 bytes)
17/01/13 23:03:50 INFO Executor: Running task 0.0 in stage 91.0 (TID 150)
17/01/13 23:03:50 INFO HadoopRDD: Input split: file:/C:/Users/Baoco/AppData/Local/Temp/RtmpOgwhNO/file22ac1c592532.csv:0+33649883
17/01/13 23:03:50 INFO ContextCleaner: Cleaned accumulator 326
17/01/13 23:03:50 INFO BlockManagerInfo: Removed broadcast_109_piece0 on localhost:63586 in memory (size: 3.0 KB, free: 482.9 MB)
17/01/13 23:03:50 INFO ContextCleaner: Cleaned accumulator 353
17/01/13 23:03:50 INFO ContextCleaner: Cleaned accumulator 352
17/01/13 23:03:50 INFO BlockManagerInfo: Removed broadcast_108_piece0 on localhost:63586 in memory (size: 4.6 KB, free: 482.9 MB)
17/01/13 23:03:50 INFO ContextCleaner: Cleaned accumulator 351
17/01/13 23:03:50 INFO BlockManagerInfo: Removed broadcast_107_piece0 on localhost:63586 in memory (size: 6.9 KB, free: 482.9 MB)
17/01/13 23:03:50 INFO ContextCleaner: Cleaned accumulator 350
17/01/13 23:03:50 INFO ContextCleaner: Cleaned shuffle 29
17/01/13 23:03:50 INFO BlockManagerInfo: Removed broadcast_106_piece0 on localhost:63586 in memory (size: 1224.0 B, free: 482.9 MB)
17/01/13 23:03:50 INFO ContextCleaner: Cleaned accumulator 341
17/01/13 23:03:50 INFO BlockManagerInfo: Removed broadcast_105_piece0 on localhost:63586 in memory (size: 4.6 KB, free: 482.9 MB)
17/01/13 23:03:50 INFO ContextCleaner: Cleaned accumulator 340
17/01/13 23:03:50 INFO BlockManagerInfo: Removed broadcast_104_piece0 on localhost:63586 in memory (size: 6.9 KB, free: 482.9 MB)
17/01/13 23:03:50 INFO ContextCleaner: Cleaned accumulator 339
17/01/13 23:03:50 INFO ContextCleaner: Cleaned shuffle 28
17/01/13 23:03:50 INFO ContextCleaner: Cleaned accumulator 338
17/01/13 23:03:50 INFO ContextCleaner: Cleaned accumulator 337
17/01/13 23:03:50 INFO ContextCleaner: Cleaned accumulator 336
17/01/13 23:03:50 INFO ContextCleaner: Cleaned accumulator 335
17/01/13 23:03:50 INFO ContextCleaner: Cleaned accumulator 334
17/01/13 23:03:50 INFO ContextCleaner: Cleaned accumulator 333
17/01/13 23:03:50 INFO ContextCleaner: Cleaned accumulator 332
17/01/13 23:03:50 INFO ContextCleaner: Cleaned accumulator 331
17/01/13 23:03:50 INFO BlockManagerInfo: Removed broadcast_102_piece0 on localhost:63586 in memory (size: 19.3 KB, free: 482.9 MB)
17/01/13 23:03:50 INFO BlockManagerInfo: Removed broadcast_101_piece0 on localhost:63586 in memory (size: 2.5 KB, free: 482.9 MB)
17/01/13 23:03:50 INFO ContextCleaner: Cleaned accumulator 329
17/01/13 23:03:50 INFO BlockManagerInfo: Removed broadcast_100_piece0 on localhost:63586 in memory (size: 19.5 KB, free: 483.0 MB)
17/01/13 23:03:50 INFO BlockManagerInfo: Removed broadcast_99_piece0 on localhost:63586 in memory (size: 3.0 KB, free: 483.0 MB)
17/01/13 23:03:50 INFO ContextCleaner: Cleaned accumulator 328
17/01/13 23:03:50 INFO ContextCleaner: Cleaned accumulator 327
17/01/13 23:03:50 INFO BlockManagerInfo: Removed broadcast_98_piece0 on localhost:63586 in memory (size: 27.8 KB, free: 483.0 MB)
17/01/13 23:03:50 INFO BlockManagerInfo: Removed broadcast_97_piece0 on localhost:63586 in memory (size: 4.7 KB, free: 483.0 MB)
17/01/13 23:03:50 INFO ContextCleaner: Cleaned accumulator 325
17/01/13 23:03:50 INFO BlockManagerInfo: Removed broadcast_96_piece0 on localhost:63586 in memory (size: 6.8 KB, free: 483.0 MB)
17/01/13 23:03:50 INFO ContextCleaner: Cleaned accumulator 324
17/01/13 23:03:50 INFO ContextCleaner: Cleaned shuffle 27
17/01/13 23:03:50 INFO BlockManagerInfo: Removed broadcast_95_piece0 on localhost:63586 in memory (size: 1224.0 B, free: 483.0 MB)
17/01/13 23:03:50 INFO ContextCleaner: Cleaned accumulator 315
17/01/13 23:03:50 INFO BlockManagerInfo: Removed broadcast_94_piece0 on localhost:63586 in memory (size: 4.6 KB, free: 483.0 MB)
17/01/13 23:03:50 INFO ContextCleaner: Cleaned accumulator 314
17/01/13 23:03:50 INFO BlockManagerInfo: Removed broadcast_93_piece0 on localhost:63586 in memory (size: 6.8 KB, free: 483.0 MB)
17/01/13 23:03:50 INFO ContextCleaner: Cleaned accumulator 313
17/01/13 23:03:50 INFO ContextCleaner: Cleaned shuffle 26
17/01/13 23:03:50 INFO ContextCleaner: Cleaned accumulator 312
17/01/13 23:03:50 INFO ContextCleaner: Cleaned accumulator 311
17/01/13 23:03:50 INFO ContextCleaner: Cleaned accumulator 310
17/01/13 23:03:50 INFO ContextCleaner: Cleaned accumulator 309
17/01/13 23:03:50 INFO ContextCleaner: Cleaned accumulator 308
17/01/13 23:03:50 INFO ContextCleaner: Cleaned accumulator 307
17/01/13 23:03:50 INFO ContextCleaner: Cleaned accumulator 306
17/01/13 23:03:50 INFO ContextCleaner: Cleaned accumulator 305
17/01/13 23:03:50 INFO Executor: Finished task 0.0 in stage 91.0 (TID 150). 2082 bytes result sent to driver
17/01/13 23:03:50 INFO DAGScheduler: ResultStage 91 (count at null:-2) finished in 0.723 s
17/01/13 23:03:50 INFO TaskSetManager: Finished task 0.0 in stage 91.0 (TID 150) in 723 ms on localhost (1/1)
17/01/13 23:03:50 INFO DAGScheduler: Job 61 finished: count at null:-2, took 0.726078 s
17/01/13 23:03:50 INFO TaskSchedulerImpl: Removed TaskSet 91.0, whose tasks have all completed, from pool 
17/01/13 23:03:50 INFO ParseDriver: Parsing command: SELECT * FROM  `batting`
17/01/13 23:03:50 INFO ParseDriver: Parse Completed
17/01/13 23:03:50 WARN CacheManager: Asked to cache already cached data.
17/01/13 23:03:50 INFO SparkContext: Starting job: sql at null:-1
17/01/13 23:03:50 INFO DAGScheduler: Registering RDD 368 (sql at null:-1)
17/01/13 23:03:50 INFO DAGScheduler: Got job 62 (sql at null:-1) with 1 output partitions
17/01/13 23:03:50 INFO DAGScheduler: Final stage: ResultStage 93 (sql at null:-1)
17/01/13 23:03:50 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 92)
17/01/13 23:03:50 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 92)
17/01/13 23:03:50 INFO DAGScheduler: Submitting ShuffleMapStage 92 (MapPartitionsRDD[368] at sql at null:-1), which has no missing parents
17/01/13 23:03:50 INFO MemoryStore: Block broadcast_112 stored as values in memory (estimated size 27.9 KB, free 29.7 MB)
17/01/13 23:03:50 INFO MemoryStore: Block broadcast_112_piece0 stored as bytes in memory (estimated size 11.2 KB, free 29.7 MB)
17/01/13 23:03:50 INFO BlockManagerInfo: Added broadcast_112_piece0 in memory on localhost:63586 (size: 11.2 KB, free: 483.0 MB)
17/01/13 23:03:50 INFO SparkContext: Created broadcast 112 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:50 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 92 (MapPartitionsRDD[368] at sql at null:-1)
17/01/13 23:03:50 INFO TaskSchedulerImpl: Adding task set 92.0 with 2 tasks
17/01/13 23:03:50 INFO TaskSetManager: Starting task 0.0 in stage 92.0 (TID 151, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:50 INFO TaskSetManager: Starting task 1.0 in stage 92.0 (TID 152, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:50 INFO Executor: Running task 0.0 in stage 92.0 (TID 151)
17/01/13 23:03:50 INFO Executor: Running task 1.0 in stage 92.0 (TID 152)
17/01/13 23:03:50 INFO BlockManager: Found block rdd_70_0 locally
17/01/13 23:03:50 INFO BlockManager: Found block rdd_70_1 locally
17/01/13 23:03:50 INFO Executor: Finished task 0.0 in stage 92.0 (TID 151). 2679 bytes result sent to driver
17/01/13 23:03:50 INFO TaskSetManager: Finished task 0.0 in stage 92.0 (TID 151) in 24 ms on localhost (1/2)
17/01/13 23:03:50 INFO Executor: Finished task 1.0 in stage 92.0 (TID 152). 2679 bytes result sent to driver
17/01/13 23:03:50 INFO TaskSetManager: Finished task 1.0 in stage 92.0 (TID 152) in 25 ms on localhost (2/2)
17/01/13 23:03:50 INFO TaskSchedulerImpl: Removed TaskSet 92.0, whose tasks have all completed, from pool 
17/01/13 23:03:50 INFO DAGScheduler: ShuffleMapStage 92 (sql at null:-1) finished in 0.026 s
17/01/13 23:03:50 INFO DAGScheduler: looking for newly runnable stages
17/01/13 23:03:50 INFO DAGScheduler: running: Set()
17/01/13 23:03:50 INFO DAGScheduler: waiting: Set(ResultStage 93)
17/01/13 23:03:50 INFO DAGScheduler: failed: Set()
17/01/13 23:03:50 INFO DAGScheduler: Submitting ResultStage 93 (MapPartitionsRDD[371] at sql at null:-1), which has no missing parents
17/01/13 23:03:50 INFO MemoryStore: Block broadcast_113 stored as values in memory (estimated size 9.3 KB, free 29.7 MB)
17/01/13 23:03:50 INFO MemoryStore: Block broadcast_113_piece0 stored as bytes in memory (estimated size 4.6 KB, free 29.7 MB)
17/01/13 23:03:50 INFO BlockManagerInfo: Added broadcast_113_piece0 in memory on localhost:63586 (size: 4.6 KB, free: 483.0 MB)
17/01/13 23:03:50 INFO SparkContext: Created broadcast 113 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 93 (MapPartitionsRDD[371] at sql at null:-1)
17/01/13 23:03:50 INFO TaskSchedulerImpl: Adding task set 93.0 with 1 tasks
17/01/13 23:03:50 INFO TaskSetManager: Starting task 0.0 in stage 93.0 (TID 153, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 23:03:50 INFO Executor: Running task 0.0 in stage 93.0 (TID 153)
17/01/13 23:03:50 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:50 INFO Executor: Finished task 0.0 in stage 93.0 (TID 153). 1830 bytes result sent to driver
17/01/13 23:03:50 INFO TaskSetManager: Finished task 0.0 in stage 93.0 (TID 153) in 4 ms on localhost (1/1)
17/01/13 23:03:50 INFO DAGScheduler: ResultStage 93 (sql at null:-1) finished in 0.006 s
17/01/13 23:03:50 INFO TaskSchedulerImpl: Removed TaskSet 93.0, whose tasks have all completed, from pool 
17/01/13 23:03:50 INFO DAGScheduler: Job 62 finished: sql at null:-1, took 0.037529 s
17/01/13 23:03:50 INFO ParseDriver: Parsing command: SELECT count(*) FROM  `batting`
17/01/13 23:03:50 INFO ParseDriver: Parse Completed
17/01/13 23:03:50 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/13 23:03:50 INFO DAGScheduler: Registering RDD 375 (collect at utils.scala:195)
17/01/13 23:03:50 INFO DAGScheduler: Got job 63 (collect at utils.scala:195) with 1 output partitions
17/01/13 23:03:50 INFO DAGScheduler: Final stage: ResultStage 95 (collect at utils.scala:195)
17/01/13 23:03:50 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 94)
17/01/13 23:03:50 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 94)
17/01/13 23:03:50 INFO DAGScheduler: Submitting ShuffleMapStage 94 (MapPartitionsRDD[375] at collect at utils.scala:195), which has no missing parents
17/01/13 23:03:50 INFO MemoryStore: Block broadcast_114 stored as values in memory (estimated size 28.0 KB, free 29.8 MB)
17/01/13 23:03:50 INFO MemoryStore: Block broadcast_114_piece0 stored as bytes in memory (estimated size 11.2 KB, free 29.8 MB)
17/01/13 23:03:50 INFO BlockManagerInfo: Added broadcast_114_piece0 in memory on localhost:63586 (size: 11.2 KB, free: 483.0 MB)
17/01/13 23:03:50 INFO SparkContext: Created broadcast 114 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:50 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 94 (MapPartitionsRDD[375] at collect at utils.scala:195)
17/01/13 23:03:50 INFO TaskSchedulerImpl: Adding task set 94.0 with 2 tasks
17/01/13 23:03:50 INFO TaskSetManager: Starting task 0.0 in stage 94.0 (TID 154, localhost, partition 0,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:50 INFO TaskSetManager: Starting task 1.0 in stage 94.0 (TID 155, localhost, partition 1,PROCESS_LOCAL, 2470 bytes)
17/01/13 23:03:50 INFO Executor: Running task 0.0 in stage 94.0 (TID 154)
17/01/13 23:03:50 INFO Executor: Running task 1.0 in stage 94.0 (TID 155)
17/01/13 23:03:50 INFO BlockManager: Found block rdd_70_0 locally
17/01/13 23:03:50 INFO BlockManager: Found block rdd_70_1 locally
17/01/13 23:03:50 INFO Executor: Finished task 1.0 in stage 94.0 (TID 155). 2679 bytes result sent to driver
17/01/13 23:03:50 INFO TaskSetManager: Finished task 1.0 in stage 94.0 (TID 155) in 17 ms on localhost (1/2)
17/01/13 23:03:50 INFO Executor: Finished task 0.0 in stage 94.0 (TID 154). 2679 bytes result sent to driver
17/01/13 23:03:50 INFO DAGScheduler: ShuffleMapStage 94 (collect at utils.scala:195) finished in 0.020 s
17/01/13 23:03:50 INFO TaskSetManager: Finished task 0.0 in stage 94.0 (TID 154) in 20 ms on localhost (2/2)
17/01/13 23:03:50 INFO DAGScheduler: looking for newly runnable stages
17/01/13 23:03:50 INFO DAGScheduler: running: Set()
17/01/13 23:03:50 INFO TaskSchedulerImpl: Removed TaskSet 94.0, whose tasks have all completed, from pool 
17/01/13 23:03:50 INFO DAGScheduler: waiting: Set(ResultStage 95)
17/01/13 23:03:50 INFO DAGScheduler: failed: Set()
17/01/13 23:03:50 INFO DAGScheduler: Submitting ResultStage 95 (MapPartitionsRDD[378] at collect at utils.scala:195), which has no missing parents
17/01/13 23:03:50 INFO MemoryStore: Block broadcast_115 stored as values in memory (estimated size 9.4 KB, free 29.8 MB)
17/01/13 23:03:50 INFO MemoryStore: Block broadcast_115_piece0 stored as bytes in memory (estimated size 4.6 KB, free 29.8 MB)
17/01/13 23:03:50 INFO BlockManagerInfo: Added broadcast_115_piece0 in memory on localhost:63586 (size: 4.6 KB, free: 483.0 MB)
17/01/13 23:03:50 INFO SparkContext: Created broadcast 115 from broadcast at DAGScheduler.scala:1006
17/01/13 23:03:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 95 (MapPartitionsRDD[378] at collect at utils.scala:195)
17/01/13 23:03:50 INFO TaskSchedulerImpl: Adding task set 95.0 with 1 tasks
17/01/13 23:03:50 INFO TaskSetManager: Starting task 0.0 in stage 95.0 (TID 156, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/13 23:03:50 INFO Executor: Running task 0.0 in stage 95.0 (TID 156)
17/01/13 23:03:50 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/13 23:03:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/01/13 23:03:50 INFO Executor: Finished task 0.0 in stage 95.0 (TID 156). 1830 bytes result sent to driver
17/01/13 23:03:50 INFO DAGScheduler: ResultStage 95 (collect at utils.scala:195) finished in 0.009 s
17/01/13 23:03:50 INFO DAGScheduler: Job 63 finished: collect at utils.scala:195, took 0.036041 s
17/01/13 23:03:50 INFO TaskSetManager: Finished task 0.0 in stage 95.0 (TID 156) in 8 ms on localhost (1/1)
17/01/13 23:03:50 INFO TaskSchedulerImpl: Removed TaskSet 95.0, whose tasks have all completed, from pool 
17/01/13 23:03:51 INFO MapPartitionsRDD: Removing RDD 70 from persistence list
17/01/13 23:03:51 INFO BlockManager: Removing RDD 70
17/01/13 23:03:52 INFO SparkContext: Invoking stop() from shutdown hook
17/01/13 23:03:52 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
17/01/13 23:03:52 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/01/13 23:03:52 INFO MemoryStore: MemoryStore cleared
17/01/13 23:03:52 INFO BlockManager: BlockManager stopped
17/01/13 23:03:52 INFO BlockManagerMaster: BlockManagerMaster stopped
17/01/13 23:03:52 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/01/13 23:03:52 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\Baoco\AppData\Local\Temp\spark-2832be6c-3ac9-44da-bbe7-34543b51c4dd\userFiles-f0f5ea2d-5cc0-415b-9d63-6baf82062343
java.io.IOException: Failed to delete: C:\Users\Baoco\AppData\Local\Temp\spark-2832be6c-3ac9-44da-bbe7-34543b51c4dd\userFiles-f0f5ea2d-5cc0-415b-9d63-6baf82062343
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:119)
	at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1219)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1755)
	at org.apache.spark.SparkContext$$anonfun$3.apply$mcV$sp(SparkContext.scala:596)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/01/13 23:03:52 INFO SparkContext: Successfully stopped SparkContext
17/01/13 23:03:52 INFO ShutdownHookManager: Shutdown hook called
17/01/13 23:03:52 INFO ShutdownHookManager: Deleting directory C:\Users\Baoco\AppData\Local\Temp\spark-2832be6c-3ac9-44da-bbe7-34543b51c4dd\httpd-136dac86-a29b-4589-80a1-a344001c5f98
17/01/13 23:03:52 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
17/01/13 23:03:52 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
17/01/13 23:03:52 INFO ShutdownHookManager: Deleting directory C:\Users\Baoco\AppData\Local\Temp\spark-2832be6c-3ac9-44da-bbe7-34543b51c4dd\userFiles-f0f5ea2d-5cc0-415b-9d63-6baf82062343
17/01/13 23:03:52 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\Baoco\AppData\Local\Temp\spark-2832be6c-3ac9-44da-bbe7-34543b51c4dd\userFiles-f0f5ea2d-5cc0-415b-9d63-6baf82062343
java.io.IOException: Failed to delete: C:\Users\Baoco\AppData\Local\Temp\spark-2832be6c-3ac9-44da-bbe7-34543b51c4dd\userFiles-f0f5ea2d-5cc0-415b-9d63-6baf82062343
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/01/13 23:03:52 INFO ShutdownHookManager: Deleting directory C:\Users\Baoco\AppData\Local\Temp\spark-a461e387-88ee-40ee-9bdd-a327e01c45e0
17/01/13 23:03:52 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
17/01/13 23:03:52 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\Baoco\AppData\Local\Temp\spark-a461e387-88ee-40ee-9bdd-a327e01c45e0
java.io.IOException: Failed to delete: C:\Users\Baoco\AppData\Local\Temp\spark-a461e387-88ee-40ee-9bdd-a327e01c45e0
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/01/13 23:03:52 INFO ShutdownHookManager: Deleting directory C:\Users\Baoco\AppData\Local\Temp\spark-2832be6c-3ac9-44da-bbe7-34543b51c4dd
17/01/13 23:03:52 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\Baoco\AppData\Local\Temp\spark-2832be6c-3ac9-44da-bbe7-34543b51c4dd
java.io.IOException: Failed to delete: C:\Users\Baoco\AppData\Local\Temp\spark-2832be6c-3ac9-44da-bbe7-34543b51c4dd
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
